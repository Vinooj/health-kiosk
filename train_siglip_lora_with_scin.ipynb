{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8991385",
   "metadata": {},
   "source": [
    "# Sigmoid vs Contrastive Loss\n",
    "\n",
    "**Recommendation:** Use **SIGMOID loss** for your use case. Here's why:\n",
    "\n",
    "| Aspect            | Sigmoid Loss                                   | Contrastive Loss                                  |\n",
    "|-------------------|------------------------------------------------|----------------------------------------------------|\n",
    "| **Best for**      | Multi-label (multiple conditions per image)    | Single-label classification                        |\n",
    "| **Your data**     | ‚úÖ 20%+ multi-label samples                     | ‚ùå Treats each sample as single class              |\n",
    "| **Output**        | Independent probabilities per class            | Probability distribution (sums to 1)               |\n",
    "| **Training**      | Each label trained independently               | Contrasts positive vs negative pairs               |\n",
    "| **Class imbalance** | ‚úÖ Handles better with per-class thresholds  | ‚ùå Can struggle with imbalanced data               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoo/projects/health-kiosk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SIGLIP sigmoid LEARNING USLING LoRA\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: google/siglip-base-patch16-224\n",
      "  Output: siglip_lora_sigmoid_202511221028\n",
      "  Device: cuda\n",
      "  Batch size: 32\n",
      "  Learning rate: 1e-05\n",
      "  Weight decay: 0.01\n",
      "  Epochs: 15\n",
      "  Lora_rank: 32\n",
      "  Output directory: siglip_lora_sigmoid_202511221028\n",
      "\n",
      "================================================================================\n",
      "Loading Data\n",
      "================================================================================\n",
      "Total real samples in ./data/coarse_labeled_metadata_with_labels.csv: 5909\n",
      "conditions: 66\n",
      "Coarse categories: 16\n",
      "\n",
      "================================================================================\n",
      "Loading and Processing Data\n",
      "================================================================================\n",
      "Raw data loaded: 5909 samples\n",
      "\n",
      "Balancing training data (target min: 30)...\n",
      "  Added 45 samples via oversampling.\n",
      "Training set size before adding synthetic data: 4147\n",
      "Loaded metadata for 1354 existing synthetic samples.\n",
      "Checking 32 rare classes (Target: 30)...\n",
      "No new images generated.\n",
      "New training set size after adding synthetic data: 5501\n",
      "================================================================================\n",
      " DATASET ANALYSIS FOR SIGLIP FINE-TUNING\n",
      "================================================================================\n",
      "\n",
      "1. OVERALL DATASET STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 5,909\n",
      "  - Training:   5,501 (93.1%)\n",
      "  - Validation: 910 (15.4%)\n",
      "  - Test:       897 (15.2%)\n",
      "\n",
      "Unique conditions: 66\n",
      "Unique coarse categories: 16\n",
      "\n",
      "2. LABEL DISTRIBUTION BY SPLIT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 15 Most Common Conditions:\n",
      "Condition                                   Train      Val     Test    Total\n",
      "--------------------------------------------------------------------------------\n",
      "Eczema                                        751      162      166     1079\n",
      "Allergic Contact Dermatitis                   391      104       95      590\n",
      "Urticaria                                     310       66       66      442\n",
      "Insect Bite                                   280       61       60      401\n",
      "Folliculitis                                  216       45       45      306\n",
      "Psoriasis                                     173       31       30      234\n",
      "Tinea                                         149       31       31      211\n",
      "Impetigo                                       97       18       21      136\n",
      "Herpes Zoster                                  92       19       19      130\n",
      "Drug Rash                                      92       20       17      129\n",
      "Pigmented purpuric eruption                    88       21       19      128\n",
      "Acne                                           71       26       26      123\n",
      "Herpes Simplex                                 77       12       20      109\n",
      "CD - Contact dermatitis                        69       14       15       98\n",
      "Acute dermatitis, NOS                          62       14       17       93\n",
      "\n",
      "3. POTENTIAL TRAINING ISSUES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  Conditions with <10 total samples: 0\n",
      "\n",
      "‚ö†Ô∏è  Conditions missing from validation set: 6\n",
      "  - Melasma\n",
      "  - Petechiae\n",
      "  - Pityriasis rubra pilaris\n",
      "  - Hyperpigmentation\n",
      "  - Bullous Pemphigoid\n",
      "  - Melanoma\n",
      "\n",
      "‚ö†Ô∏è  Conditions missing from test set: 6\n",
      "  - Vitiligo\n",
      "  - Melasma\n",
      "  - Petechiae\n",
      "  - Pityriasis rubra pilaris\n",
      "  - Hyperpigmentation\n",
      "  - SCC/SCCIS\n",
      "\n",
      "4. MULTI-LABEL ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Labels per Image          Train        Val       Test\n",
      "--------------------------------------------------------------------------------\n",
      "1 label(s):       2160        177        191\n",
      "2 label(s):       1007        215        208\n",
      "3 label(s):       1485        325        310\n",
      "4 label(s):        253         55         57\n",
      "5 label(s):        204         45         46\n",
      "6 label(s):        211         49         50\n",
      "7 label(s):        113         32         18\n",
      "8 label(s):         47         11         15\n",
      "9 label(s):         21          1          2\n",
      "\n",
      "Multi-label samples in training: 3,341 (60.7%)\n",
      "\n",
      "5. COARSE CATEGORY DISTRIBUTION\n",
      "--------------------------------------------------------------------------------\n",
      "Category                                    Train      Val     Test    Total\n",
      "--------------------------------------------------------------------------------\n",
      "Acne/Follicular                               202       55       62      319\n",
      "Autoimmune/Lichenoid                          121       25       22      168\n",
      "Bacterial Infections                          379       85       83      547\n",
      "Benign Tumors                                  51       10        9       70\n",
      "Bullous Disorders                               5        0        1        6\n",
      "Fungal Infections                             201       40       42      283\n",
      "Inflammatory Dermatitis                      1535      345      338     2218\n",
      "Parasitic/Insect                              310       66       69      445\n",
      "Pigmentary Disorders                           24        4        1       29\n",
      "Pruritic Conditions                            33       12        9       54\n",
      "Psoriatic Conditions                          237       46       46      329\n",
      "Skin Cancer                                   121        9        7      137\n",
      "Trauma/Wounds                                  77       21       20      118\n",
      "Urticaria/Allergic                            447      100       91      638\n",
      "Vascular Disorders                            193       41       40      274\n",
      "Viral Infections                              263       51       57      371\n",
      "\n",
      "6. CLASS IMBALANCE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Training set class imbalance:\n",
      "  - Most common class:  Eczema                                   (751 samples)\n",
      "  - Least common class: Lichen planus lichenoid eruption         (3 samples)\n",
      "  - Imbalance ratio:    250.3:1\n",
      "\n",
      "Median samples per class: 51\n",
      "Mean samples per class:   79.7\n",
      "\n",
      "7. RECOMMENDATIONS FOR SIGLIP FINE-TUNING\n",
      "================================================================================\n",
      "\n",
      "üìä Data Considerations:\n",
      "  ‚ö†Ô∏è  SEVERE class imbalance detected (>100:1)\n",
      "     ‚Üí Consider: class weights, focal loss, or oversampling rare classes\n",
      "\n",
      "  ‚ÑπÔ∏è  High multi-label percentage (60.7%)\n",
      "     ‚Üí Use SIGMOID loss (not contrastive) for multi-label classification\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Loading Model and Creating Datasets\n",
      "================================================================================\n",
      "‚úì Model loaded with 204,337,156 parameters\n",
      "‚úì Datasets created\n",
      "  Train: 5501 samples\n",
      "  Val: 910 samples\n",
      "  Test: 897 samples\n",
      "\n",
      "Sample fine-grained texts:\n",
      "  [Urticaria] Skin lesion consistent with Urticaria...\n",
      "  [Tinea] Dermatoscopy image showing Tinea...\n",
      "  [Viral Exanthem] Skin condition diagnosed as Viral Exanthem...\n",
      "\n",
      "Applying LoRA fine-tuning...\n",
      "‚úÖ ICD Map and Matrix loaded successfully.\n",
      "\n",
      "================================================================================\n",
      "ICD MAP INTEGRITY CHECK\n",
      "================================================================================\n",
      "\n",
      "--- ICD MAP INTEGRITY CHECK ---\n",
      "‚úÖ Map Size: 66 conditions.\n",
      "---------------------------------\n",
      "Sample Keys (First 25 for detailed verification):\n",
      "  [ 0 | Index 0] Key: 'Abrasion, scrape, or scab' (Raw: 'Abrasion, scrape, or scab')\n",
      "  [ 1 | Index 1] Key: 'Abscess' (Raw: 'Abscess')\n",
      "  [ 2 | Index 2] Key: 'Acne' (Raw: 'Acne')\n",
      "  [ 3 | Index 3] Key: 'Actinic Keratosis' (Raw: 'Actinic Keratosis')\n",
      "  [ 4 | Index 4] Key: 'Acute and chronic dermatitis' (Raw: 'Acute and chronic dermatitis')\n",
      "  [ 5 | Index 5] Key: 'Acute dermatitis, NOS' (Raw: 'Acute dermatitis, NOS')\n",
      "  [ 6 | Index 6] Key: 'Allergic Contact Dermatitis' (Raw: 'Allergic Contact Dermatitis')\n",
      "  [ 7 | Index 7] Key: 'Basal Cell Carcinoma' (Raw: 'Basal Cell Carcinoma')\n",
      "  [ 8 | Index 8] Key: 'Bullous Pemphigoid' (Raw: 'Bullous Pemphigoid')\n",
      "  [ 9 | Index 9] Key: 'CD - Contact dermatitis' (Raw: 'CD - Contact dermatitis')\n",
      "  [10 | Index 10] Key: 'Cellulitis' (Raw: 'Cellulitis')\n",
      "  [11 | Index 11] Key: 'Chronic dermatitis, NOS' (Raw: 'Chronic dermatitis, NOS')\n",
      "  [12 | Index 12] Key: 'Cutaneous lupus' (Raw: 'Cutaneous lupus')\n",
      "  [13 | Index 13] Key: 'Dermatofibroma' (Raw: 'Dermatofibroma')\n",
      "  [14 | Index 14] Key: 'Drug Rash' (Raw: 'Drug Rash')\n",
      "  [15 | Index 15] Key: 'Ecthyma' (Raw: 'Ecthyma')\n",
      "  [16 | Index 16] Key: 'Eczema' (Raw: 'Eczema')\n",
      "  [17 | Index 17] Key: 'Erythema multiforme' (Raw: 'Erythema multiforme')\n",
      "  [18 | Index 18] Key: 'Folliculitis' (Raw: 'Folliculitis')\n",
      "  [19 | Index 19] Key: 'Granuloma annulare' (Raw: 'Granuloma annulare')\n",
      "  [20 | Index 20] Key: 'Herpes Simplex' (Raw: 'Herpes Simplex')\n",
      "  [21 | Index 21] Key: 'Herpes Zoster' (Raw: 'Herpes Zoster')\n",
      "  [22 | Index 22] Key: 'Hyperpigmentation' (Raw: 'Hyperpigmentation')\n",
      "  [23 | Index 23] Key: 'Hypersensitivity' (Raw: 'Hypersensitivity')\n",
      "  [24 | Index 24] Key: 'Impetigo' (Raw: 'Impetigo')\n",
      "\n",
      "--- Failing Input Check ---\n",
      "Input Check: 'Eczema' in map? True\n",
      "\n",
      "================================================================================\n",
      "STARTING FINE-GRAINED TRAINING\n",
      "================================================================================\n",
      "Training with fine-grained condition labels...\n",
      "Focus on discriminating between 66 specific conditions\n",
      "\n",
      "[SCHEDULE] step=0 | alpha=0.0000 beta=0.0000 gamma=0.0000\n",
      "[Step 0] L_total=0.705592 | L_siglip=0.705592 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=100 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 100] L_total=0.600375 | L_siglip=0.600375 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=200 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 200] L_total=0.402366 | L_siglip=0.402366 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=300 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 300] L_total=0.378495 | L_siglip=0.378495 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=400 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 400] L_total=0.372511 | L_siglip=0.372511 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=500 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 500] L_total=0.370727 | L_siglip=0.370727 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=600 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 600] L_total=0.369477 | L_siglip=0.369477 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=700 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 700] L_total=0.369756 | L_siglip=0.369756 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=800 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 800] L_total=0.369466 | L_siglip=0.369466 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=900 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 900] L_total=0.368779 | L_siglip=0.368779 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1000 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1000] L_total=0.370172 | L_siglip=0.370172 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1100 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1100] L_total=0.369934 | L_siglip=0.369934 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1200 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1200] L_total=0.369118 | L_siglip=0.369118 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1300 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1300] L_total=0.370719 | L_siglip=0.370719 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1400 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1400] L_total=0.369442 | L_siglip=0.369442 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1500 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1500] L_total=0.369282 | L_siglip=0.369282 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1600 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1600] L_total=0.371023 | L_siglip=0.371023 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1700 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1700] L_total=0.369365 | L_siglip=0.369365 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1800 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1800] L_total=0.368625 | L_siglip=0.368625 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=1900 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 1900] L_total=0.370461 | L_siglip=0.370461 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2000 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2000] L_total=0.370178 | L_siglip=0.370178 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2100 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2100] L_total=0.369583 | L_siglip=0.369583 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2200 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2200] L_total=0.369803 | L_siglip=0.369803 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2300 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2300] L_total=0.369598 | L_siglip=0.369598 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2400 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2400] L_total=0.369156 | L_siglip=0.369156 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "[SCHEDULE] step=2500 | alpha=0.0500 beta=1.0000 gamma=1.0000\n",
      "[Step 2500] L_total=0.368942 | L_siglip=0.368942 | L_icd=0.000000 | L_mmd=0.000000 | L_hardneg=0.000000 | alpha=0.0000, beta=0.0000, gamma=0.0000\n",
      "\n",
      "‚úì Training complete! Final loss: 0.3867\n",
      "Test Loss: 0.2620\n",
      "‚úì Test loss: 0.2620\n",
      "‚úì Model saved to: siglip_lora_sigmoid_202511221028/final_model\n",
      "\n",
      "================================================================================\n",
      "Testing Fine-Grained Retrieval\n",
      "================================================================================\n",
      "Testing retrieval on 20 samples for R@5 accuracy...\n",
      "\n",
      "================================================================================\n",
      "DEBUG SAMPLE 1 | Condition: Psoriasis\n",
      "Image Path: /home/agentic-health/data/scin/images/-4241920756193111728.png\n",
      "Result (R@5): ‚ùå INCORRECT\n",
      "TRUE LABEL RANK: 9th\n",
      "----------------------------------------\n",
      "Model's Top 5 Guesses:\n",
      "  [0.2555] A dermatological image of Actinic Keratosis: Skin condition diagnosed as Actinic Keratosis \n",
      "  [0.2268] A dermatological image of Perioral Dermatitis: Skin condition diagnosed as Perioral Dermatitis \n",
      "  [0.2220] A dermatological image of Granuloma annulare: Skin condition diagnosed as Granuloma annulare \n",
      "  [0.2143] A dermatological image of Scabies: A dermatological image showing Scabies \n",
      "  [0.2044] A dermatological image of Scar Condition: Medical photograph of Scar Condition \n",
      "  ...\n",
      "  [0.2024] A dermatological image of Psoriasis: Patient presenting with Psoriasis üëà TRUE LABEL (Actual Rank: 9th)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEBUG SAMPLE 2 | Condition: Pityriasis rosea\n",
      "Image Path: /home/agentic-health/data/scin/images/6297023132503235706.png\n",
      "Result (R@5): ‚ùå INCORRECT\n",
      "TRUE LABEL RANK: 7th\n",
      "----------------------------------------\n",
      "Model's Top 5 Guesses:\n",
      "  [0.2596] A dermatological image of Ecthyma: Medical photograph of Ecthyma \n",
      "  [0.2497] A dermatological image of Herpes Zoster: Medical photograph of Herpes Zoster \n",
      "  [0.2493] A dermatological image of Granuloma annulare: A dermatological image showing Granuloma annulare \n",
      "  [0.2459] A dermatological image of Erythema multiforme: Dermatoscopy image showing Erythema multiforme \n",
      "  [0.2438] A dermatological image of Allergic Contact Dermatitis: A dermatological image showing Allergic Conta... \n",
      "  ...\n",
      "  [0.2364] A dermatological image of Pityriasis rosea: Skin lesion consistent with Pityriasis rosea üëà TRUE LABEL (Actual Rank: 7th)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEBUG SAMPLE 3 | Condition: Allergic Contact Dermatitis\n",
      "Image Path: /home/agentic-health/data/scin/images/-3221416279462754613.png\n",
      "Result (R@5): ‚úÖ CORRECT\n",
      "TRUE LABEL RANK: 3th\n",
      "----------------------------------------\n",
      "Model's Top 5 Guesses:\n",
      "  [0.2404] A dermatological image of Lichen planus/lichenoid eruption: Medical photograph of Lichen planus/lich... \n",
      "  [0.2403] A dermatological image of Erythema multiforme: A dermatological image showing Erythema multiforme \n",
      "  [0.2249] A dermatological image of Allergic Contact Dermatitis: Clinical presentation of Allergic Contact Der... üëà TRUE LABEL\n",
      "  [0.2161] A dermatological image of Cutaneous lupus: Skin lesion consistent with Cutaneous lupus \n",
      "  [0.2037] A dermatological image of Tinea: Medical photograph of Tinea \n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEBUG SAMPLE 4 | Condition: Urticaria\n",
      "Image Path: /home/agentic-health/data/scin/images/-8019240739148978384.png\n",
      "Result (R@5): ‚ùå INCORRECT\n",
      "TRUE LABEL RANK: 8th\n",
      "----------------------------------------\n",
      "Model's Top 5 Guesses:\n",
      "  [0.2608] A dermatological image of CD - Contact dermatitis: A dermatological image showing CD - Contact derma... \n",
      "  [0.2422] A dermatological image of Abrasion, scrape, or scab: A dermatological image showing Abrasion, scrape... \n",
      "  [0.2295] A dermatological image of Leukocytoclastic Vasculitis: Patient presenting with Leukocytoclastic Vasc... \n",
      "  [0.2246] A dermatological image of Acute and chronic dermatitis: Dermatoscopy image showing Acute and chronic... \n",
      "  [0.2195] A dermatological image of Acute dermatitis, NOS: Skin lesion consistent with Acute dermatitis, NOS \n",
      "  ...\n",
      "  [0.2055] A dermatological image of Urticaria: Skin condition diagnosed as Urticaria üëà TRUE LABEL (Actual Rank: 8th)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEBUG SAMPLE 5 | Condition: Insect Bite\n",
      "Image Path: /home/agentic-health/data/scin/images/-1844222098674246997.png\n",
      "Result (R@5): ‚ùå INCORRECT\n",
      "TRUE LABEL RANK: 10th\n",
      "----------------------------------------\n",
      "Model's Top 5 Guesses:\n",
      "  [0.2566] A dermatological image of Kaposi's sarcoma of skin: A dermatological image showing Kaposi's sarcoma ... \n",
      "  [0.2407] A dermatological image of Erythema multiforme: A dermatological image showing Erythema multiforme \n",
      "  [0.2380] A dermatological image of Irritant Contact Dermatitis: Clinical presentation of Irritant Contact Der... \n",
      "  [0.2366] A dermatological image of Abrasion, scrape, or scab: A dermatological image showing Abrasion, scrape... \n",
      "  [0.2365] A dermatological image of Leukocytoclastic Vasculitis: Skin lesion consistent with Leukocytoclastic ... \n",
      "  ...\n",
      "  [0.2067] A dermatological image of Insect Bite: Patient presenting with Insect Bite üëà TRUE LABEL (Actual Rank: 10th)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úì Fine-grained retrieval accuracy (R@5): 30.00%\n",
      "\n",
      "================================================================================\n",
      "FINE-GRAINED TRAINING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final metrics:\n",
      "  Test loss: 0.2620\n",
      "  Retrieval accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Train SigLIP.\n",
    "Uses the 66 specific condition labels with their descriptions for more precise matching.\n",
    "\n",
    "This approach should provide:\n",
    "- Better discrimination between specific conditions\n",
    "- More precise retrieval accuracy\n",
    "- Direct condition-level matching without hierarchical complexity\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Get the current timestamp\n",
    "now = datetime.now()\n",
    "\n",
    "timestamp_str = now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoProcessor,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model\n",
    "    model_name: str = 'google/siglip-base-patch16-224'\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 15\n",
    "    learning_rate: float = 1e-5 # 4e-5 seems like a good sweet spot.\n",
    "    weight_decay: float = 0.01  #Was 5e-02 or 0.05. High decay prevents overfitting.\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    #Lora parameters\n",
    "    lora_rank: int = 32\n",
    "    lora_alpha: int = 32\n",
    "\n",
    "    # Device\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    fp16: bool = torch.cuda.is_available()\n",
    "    temperature: float = 0.7\n",
    "    \n",
    "    # Set this to \"contrastive\" or \"sigmoid\"\n",
    "    loss_type: str = \"sigmoid\" # or \"contrastive\"\n",
    "    \n",
    "    # finetune_mode. # Options: \"FULL\", \"LoRA\"\n",
    "    finetune_mode: str = \"LoRA\"  \n",
    "    \n",
    "    # Data paths\n",
    "    log_dir: Path = Path('./runs')\n",
    "    data_dir: Path = Path('/home/agentic-health/data')\n",
    "    visual_descriptions_path: Path ='./data/condition_visual_descriptions.json'\n",
    "    metadata_file: Path = './data/coarse_labeled_metadata_with_labels.csv'\n",
    "    images_dir: Path = data_dir / 'scin/images'\n",
    "    ICD_DISSIMILARITY_MATRIX: Path = Path('./data/icd_dissimilarity_matrix.pt')\n",
    "    CONDITION_IDX_MAP: Path = Path('./data/condition_idx_map.json')\n",
    "    output_dir: Path = Path(f'./siglip_{finetune_mode.lower()}_{loss_type.lower()}_{timestamp_str}')\n",
    "\n",
    "config = Config()\n",
    "config.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class DualLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filepath, \"w\")  # Overwrite file each run\n",
    "        # Store original stdout before any redirection\n",
    "        self._original_stdout = sys.__stdout__\n",
    "\n",
    "    def write(self, message):\n",
    "        # Write to Jupyter's display system\n",
    "        self._original_stdout.write(message)\n",
    "        self._original_stdout.flush()\n",
    "        \n",
    "        # Also write to file\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self._original_stdout.flush()\n",
    "        self.log.flush()\n",
    "        \n",
    "log_file_path = config.output_dir / \"training_log.txt\"\n",
    "sys.stdout = DualLogger(log_file_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"SIGLIP {config.loss_type} LEARNING USLING {config.finetune_mode}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Output: {config.output_dir}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Weight decay: {config.weight_decay}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Lora_rank: {config.lora_rank}\")   \n",
    "print(f\"  Output directory: {config.output_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_csv(config.metadata_file)\n",
    "print(f\"Total real samples in {config.metadata_file}: {len(df)}\")\n",
    "print(f\"conditions: {df['condition'].nunique()}\")\n",
    "print(f\"Coarse categories: {df['coarse_category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b35afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_item(item):\n",
    "    \"\"\"\n",
    "        Extract label from a dataset item (dictionary row).\n",
    "        We prefer the 'description' column if available, otherwise template the condition\n",
    "    \"\"\"   \n",
    "    if pd.notna(item['description']):\n",
    "        text = item['description']\n",
    "    else:\n",
    "        # Fallback template if description is missing\n",
    "        text = f\"A dermatological photo showing {item['condition']}\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def stratified_split_data(all_data, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Splits data while ensuring rare classes are distributed across all sets.\n",
    "    Strategy: Assigns rare classes first to ensure Val/Test coverage.\n",
    "    \"\"\"\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "    \n",
    "    # 1. Map each class to its samples\n",
    "    class_to_samples = defaultdict(list)\n",
    "    sample_to_id = {} # Track assigned samples\n",
    "    \n",
    "    for idx, item in enumerate(all_data):\n",
    "        labels = get_labels_from_item(item)\n",
    "        sample_to_id[idx] = False # False = not assigned\n",
    "        for label in labels:\n",
    "            class_to_samples[label].append(idx)\n",
    "            \n",
    "    # 2. Sort classes by rarity (rarest first)\n",
    "    sorted_classes = sorted(class_to_samples.keys(), key=lambda k: len(class_to_samples[k]))\n",
    "    \n",
    "    for cls in sorted_classes:\n",
    "        indices = class_to_samples[cls]\n",
    "        # Filter for unassigned indices\n",
    "        available_indices = [i for i in indices if not sample_to_id[i]]\n",
    "        random.shuffle(available_indices)\n",
    "        \n",
    "        n = len(available_indices)\n",
    "        # Ensure at least 1 sample in Val/Test if possible\n",
    "        n_val = max(1, int(n * val_ratio)) if n > 1 else 0\n",
    "        n_test = max(1, int(n * test_ratio)) if n > 2 else 0\n",
    "        \n",
    "        # Allocate indices\n",
    "        val_idxs = available_indices[:n_val]\n",
    "        test_idxs = available_indices[n_val:n_val+n_test]\n",
    "        train_idxs = available_indices[n_val+n_test:]\n",
    "        \n",
    "        for i in val_idxs:\n",
    "            if not sample_to_id[i]:\n",
    "                val_data.append(all_data[i])\n",
    "                sample_to_id[i] = True\n",
    "        for i in test_idxs:\n",
    "            if not sample_to_id[i]:\n",
    "                test_data.append(all_data[i])\n",
    "                sample_to_id[i] = True\n",
    "        for i in train_idxs:\n",
    "            if not sample_to_id[i]:\n",
    "                train_data.append(all_data[i])\n",
    "                sample_to_id[i] = True\n",
    "\n",
    "    # 3. Assign any leftovers\n",
    "    leftovers = [all_data[i] for i in range(len(all_data)) if not sample_to_id[i]]\n",
    "    for item in leftovers:\n",
    "        r = random.random()\n",
    "        if r < val_ratio: val_data.append(item)\n",
    "        elif r < val_ratio + test_ratio: test_data.append(item)\n",
    "        else: train_data.append(item)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def balance_training_data(train_data, target_min_samples=30):\n",
    "    \"\"\"\n",
    "    Oversamples rare classes in the training set to reach a minimum count.\n",
    "    \"\"\"\n",
    "    print(f\"\\nBalancing training data (target min: {target_min_samples})...\")\n",
    "    class_counts = Counter()\n",
    "    class_samples = defaultdict(list)\n",
    "    \n",
    "    for item in train_data:\n",
    "        labels = get_labels_from_item(item)\n",
    "        for label in labels:\n",
    "            class_counts[label] += 1\n",
    "            class_samples[label].append(item)\n",
    "            \n",
    "    balanced_data = list(train_data)\n",
    "    added_count = 0\n",
    "    \n",
    "    for cls, count in class_counts.items():\n",
    "        if count < target_min_samples:\n",
    "            needed = target_min_samples - count\n",
    "            available = class_samples[cls]\n",
    "            if not available: continue\n",
    "            \n",
    "            # Resample with replacement\n",
    "            extras = random.choices(available, k=needed)\n",
    "            balanced_data.extend(extras)\n",
    "            added_count += needed\n",
    "\n",
    "    random.shuffle(balanced_data)\n",
    "    print(f\"  Added {added_count} samples via oversampling.\")\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a523e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def generate_synthetic_data_pil(image, num_samples=5, use_edge_enhance=False):\n",
    "    \"\"\"\n",
    "    Creates synthetic medical images using Pillow (PIL).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image.\n",
    "        num_samples (int): Number of synthetic versions to create.\n",
    "        use_edge_enhance (bool): If True, applies edge enhancement (like Sobel).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of augmented PIL images.\n",
    "    \"\"\"\n",
    "    synthetic_images = []\n",
    "    w, h = image.size\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        aug_img = image.copy()\n",
    "        \n",
    "        # --- A. GEOMETRIC TRANSFORMS ---\n",
    "        \n",
    "        # 1. Random Flip\n",
    "        if random.random() > 0.5:\n",
    "            # Mirror (Horizontal)\n",
    "            if random.random() > 0.5:\n",
    "                aug_img = ImageOps.mirror(aug_img)\n",
    "            # Flip (Vertical) - Valid for skin lesions\n",
    "            else:\n",
    "                aug_img = ImageOps.flip(aug_img)\n",
    "\n",
    "        # 2. Random Rotation (-30 to 30 degrees)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-30, 30)\n",
    "            aug_img = aug_img.rotate(angle, resample=Image.BICUBIC, expand=False)\n",
    "\n",
    "        # 3. Random Resized Crop (Zoom effect)\n",
    "        if random.random() > 0.3:\n",
    "            scale = random.uniform(0.75, 0.95)\n",
    "            new_w, new_h = int(w * scale), int(h * scale)\n",
    "            \n",
    "            left = random.randint(0, w - new_w)\n",
    "            top = random.randint(0, h - new_h)\n",
    "            \n",
    "            aug_img = aug_img.crop((left, top, left + new_w, top + new_h))\n",
    "            aug_img = aug_img.resize((w, h), resample=Image.BICUBIC)\n",
    "\n",
    "        # --- B. PIXEL TRANSFORMS ---\n",
    "\n",
    "        # 4. Gaussian Blur\n",
    "        if random.random() > 0.7:\n",
    "            radius = random.uniform(1, 2)\n",
    "            aug_img = aug_img.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "        # 5. Gaussian Noise\n",
    "        if random.random() > 0.7:\n",
    "            # Convert to numpy to add noise\n",
    "            img_arr = np.array(aug_img).astype(float)\n",
    "            noise = np.random.normal(0, 15, img_arr.shape)\n",
    "            img_arr = np.clip(img_arr + noise, 0, 255).astype('uint8')\n",
    "            aug_img = Image.fromarray(img_arr)\n",
    "\n",
    "        # 6. Color Jitter (Brightness/Contrast/Saturation)\n",
    "        if random.random() > 0.5:\n",
    "            # Brightness\n",
    "            enhancer = ImageEnhance.Brightness(aug_img)\n",
    "            aug_img = enhancer.enhance(random.uniform(0.8, 1.2))\n",
    "            \n",
    "            # Contrast\n",
    "            enhancer = ImageEnhance.Contrast(aug_img)\n",
    "            aug_img = enhancer.enhance(random.uniform(0.9, 1.1))\n",
    "            \n",
    "            # Saturation\n",
    "            enhancer = ImageEnhance.Color(aug_img)\n",
    "            aug_img = enhancer.enhance(random.uniform(0.9, 1.1))\n",
    "\n",
    "        # --- C. STRUCTURAL TRANSFORMS ---\n",
    "        \n",
    "        # 7. Edge Enhancement (Alternative to Sobel)\n",
    "        if use_edge_enhance and random.random() > 0.8:\n",
    "            # Convert to grayscale and find edges\n",
    "            gray = aug_img.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n",
    "            # Convert back to RGB\n",
    "            edge_img = gray.convert(\"RGB\")\n",
    "            # Blend with original (50%)\n",
    "            aug_img = Image.blend(aug_img, edge_img, alpha=0.5)\n",
    "\n",
    "        synthetic_images.append(aug_img)\n",
    "\n",
    "    return synthetic_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c609f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "def augment_rare_classes(df, images_dir, output_dir, target_count=30):\n",
    "    \"\"\"\n",
    "    Identifies rare classes and generates synthetic images to reach target_count.\n",
    "    Skips generation if sufficient synthetic data already exists in output_dir.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Original training dataframe.\n",
    "        images_dir (str/Path): Directory containing original images.\n",
    "        output_dir (str/Path): Directory to save synthetic images and metadata.\n",
    "        target_count (int): Minimum samples desired per class.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing ALL synthetic data (previously existing + newly generated).\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Load existing synthetic metadata if it exists\n",
    "    metadata_path = output_path / \"synthetic_metadata.csv\"\n",
    "    if metadata_path.exists():\n",
    "        try:\n",
    "            existing_syn_df = pd.read_csv(metadata_path)\n",
    "            print(f\"Loaded metadata for {len(existing_syn_df)} existing synthetic samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read existing metadata ({e}). Starting fresh.\")\n",
    "            existing_syn_df = pd.DataFrame()\n",
    "    else:\n",
    "        existing_syn_df = pd.DataFrame()\n",
    "\n",
    "    # Identify rare classes based on ORIGINAL data\n",
    "    counts = df['condition'].value_counts()\n",
    "    rare_classes = counts[counts < target_count].index.tolist()\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    print(f\"Checking {len(rare_classes)} rare classes (Target: {target_count})...\")\n",
    "\n",
    "    for condition in rare_classes:\n",
    "        # Count original samples\n",
    "        original_count = len(df[df['condition'] == condition])\n",
    "        \n",
    "        # Count existing synthetic samples (if any)\n",
    "        if not existing_syn_df.empty:\n",
    "            syn_count = len(existing_syn_df[existing_syn_df['condition'] == condition])\n",
    "        else:\n",
    "            syn_count = 0\n",
    "            \n",
    "        # Calculate how many MORE we need\n",
    "        total_current = original_count + syn_count\n",
    "        needed = target_count - total_current\n",
    "        \n",
    "        if needed <= 0:\n",
    "            # print(f\"  ‚úì {condition}: Has {total_current} (Skipping)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  - {condition}: Generating {needed} new images (Original: {original_count}, Existing Syn: {syn_count})...\")\n",
    "        \n",
    "        # Get existing samples to augment\n",
    "        existing_samples = df[df['condition'] == condition]\n",
    "        if existing_samples.empty: continue\n",
    "\n",
    "        generated_count = 0\n",
    "        while generated_count < needed:\n",
    "            # Pick random source image\n",
    "            source_row = existing_samples.sample(1).iloc[0]\n",
    "            \n",
    "            img_path = Path(images_dir) / source_row['image_path']\n",
    "            try:\n",
    "                original_img = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # Generate synthetic version\n",
    "            # Assuming generate_synthetic_data_pil is defined as before\n",
    "            synthetic_imgs = generate_synthetic_data_pil(original_img, num_samples=1)\n",
    "            syn_img = synthetic_imgs[0]\n",
    "            \n",
    "            # --- METADATA ---\n",
    "            new_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Sanitize filename\n",
    "            safe_condition_name = condition.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "            new_filename = f\"syn_{safe_condition_name}_{new_id[:8]}.png\"\n",
    "            save_path = output_path / new_filename\n",
    "            \n",
    "            # Save Image\n",
    "            syn_img.save(save_path)\n",
    "            \n",
    "            # Create Metadata Row\n",
    "            new_row = source_row.copy()\n",
    "            new_row['image_id'] = new_id\n",
    "            new_row['image_path'] = new_filename # Just filename, relative to output_dir\n",
    "            new_row['split'] = 'train'\n",
    "            \n",
    "            # Add helper flag (optional)\n",
    "            new_row['is_synthetic'] = True\n",
    "            \n",
    "            new_rows.append(new_row)\n",
    "            generated_count += 1\n",
    "\n",
    "    # 2. Combine and Save\n",
    "    if new_rows:\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        if not existing_syn_df.empty:\n",
    "            # Align columns before concat to prevent errors\n",
    "            # (Optional safety step if schemas drift)\n",
    "            final_syn_df = pd.concat([existing_syn_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            final_syn_df = new_df\n",
    "            \n",
    "        # Save updated metadata\n",
    "        final_syn_df.to_csv(metadata_path, index=False)\n",
    "        print(f\"Saved updated metadata with {len(new_rows)} new samples to {metadata_path}\")\n",
    "        return final_syn_df\n",
    "    else:\n",
    "        print(\"No new images generated.\")\n",
    "        return existing_syn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1d3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  IMPROVED DATA LOADING SECTION (Replaces original loading logic)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading and Processing Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Load raw CSV\n",
    "df = pd.read_csv(config.metadata_file)\n",
    "print(f\"Raw data loaded: {len(df)} samples\")\n",
    "\n",
    "# 2. Convert to list of dictionaries for processing\n",
    "all_data = df.to_dict('records')\n",
    "\n",
    "# 3. Perform Stratified Split (Fixes missing classes in Val/Test)\n",
    "train_raw, val_data, test_data = stratified_split_data(\n",
    "    all_data, \n",
    "    val_ratio=0.15, \n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "# 4. Balance Training Data (Fixes class imbalance)\n",
    "# Target 30 samples ensures the model sees rare classes ~1 per batch on average\n",
    "train_balanced = balance_training_data(train_raw, target_min_samples=30)\n",
    "\n",
    "# 5. Convert back to DataFrames for compatibility\n",
    "train_df = pd.DataFrame(train_balanced)\n",
    "print(f\"Training set size before adding synthetic data: {len(train_df)}\")\n",
    "synthetic_dir = \"./scin_synthetic\"\n",
    "synthetic_df = augment_rare_classes(\n",
    "    train_df, \n",
    "    config.images_dir, \n",
    "    synthetic_dir, \n",
    "    target_count=30\n",
    ")\n",
    "\n",
    "# Merge\n",
    "train_df = pd.concat([train_df, synthetic_df], ignore_index=True)\n",
    "print(f\"New training set size after adding synthetic data: {len(train_df)}\")\n",
    "\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c556acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def print_comprehensive_report(df, train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    Print a comprehensive data analysis report for SigLip fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\" DATASET ANALYSIS FOR SIGLIP FINE-TUNING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n1. OVERALL DATASET STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total samples: {len(df):,}\")\n",
    "    print(f\"  - Training:   {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  - Validation: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  - Test:       {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nUnique conditions: {df['condition'].nunique()}\")\n",
    "    print(f\"Unique coarse categories: {df['coarse_category'].nunique()}\")\n",
    "    \n",
    "    # Label distribution by split\n",
    "    print(\"\\n2. LABEL DISTRIBUTION BY SPLIT\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get all unique conditions\n",
    "    all_conditions = df['condition'].unique()\n",
    "    \n",
    "    # Create distribution table\n",
    "    distribution_data = []\n",
    "    for condition in sorted(all_conditions):\n",
    "        train_count = len(train_df[train_df['condition'] == condition])\n",
    "        val_count = len(val_df[val_df['condition'] == condition])\n",
    "        test_count = len(test_df[test_df['condition'] == condition])\n",
    "        total = train_count + val_count + test_count\n",
    "        \n",
    "        distribution_data.append({\n",
    "            'Condition': condition,\n",
    "            'Train': train_count,\n",
    "            'Val': val_count,\n",
    "            'Test': test_count,\n",
    "            'Total': total\n",
    "        })\n",
    "    \n",
    "    dist_df = pd.DataFrame(distribution_data).sort_values('Total', ascending=False)\n",
    "    \n",
    "    # Print top 15 most common conditions\n",
    "    print(\"\\nTop 15 Most Common Conditions:\")\n",
    "    print(f\"{'Condition':<40} {'Train':>8} {'Val':>8} {'Test':>8} {'Total':>8}\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in dist_df.head(15).iterrows():\n",
    "        print(f\"{row['Condition']:<40} {row['Train']:>8} {row['Val']:>8} {row['Test']:>8} {row['Total']:>8}\")\n",
    "    \n",
    "    # Print conditions with imbalanced splits (potential issues)\n",
    "    print(\"\\n3. POTENTIAL TRAINING ISSUES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Classes with very few samples\n",
    "    rare_classes = dist_df[dist_df['Total'] < 10]\n",
    "    print(f\"\\n‚ö†Ô∏è  Conditions with <10 total samples: {len(rare_classes)}\")\n",
    "    if len(rare_classes) > 0:\n",
    "        print(\"(These may cause overfitting or poor generalization)\")\n",
    "        for _, row in rare_classes.iterrows():\n",
    "            print(f\"  - {row['Condition']:<40} (Total: {row['Total']})\")\n",
    "    \n",
    "    # Classes missing from validation or test\n",
    "    missing_val = dist_df[dist_df['Val'] == 0]\n",
    "    missing_test = dist_df[dist_df['Test'] == 0]\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Conditions missing from validation set: {len(missing_val)}\")\n",
    "    if len(missing_val) > 0 and len(missing_val) <= 10:\n",
    "        for _, row in missing_val.iterrows():\n",
    "            print(f\"  - {row['Condition']}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Conditions missing from test set: {len(missing_test)}\")\n",
    "    if len(missing_test) > 0 and len(missing_test) <= 10:\n",
    "        for _, row in missing_test.iterrows():\n",
    "            print(f\"  - {row['Condition']}\")\n",
    "    \n",
    "    # Multi-label analysis\n",
    "    print(\"\\n4. MULTI-LABEL ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    def count_labels(df_subset):\n",
    "        \"\"\"Count images by number of labels, safely evaluating string representations.\"\"\"\n",
    "        import ast\n",
    "        label_counts = []\n",
    "        \n",
    "        for all_conds in df_subset['all_conditions']:\n",
    "            num_labels = 1 # Default to 1 label if we encounter valid data but no list structure\n",
    "            \n",
    "            # 1. Check if the value is already a list/tuple (Path A)\n",
    "            if isinstance(all_conds, (list, tuple)):\n",
    "                num_labels = len(all_conds)\n",
    "                \n",
    "            # 2. Check if the value is a string (Path B - The source of the bug)\n",
    "            elif isinstance(all_conds, str) and all_conds.strip():\n",
    "                try:\n",
    "                    # Safely convert the string representation of the list\n",
    "                    conds = ast.literal_eval(all_conds)\n",
    "                    \n",
    "                    if isinstance(conds, (list, tuple)):\n",
    "                        num_labels = len(conds)\n",
    "                        \n",
    "                except:\n",
    "                    # If ast.literal_eval fails (e.g., malformed string), \n",
    "                    # we maintain the default assumption of 1 label.\n",
    "                    pass\n",
    "            \n",
    "            # 3. Final Check: If filtering failed somewhere and we hit 0, revert to 1 (Adhering to previous constraints)\n",
    "            if num_labels == 0:\n",
    "                num_labels = 1\n",
    "                \n",
    "            label_counts.append(num_labels)\n",
    "                \n",
    "        return Counter(label_counts)\n",
    "    \n",
    "    train_label_dist = count_labels(train_df)\n",
    "    val_label_dist = count_labels(val_df)\n",
    "    test_label_dist = count_labels(test_df)\n",
    "    \n",
    "    print(f\"{'Labels per Image':<20} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    all_label_counts = sorted(set(list(train_label_dist.keys()) + list(val_label_dist.keys()) + list(test_label_dist.keys())))\n",
    "    for num_labels in all_label_counts:\n",
    "        print(f\"{num_labels} label(s): {train_label_dist.get(num_labels, 0):>10} {val_label_dist.get(num_labels, 0):>10} {test_label_dist.get(num_labels, 0):>10}\")\n",
    "    \n",
    "    # Calculate multi-label percentage\n",
    "    multi_label_train = sum(count for labels, count in train_label_dist.items() if labels > 1)\n",
    "    multi_label_pct = multi_label_train / len(train_df) * 100 if len(train_df) > 0 else 0\n",
    "    print(f\"\\nMulti-label samples in training: {multi_label_train:,} ({multi_label_pct:.1f}%)\")\n",
    "    \n",
    "    # Coarse category distribution\n",
    "    print(\"\\n5. COARSE CATEGORY DISTRIBUTION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Category':<40} {'Train':>8} {'Val':>8} {'Test':>8} {'Total':>8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for category in sorted(df['coarse_category'].unique()):\n",
    "        train_count = len(train_df[train_df['coarse_category'] == category])\n",
    "        val_count = len(val_df[val_df['coarse_category'] == category])\n",
    "        test_count = len(test_df[test_df['coarse_category'] == category])\n",
    "        total = train_count + val_count + test_count\n",
    "        print(f\"{category:<40} {train_count:>8} {val_count:>8} {test_count:>8} {total:>8}\")\n",
    "    \n",
    "    # Class imbalance metrics\n",
    "    print(\"\\n6. CLASS IMBALANCE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    train_condition_counts = train_df['condition'].value_counts()\n",
    "    max_samples = train_condition_counts.max()\n",
    "    min_samples = train_condition_counts.min()\n",
    "    imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Training set class imbalance:\")\n",
    "    print(f\"  - Most common class:  {train_condition_counts.index[0]:<40} ({max_samples} samples)\")\n",
    "    print(f\"  - Least common class: {train_condition_counts.index[-1]:<40} ({min_samples} samples)\")\n",
    "    print(f\"  - Imbalance ratio:    {imbalance_ratio:.1f}:1\")\n",
    "    print(f\"\\nMedian samples per class: {train_condition_counts.median():.0f}\")\n",
    "    print(f\"Mean samples per class:   {train_condition_counts.mean():.1f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n7. RECOMMENDATIONS FOR SIGLIP FINE-TUNING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä Data Considerations:\")\n",
    "    if imbalance_ratio > 100:\n",
    "        print(\"  ‚ö†Ô∏è  SEVERE class imbalance detected (>100:1)\")\n",
    "        print(\"     ‚Üí Consider: class weights, focal loss, or oversampling rare classes\")\n",
    "    elif imbalance_ratio > 20:\n",
    "        print(\"  ‚ö†Ô∏è  Significant class imbalance (>20:1)\")\n",
    "        print(\"     ‚Üí Consider: balanced sampling or class weights\")\n",
    "    \n",
    "    if multi_label_pct > 20:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  High multi-label percentage ({multi_label_pct:.1f}%)\")\n",
    "        print(\"     ‚Üí Use SIGMOID loss (not contrastive) for multi-label classification\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Low multi-label percentage ({multi_label_pct:.1f}%)\")\n",
    "        print(\"     ‚Üí Either SIGMOID or CONTRASTIVE loss can work\")\n",
    "    \n",
    "    if len(rare_classes) > 0:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  {len(rare_classes)} classes with <10 samples\")\n",
    "        print(\"     ‚Üí These may not learn well; consider grouping into coarse categories\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Call the report function\n",
    "print_comprehensive_report(df, train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67e98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json # Ensure this is imported at the top of your file\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for SigLIP fine-tuning, incorporating ICD indices and verbose descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, images_dir: Path, processor, mode='train', \n",
    "                 visual_descriptions_path=None, icd_idx_map_path=None): # Accept map path\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "        \n",
    "        # --- 1. Load Visual Descriptions (Text Signal) ---\n",
    "        self.visual_descriptions = {}\n",
    "        if visual_descriptions_path and Path(visual_descriptions_path).exists():\n",
    "            try:\n",
    "                with open(visual_descriptions_path, 'r') as f:\n",
    "                    self.visual_descriptions = json.load(f)\n",
    "                print(f\"[{mode}] Loaded {len(self.visual_descriptions)} visual descriptions.\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # --- 2. Load ICD Index Map (Geometric Signal) ---\n",
    "        self.condition_to_idx = {}\n",
    "        if icd_idx_map_path and Path(icd_idx_map_path).exists():\n",
    "            try:\n",
    "                with open(icd_idx_map_path, 'r') as f:\n",
    "                    self.condition_to_idx = json.load(f)\n",
    "                print(f\"[{mode}] Loaded {len(self.condition_to_idx)} ICD index mappings.\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def _get_text_prompt(self, row):\n",
    "        \"\"\"Helper to generate the text prompt based on mode and description availability.\"\"\"\n",
    "        condition = row['condition']\n",
    "        \n",
    "        # Priority 1: AI-generated visual description\n",
    "        visual_desc = self.visual_descriptions.get(condition, \"\")\n",
    "        \n",
    "        # Priority 2: Fallback to dataset's raw description\n",
    "        raw_desc = str(row.get('description', '')).strip()\n",
    "        if raw_desc.lower() == 'nan' or not raw_desc:\n",
    "            raw_desc = \"\"\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Training: Prioritize the strongest visual feature for the model to learn from\n",
    "            if visual_desc:\n",
    "                text = f\"A high-resolution dermatological image of {condition}: {visual_desc}\"\n",
    "            elif raw_desc:\n",
    "                 # Fallback template for generic raw descriptions\n",
    "                text = np.random.choice([\n",
    "                    f\"Patient presenting with {condition}: {raw_desc}\",\n",
    "                    f\"Clinical image of {condition}, described as: {raw_desc}\"\n",
    "                ])\n",
    "            else:\n",
    "                text = f\"A dermatological photo of {condition}\"\n",
    "        else:\n",
    "            # Testing/Evaluation Mode: Consistent format using the best available description\n",
    "            if visual_desc:\n",
    "                text = visual_desc\n",
    "            elif raw_desc:\n",
    "                text = f\"{condition}: {raw_desc}\"\n",
    "            else:\n",
    "                text = f\"A photo of {condition}\"\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- Image/Text Loading (Omitted for brevity) ---\n",
    "        image_path = self.images_dir / row['image_path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "        text = self._get_text_prompt(row)\n",
    "        \n",
    "        # 1. Prepare and clean the condition name\n",
    "        # FIX: Ensure all whitespace is removed from the beginning/end\n",
    "        condition_name_raw = str(row['condition'])\n",
    "        lookup_key = condition_name_raw.strip()\n",
    "        \n",
    "        # --- Lookup the numerical index ---\n",
    "        # The key is now guaranteed to be clean.\n",
    "        cond_idx = self.condition_to_idx.get(lookup_key, -1) \n",
    "\n",
    "        # --- DEBUG CHECK (For verification only) ---\n",
    "        # The fact that Min/Max is -1, -1 means this check is still failing\n",
    "        if cond_idx == -1 and self.condition_to_idx:\n",
    "            print(f\"FATAL LOOKUP ERROR: Looked for '{lookup_key}', but failed.\")\n",
    "            print(f\"DEBUG: First Map Key: '{list(self.condition_to_idx.keys())[0]}'\")\n",
    "\n",
    "        # --- Processor Call ---\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 1. Remove batch dimension from all processor outputs\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].squeeze(0)\n",
    "\n",
    "        # --- CRITICAL FIX: ADD ICD ARTIFACT TO INPUTS DICTIONARY ---\n",
    "        # The ICD Loss requires this index tensor.\n",
    "        inputs[\"condition_indices\"] = torch.tensor(cond_idx, dtype=torch.long)\n",
    "        \n",
    "        # 2. Return the complete dictionary (which now includes condition_indices)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83983d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLipModel(nn.Module):\n",
    "    \"\"\"SigLIP model for contrastive learning.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.temperature = temperature\n",
    "        self.logit_bias = nn.Parameter(torch.zeros(1))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 768) \n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        image_feat = outputs.vision_model_output.pooler_output\n",
    "        image_embeds = F.normalize(self.proj(image_feat), dim=-1)\n",
    "\n",
    "        image_embeds = outputs.vision_model_output.pooler_output\n",
    "        text_embeds = outputs.text_model_output.pooler_output\n",
    "\n",
    "        image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "        text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        batch_size = image_embeds.shape[0]\n",
    "        \n",
    "        # --- Switchable Loss Calculation ---\n",
    "        if config.loss_type == \"contrastive\":\n",
    "            labels = torch.arange(batch_size, device=image_embeds.device)\n",
    "            loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "            loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "            loss = (loss_i + loss_t) / 2\n",
    "        elif config.loss_type == \"sigmoid\":\n",
    "            labels = torch.eye(batch_size, device=image_embeds.device)\n",
    "            loss_i = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "            loss_t = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "            loss = (loss_i + loss_t) / 2.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss_type: {config.loss_type}\")\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits_per_image': logits_per_image,\n",
    "            'logits_per_text': logits_per_text,\n",
    "            'image_embeds': image_embeds,\n",
    "            'text_embeds': text_embeds,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b5d1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Model and Creating Datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. Load Processor ---\n",
    "# The processor handles image resizing/normalization and text tokenization.\n",
    "processor = AutoProcessor.from_pretrained(config.model_name)\n",
    "\n",
    "# --- 2. Load Model & Weights ---\n",
    "# AutoModel detects that 'google/siglip-base-patch16-224' is a SiglipModel \n",
    "# and loads the corresponding pre-trained weights.\n",
    "base_model = SigLipModel(\n",
    "    model_name=config.model_name,\n",
    "    temperature=config.temperature\n",
    ")\n",
    "\n",
    "# --- 3. Cast and Move to Device ---\n",
    "# For stability, we explicitly cast the entire model to float32.\n",
    "# This prevents the NaN/Inf errors often seen with half-precision (fp16) during loss calculation.\n",
    "model = base_model.to(config.device).float()\n",
    "\n",
    "print(f\"‚úì Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Create datasets\n",
    "# Note: Ensure your df has 'condition' and 'description' columns\n",
    "train_dataset = CreateDataset(train_df, config.images_dir, processor, mode='train')\n",
    "val_dataset = CreateDataset(val_df, config.images_dir, processor, mode='val')\n",
    "test_dataset = CreateDataset(test_df, config.images_dir, processor, mode='test')\n",
    "\n",
    "print(f\"‚úì Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Show sample texts\n",
    "print(\"\\nSample fine-grained texts:\")\n",
    "for i in range(3):\n",
    "    sample_text = train_df.iloc[i]['description']\n",
    "    condition = train_df.iloc[i]['condition']\n",
    "    print(f\"  [{condition}] {sample_text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed509424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Production-quality SigLipTrainer\n",
    "\n",
    "Usage:\n",
    "    trainer = SigLipTrainer(..., use_temp=False, log_interval=100)\n",
    "    trainer.freeze_logit_scale()   # recommended\n",
    "    trainer.train()\n",
    "\n",
    "Notes:\n",
    "- Expects model.forward(...) to return a dict or ModelOutput containing:\n",
    "    - 'image_embeds' (B, D)\n",
    "    - 'text_embeds'  (B, D)\n",
    "    - optionally 'logits_per_image' (B, B) (not required; computed from embeddings here)\n",
    "- Optional dataset must put 'condition_indices' into inputs when you want ICD to be used.\n",
    "- Default behavior: pure SigLIP (no temperature). To experiment with temperature, set use_temp=True.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Any, Union, Optional\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Default ramp sizes (tune for your dataset)\n",
    "ICD_RAMP_STEPS = 4000\n",
    "WASS_RAMP_STEPS = 4000\n",
    "HARDNEG_RAMP_STEPS = 1000\n",
    "\n",
    "class SigLipTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        use_temp: bool = False,\n",
    "        log_interval: int = 100,\n",
    "        icd_matrix_path: Optional[str] = config.ICD_DISSIMILARITY_MATRIX,\n",
    "        condition_idx_map: Optional[str] = config.CONDITION_IDX_MAP,\n",
    "        alpha_max_influence: float = 0.05,\n",
    "        total_icd_steps: int = ICD_RAMP_STEPS,\n",
    "        total_wass_steps: int = WASS_RAMP_STEPS,\n",
    "        total_hardneg_steps: int = HARDNEG_RAMP_STEPS,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_temp: if True, use model.logit_scale (CLIP-like). Default False (pure SigLIP).\n",
    "            log_interval: how often (in steps) to print debug information.\n",
    "            icd_matrix_path: optional path to ICD dissimilarity matrix (torch.load-compatible).\n",
    "            condition_idx_map: optional json path mapping condition->index (if needed).\n",
    "            alpha_max_influence: max fraction of total loss contributed by ICD (0..1).\n",
    "            total_*_steps: durations of ramp phases for alpha, beta, gamma.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.use_temp = use_temp\n",
    "        self.log_interval = int(log_interval)\n",
    "\n",
    "        # Load ICD matrix / mapping if provided\n",
    "        self.icd_matrix = None\n",
    "        self.condition_to_idx = None\n",
    "        \n",
    "        # Get the target device (e.g., 'cuda:0')\n",
    "        target_device = self.args.device\n",
    "        \n",
    "        try:\n",
    "            if icd_matrix_path is not None:\n",
    "                # 1. Load the matrix (which is currently on CPU)\n",
    "                self.icd_matrix = torch.load(icd_matrix_path)\n",
    "                \n",
    "                # 2. Move the ENTIRE matrix to the GPU/Target Device\n",
    "                self.icd_matrix = self.icd_matrix.to(target_device)\n",
    "                \n",
    "            if condition_idx_map is not None:\n",
    "                with open(condition_idx_map, \"r\") as f:\n",
    "                    raw_map = json.load(f)\n",
    "                # Strip all whitespace from the keys before assigning them to self.condition_to_idx.\n",
    "                self.condition_to_idx = {k.strip(): v for k, v in raw_map.items()}\n",
    "                    \n",
    "            if not self.condition_to_idx:\n",
    "                print(f\"FATAL ERROR: ICD Map loaded but is EMPTY: {icd_map_path}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ ICD Map and Matrix loaded successfully.\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"FATAL ERROR: ICD Index Map NOT FOUND at {icd_map_path.resolve()}.\")\n",
    "            print(\"ACTION REQUIRED: Ensure 'icd_geometry' folder exists and files were generated.\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"FATAL ERROR: ICD Index Map is CORRUPTED: {icd_map_path}.\")        \n",
    "                \n",
    "\n",
    "        # Modulation coefficients (will be scheduled in training_step)\n",
    "        self.alpha = 0.0\n",
    "        self.beta = 0.0\n",
    "        self.gamma = 0.0\n",
    "\n",
    "        self.total_icd_steps = 0\n",
    "        self.total_wass_steps = 0\n",
    "        self.total_hardneg_steps = 0\n",
    "        \n",
    "        # Store for scheduling\n",
    "        # self.total_icd_steps = int(total_icd_steps)\n",
    "        # self.total_wass_steps = int(total_wass_steps)\n",
    "        # self.total_hardneg_steps = int(total_hardneg_steps)\n",
    "        \n",
    "        self.alpha_max_influence = float(alpha_max_influence)\n",
    "\n",
    "        # Safety: make sure alpha_max_influence in [0,1]\n",
    "        if not (0.0 <= self.alpha_max_influence <= 1.0):\n",
    "            raise ValueError(\"alpha_max_influence must be between 0 and 1\")\n",
    "\n",
    "        # For debugging/logging\n",
    "        self._last_logged_step = -1\n",
    "\n",
    "    # -----------------------\n",
    "    # Utility kernels / small helpers\n",
    "    # -----------------------\n",
    "    @staticmethod\n",
    "    def _gaussian_kernel(x: torch.Tensor, y: torch.Tensor, sigma: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"RBF kernel matrix between x and y. x: (n, d), y: (m, d) -> (n, m).\"\"\"\n",
    "        xx = (x ** 2).sum(dim=1, keepdim=True)  # (n,1)\n",
    "        yy = (y ** 2).sum(dim=1, keepdim=True)  # (m,1)\n",
    "        dist = xx + yy.t() - 2.0 * (x @ y.t())\n",
    "        return torch.exp(-dist / (2.0 * sigma ** 2))\n",
    "\n",
    "    # -----------------------\n",
    "    # SigLIP loss (pairwise sigmoid BCE)\n",
    "    # -----------------------\n",
    "    def siglip_loss(\n",
    "        self,\n",
    "        image_embeds: torch.Tensor,\n",
    "        text_embeds: torch.Tensor,\n",
    "        scale: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute SigLIP pairwise sigmoid loss and return (loss_scalar, logits_per_image, logits_per_text).\n",
    "\n",
    "        scale: scalar multiplier applied to cosine similarities. For pure SigLIP use scale=1.0.\n",
    "        If using temperature, pass model.logit_scale.exp().\n",
    "        \"\"\"\n",
    "        device = image_embeds.device\n",
    "\n",
    "        # Normalize to get cosine similarities\n",
    "        img = F.normalize(image_embeds, dim=-1)\n",
    "        txt = F.normalize(text_embeds, dim=-1)\n",
    "\n",
    "        logits_per_image = scale * (img @ txt.t())  # (B, B)\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        B = logits_per_image.size(0)\n",
    "        if B <= 1:\n",
    "            # Return a dummy loss with gradient to avoid HF errors\n",
    "            dummy = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            return dummy, logits_per_image, logits_per_text\n",
    "\n",
    "        labels = torch.eye(B, device=device)\n",
    "        # Use BCEWithLogitsLoss (stable) averaged over all entries\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_i = loss_fn(logits_per_image, labels)\n",
    "        loss_t = loss_fn(logits_per_text, labels)\n",
    "        loss = 0.5 * (loss_i + loss_t)\n",
    "        return loss, logits_per_image, logits_per_text\n",
    "\n",
    "    # -----------------------\n",
    "    # ICD structural loss (MSE between pairwise distances)\n",
    "    # -----------------------\n",
    "    def icd_loss(self, image_embeds: torch.Tensor, text_embeds: torch.Tensor, indices: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Compute ICD MSE between predicted pairwise distances and ICD D-matrix for this batch.\n",
    "\n",
    "        - predicted_distance = 1 - cosine_similarity (range [0,2])\n",
    "        - both predicted_distance and D_batch are normalized to [0,1] before computing MSE\n",
    "        - diagonal entries are ignored\n",
    "        \"\"\"\n",
    "        device = image_embeds.device\n",
    "        B = image_embeds.size(0)\n",
    "        if B <= 1:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        # predicted pairwise similarity -> distance\n",
    "        img = F.normalize(image_embeds, dim=-1)\n",
    "        txt = F.normalize(text_embeds, dim=-1)\n",
    "        pred_sim = img @ txt.t()  # (B,B)\n",
    "        pred_dist = 1.0 - pred_sim  # in [0,2]\n",
    "\n",
    "        # prepare D_batch from ICD matrix\n",
    "        if self.icd_matrix is None:\n",
    "            raise RuntimeError(\"ICD matrix was not provided but icd_loss was called\")\n",
    "\n",
    "        indices = indices.long().to(device)\n",
    "        if indices.numel() != B:\n",
    "            # mismatched mapping; skip with a warning\n",
    "            if (self.state.global_step % max(1, self.log_interval)) == 0:\n",
    "                print(f\"WARNING: icd_loss indices length ({indices.numel()}) != batch_size ({B}); skipping ICD term\")\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Index and cast to float\n",
    "        D_batch = self.icd_matrix[indices][:, indices].to(device).float()\n",
    "\n",
    "        # Normalize both to [0,1]\n",
    "        pd_min, pd_max = float(pred_dist.min().detach()), float(pred_dist.max().detach())\n",
    "        pred_norm = (pred_dist - pd_min) / (pd_max - pd_min + 1e-8)\n",
    "\n",
    "        D_min, D_max = float(D_batch.min().detach()), float(D_batch.max().detach())\n",
    "        D_norm = (D_batch - D_min) / (D_max - D_min + 1e-8)\n",
    "\n",
    "        # mask out diagonal\n",
    "        mask = ~torch.eye(B, dtype=torch.bool, device=device)\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        return F.mse_loss(pred_norm[mask], D_norm[mask])\n",
    "\n",
    "    # -----------------------\n",
    "    # MMD (Wasserstein proxy) loss\n",
    "    # -----------------------\n",
    "    def mmd_loss(self, image_embeds: torch.Tensor, text_embeds: torch.Tensor, sigma: float = 1.0):\n",
    "        device = image_embeds.device\n",
    "        B = image_embeds.size(0)\n",
    "        if B <= 1:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        Kxx = self._gaussian_kernel(image_embeds, image_embeds, sigma)\n",
    "        Kyy = self._gaussian_kernel(text_embeds, text_embeds, sigma)\n",
    "        Kxy = self._gaussian_kernel(image_embeds, text_embeds, sigma)\n",
    "        return Kxx.mean() + Kyy.mean() - 2.0 * Kxy.mean()\n",
    "\n",
    "    # -----------------------\n",
    "    # Hard negative margin-based loss\n",
    "    # -----------------------\n",
    "    def hard_negative_loss(self, image_embeds: torch.Tensor, text_embeds: torch.Tensor, margin: float = 0.2):\n",
    "        device = image_embeds.device\n",
    "        B = image_embeds.size(0)\n",
    "        if B <= 1:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        sims = image_embeds @ text_embeds.t()\n",
    "        pos_sim = sims.diag()  # (B,)\n",
    "        sims_no_pos = sims.clone()\n",
    "        sims_no_pos.fill_diagonal_(-1e9)\n",
    "        hard_neg_sim, _ = sims_no_pos.max(dim=1)  # (B,)\n",
    "        loss = F.relu(margin + hard_neg_sim - pos_sim).mean()\n",
    "        return loss\n",
    "\n",
    "    # -----------------------\n",
    "    # compute_loss (core)\n",
    "    # -----------------------\n",
    "    def compute_loss(self, model, inputs: Dict[str, Any], return_outputs: bool = False, num_items_in_batch: int = None):\n",
    "        \"\"\"\n",
    "        Compute full layered loss:\n",
    "            L_total = (1 - alpha) * L_siglip + alpha * L_icd + beta * L_mmd + gamma * L_hardneg\n",
    "\n",
    "        This function is robust to ModelOutput/dict outputs from model.forward.\n",
    "        \"\"\"\n",
    "        device = self.args.device or (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # pop condition indices if present (dataset must place them there)\n",
    "        condition_indices = inputs.pop(\"condition_indices\", None)\n",
    "        \n",
    "        # if condition_indices is not None:\n",
    "            # print(f\"DEBUG: Indices Shape: {condition_indices.shape} | Indices Dtype: {condition_indices.dtype}\")\n",
    "            # print(f\"DEBUG: Indices Min/Max: {condition_indices.min()}, {condition_indices.max()}\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract embeddings robustly\n",
    "        if isinstance(outputs, dict):\n",
    "            image_embeds = outputs.get(\"image_embeds\", None)\n",
    "            text_embeds = outputs.get(\"text_embeds\", None)\n",
    "            logits_provided = outputs.get(\"logits_per_image\", None)\n",
    "        else:\n",
    "            image_embeds = getattr(outputs, \"image_embeds\", None)\n",
    "            text_embeds = getattr(outputs, \"text_embeds\", None)\n",
    "            logits_provided = getattr(outputs, \"logits_per_image\", None)\n",
    "\n",
    "        if image_embeds is None or text_embeds is None:\n",
    "            raise RuntimeError(\"Model forward must return 'image_embeds' and 'text_embeds'\")\n",
    "\n",
    "        B = image_embeds.size(0)\n",
    "        if B <= 1:\n",
    "            dummy = torch.tensor(0.0, device=image_embeds.device, requires_grad=True)\n",
    "            return (dummy, outputs) if return_outputs else dummy\n",
    "\n",
    "        # optionally apply small dropout on embeddings to regularize\n",
    "        image_embeds = F.dropout(image_embeds, p=0.1, training=self.model.training)\n",
    "        text_embeds = F.dropout(text_embeds, p=0.1, training=self.model.training)\n",
    "\n",
    "        # Determine scale for SigLIP: either 1.0 (pure) or provided logit_scale\n",
    "        if self.use_temp and hasattr(model, \"logit_scale\"):\n",
    "            # Use scalar value (clamped to avoid massive BCE)\n",
    "            scale = float(model.logit_scale.exp().clamp(max=100.0))\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        # -------------------------\n",
    "        # SigLIP baseline\n",
    "        # -------------------------\n",
    "        loss_siglip, logits_per_image, logits_per_text = self.siglip_loss(image_embeds, text_embeds, scale)\n",
    "\n",
    "        # -------------------------\n",
    "        # ICD structural loss (alpha)\n",
    "        # -------------------------\n",
    "        loss_icd = torch.tensor(0.0, device=image_embeds.device)\n",
    "        if (self.alpha > 0.0) and (condition_indices is not None) and (self.icd_matrix is not None):\n",
    "            # Condition indices may be list or tensor\n",
    "            if isinstance(condition_indices, torch.Tensor):\n",
    "                indices = condition_indices\n",
    "            else:\n",
    "                indices = torch.tensor(condition_indices, device=image_embeds.device)\n",
    "            if indices.numel() == B:\n",
    "                loss_icd = self.icd_loss(image_embeds, text_embeds, indices)\n",
    "            else:\n",
    "                # warn occasionally\n",
    "                if (self.state.global_step % max(1, self.log_interval)) == 0:\n",
    "                    print(f\"WARNING: ICD indices length mismatch ({indices.numel()} != {B}) ‚Äî skipping ICD term\")\n",
    "\n",
    "        # -------------------------\n",
    "        # MMD/Wasserstein proxy (beta)\n",
    "        # -------------------------\n",
    "        loss_mmd = torch.tensor(0.0, device=image_embeds.device)\n",
    "        if getattr(self, \"beta\", 0.0) > 0.0:\n",
    "            loss_mmd = self.mmd_loss(image_embeds, text_embeds, sigma=1.0)\n",
    "\n",
    "        # -------------------------\n",
    "        # Hard negative (gamma)\n",
    "        # -------------------------\n",
    "        loss_hardneg = torch.tensor(0.0, device=image_embeds.device)\n",
    "        if getattr(self, \"gamma\", 0.0) > 0.0:\n",
    "            loss_hardneg = self.hard_negative_loss(image_embeds, text_embeds, margin=0.2)\n",
    "\n",
    "        # -------------------------\n",
    "        # Aggregate layered loss\n",
    "        # -------------------------\n",
    "        alpha = float(self.alpha)\n",
    "        beta = float(getattr(self, \"beta\", 0.0))\n",
    "        gamma = float(getattr(self, \"gamma\", 0.0))\n",
    "\n",
    "        loss_total = (1.0 - alpha) * loss_siglip\n",
    "        if alpha > 0.0:\n",
    "            loss_total = loss_total + alpha * loss_icd\n",
    "        if beta > 0.0:\n",
    "            loss_total = loss_total + beta * loss_mmd\n",
    "        if gamma > 0.0:\n",
    "            loss_total = loss_total + gamma * loss_hardneg\n",
    "\n",
    "        # Debug logging for components\n",
    "        step = int(self.state.global_step)\n",
    "        if step % max(1, self.log_interval) == 0 and step != self._last_logged_step:\n",
    "            def _safe_float(t: torch.Tensor) -> Optional[float]:\n",
    "                try:\n",
    "                    return float(t.detach().cpu())\n",
    "                except Exception:\n",
    "                    return None\n",
    "            print(\n",
    "                f\"[Step {step}] L_total={_safe_float(loss_total):.6f} | \"\n",
    "                f\"L_siglip={_safe_float(loss_siglip):.6f} | L_icd={_safe_float(loss_icd):.6f} | \"\n",
    "                f\"L_mmd={_safe_float(loss_mmd):.6f} | L_hardneg={_safe_float(loss_hardneg):.6f} | \"\n",
    "                f\"alpha={alpha:.4f}, beta={beta:.4f}, gamma={gamma:.4f}\"\n",
    "            )\n",
    "            self._last_logged_step = step\n",
    "\n",
    "        return (loss_total, outputs) if return_outputs else loss_total\n",
    "\n",
    "    # -----------------------\n",
    "    # prediction_step (keeps consistent logic)\n",
    "    # -----------------------\n",
    "    def prediction_step(self, model, inputs: Dict[str, Any], prediction_loss_only: bool, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Compute logits for evaluation and (optionally) the BCE loss on validation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = outputs.get(\"logits_per_image\", None)\n",
    "            else:\n",
    "                logits = getattr(outputs, \"logits_per_image\", None)\n",
    "\n",
    "            if logits is None:\n",
    "                # compute from embeddings if model didn't provide logits\n",
    "                image_embeds = outputs.get(\"image_embeds\", getattr(outputs, \"image_embeds\", None))\n",
    "                text_embeds = outputs.get(\"text_embeds\", getattr(outputs, \"text_embeds\", None))\n",
    "                if image_embeds is None or text_embeds is None:\n",
    "                    return (None, None, None)\n",
    "                scale = float(getattr(model, \"logit_scale\", torch.tensor(1.0)).exp()) if self.use_temp else 1.0\n",
    "                _, logits, _ = self.siglip_loss(image_embeds, text_embeds, scale)\n",
    "\n",
    "            # safe check\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                return (None, None, None)\n",
    "\n",
    "            loss = None\n",
    "            if not prediction_loss_only:\n",
    "                B = logits.size(0)\n",
    "                if B > 1:\n",
    "                    labels = torch.eye(B, device=logits.device)\n",
    "                    loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "            preds = logits.cpu().contiguous()\n",
    "            return (loss, preds, None)\n",
    "\n",
    "    # -----------------------\n",
    "    # training_step override: update schedule BEFORE doing the step\n",
    "    # -----------------------\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Any], dataloader_or_placeholder=None):\n",
    "        \"\"\"\n",
    "        Update alpha/beta/gamma schedule first, then call the parent Trainer.training_step which\n",
    "        will call compute_loss (and thus use the updated coefficients).\n",
    "        \"\"\"\n",
    "        global_step = int(self.state.global_step)\n",
    "\n",
    "        icd_end = max(1, self.total_icd_steps)\n",
    "        wass_end = icd_end + max(1, self.total_wass_steps)\n",
    "        hard_end = wass_end + max(1, self.total_hardneg_steps)\n",
    "\n",
    "        if global_step < icd_end:\n",
    "            alpha_raw = float(global_step) / float(icd_end)\n",
    "            self.alpha = min(alpha_raw, self.alpha_max_influence)\n",
    "            self.beta = 0.0\n",
    "            self.gamma = 0.0\n",
    "        elif global_step < wass_end:\n",
    "            self.alpha = self.alpha_max_influence\n",
    "            steps_in = global_step - icd_end\n",
    "            self.beta = float(steps_in) / float(max(1, self.total_wass_steps))\n",
    "            self.gamma = 0.0\n",
    "        elif global_step < hard_end:\n",
    "            self.alpha = self.alpha_max_influence\n",
    "            self.beta = 1.0\n",
    "            steps_in = global_step - wass_end\n",
    "            self.gamma = float(steps_in) / float(max(1, self.total_hardneg_steps))\n",
    "        else:\n",
    "            self.alpha = self.alpha_max_influence\n",
    "            self.beta = 1.0\n",
    "            self.gamma = 1.0\n",
    "\n",
    "        if global_step % max(1, self.log_interval) == 0:\n",
    "            print(f\"[SCHEDULE] step={global_step} | alpha={self.alpha:.4f} beta={self.beta:.4f} gamma={self.gamma:.4f}\")\n",
    "            \n",
    "        # If the total duration for a ramp was 0, the coefficient MUST be 0, \n",
    "        # regardless of what the scheduler calculated above.\n",
    "\n",
    "        if self.total_icd_steps == 0:\n",
    "            self.alpha = 0.0\n",
    "        \n",
    "        if self.total_wass_steps == 0:\n",
    "            self.beta = 0.0\n",
    "            \n",
    "        if self.total_hardneg_steps == 0:\n",
    "            self.gamma = 0.0\n",
    "\n",
    "        # Now forward/backprop is performed by the parent Trainer which will call compute_loss.\n",
    "        return super().training_step(model, inputs, dataloader_or_placeholder)\n",
    "\n",
    "    # -----------------------\n",
    "    # Utilities: freeze logit scale, and a tune_report\n",
    "    # -----------------------\n",
    "    def freeze_logit_scale(self, model=None, value: Optional[float] = None):\n",
    "        \"\"\"Freeze model.logit_scale if present. Optionally set to a specific log-value (value is logit_scale.raw, e.g. np.log(1/0.07)).\"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        if hasattr(model, \"logit_scale\"):\n",
    "            model.logit_scale.requires_grad = False\n",
    "            if value is not None:\n",
    "                with torch.no_grad():\n",
    "                    model.logit_scale.data = torch.tensor(float(value)).to(next(model.parameters()).device)\n",
    "\n",
    "    def tune_report(self) -> str:\n",
    "        \"\"\"Structured step-by-step tuning plan to run experiments in a safe order.\"\"\"\n",
    "        lines = [\n",
    "            \"SigLipTrainer Tuning Plan:\",\n",
    "            \"1) Baseline: use_temp=False, alpha=beta=gamma=0. Train backbone LR=1e-5, head LR=5e-5, batch>=64.\",\n",
    "            \"2) Freeze model.logit_scale: trainer.freeze_logit_scale(). This prevents BCE explosion.\",\n",
    "            \"3) If small dataset: freeze vision encoder for first N epochs (5-10), train only projection/head.\",\n",
    "            \"4) Add ICD (alpha) slowly: alpha_max_influence=0.05, long total_icd_steps (e.g., 10000).\",\n",
    "            \"5) Add MMD (beta) tiny: beta=0.01. Monitor L_mmd and val retrieval.\",\n",
    "            \"6) Add HardNeg (gamma) last (0.01->0.1). Use margin=0.2.\",\n",
    "            \"7) Use aggressive augmentations for images and token dropout for text on small-data.\",\n",
    "            \"8) Use gradient clipping (1.0), weight decay (1e-2) and linear warmup (500-2000 steps).\",\n",
    "        ]\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eda8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def plot_confusion_matrix_fig(predictions, class_names):\n",
    "    \"\"\"Generates a Matplotlib Figure of the confusion matrix.\"\"\"\n",
    "    \n",
    "    # 1. Generate True Labels (Assuming diagonal alignment based on prediction size)\n",
    "    # The true labels are always [0, 1, 2, ..., N-1] repeated across all batches\n",
    "    N_total_samples = len(predictions)\n",
    "    true_labels = np.arange(N_total_samples) % predictions.shape[1] \n",
    "\n",
    "    # 2. Convert Predictions (Find Top-1 class index for each sample)\n",
    "    # The predictions passed here are the raw logits (N_total_samples, N_candidates)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 3. Handle Multiclass vs Multilabel Confusion Matrix (Crucial distinction)\n",
    "    # We are performing a contrastive retrieval task, which is a multi-class problem\n",
    "    # where the 'classes' are the indices 0 to BATCH_SIZE - 1. \n",
    "    # For reporting, we must simplify this. Let's rely on simple binary correctness.\n",
    "    # --- Since we cannot easily reconstruct the *true* class name here, \n",
    "    #     we will use the \"Correct Index\" (0) vs \"Incorrect Index\" (1..N-1) logic.\n",
    "\n",
    "    # NOTE: The Trainer's compute_metrics function already handles this logic.\n",
    "    # For a *report*, the simplest visualization is usually the Top-1 accuracy over the whole set.\n",
    "    # However, since we want a full confusion matrix, we must use the indices 0..N-1.\n",
    "    \n",
    "    # We must generate the true class names (the 66 condition names) for the axis labels.\n",
    "    # Since we don't have the final class list, we'll use numeric indices.\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_classes, labels=np.arange(predictions.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    \n",
    "    ax.set_title(\"Top-1 Retrieval Confusion Matrix (Indices 0-N)\")\n",
    "    ax.set_ylabel('True Index (Expected Rank)')\n",
    "    ax.set_xlabel('Predicted Index (Model Rank)')\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7caee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming plot_confusion_matrix_fig is defined above\n",
    "# Assuming explicit_writer is defined globally\n",
    "\n",
    "class ConfusionMatrixCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Logs the confusion matrix to TensorBoard after each evaluation.\n",
    "    Requires the SummaryWriter instance upon initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, writer):\n",
    "        # Store the provided SummaryWriter instance\n",
    "        self.writer = writer\n",
    "        \n",
    "        # You may want to initialize other internal variables here\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # NOTE: You must also ensure the logic in on_evaluate \n",
    "        # uses self.writer instead of a global tb_writer.\n",
    "        \n",
    "        # Call the parent TrainerCallback init (optional, but good practice)\n",
    "        super().__init__()\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # 1. Check for predictions in Trainer state (necessary for manual retrieval)\n",
    "        if hasattr(state, 'predictions') and state.predictions is not None:\n",
    "            # state.predictions holds the result from the prediction_step\n",
    "            # Format: (loss_values, predictions_array, labels_array)\n",
    "            predictions_raw = state.predictions[1] \n",
    "\n",
    "            if isinstance(predictions_raw, np.ndarray):\n",
    "                # The predictions are already collected as a NumPy array (logits)\n",
    "                final_predictions = predictions_raw\n",
    "            elif isinstance(predictions_raw, tuple):\n",
    "                # Trainer sometimes packs predictions as a tuple (e.g., if multiple heads)\n",
    "                # We expect the first element to be the logits array\n",
    "                final_predictions = predictions_raw[0]\n",
    "            else:\n",
    "                return # Cannot process, data format is incorrect\n",
    "\n",
    "            # 2. Generate Matplotlib Figure\n",
    "            try:\n",
    "                # We need the class names, but since we don't have them, we use the raw predictions\n",
    "                fig = plot_confusion_matrix_fig(final_predictions, None) \n",
    "                print(f\"| DEBUG: After plotting confusion matrix figure.\")\n",
    "                \n",
    "                # 3. Log the figure to TensorBoard\n",
    "                global explicit_writer\n",
    "                if explicit_writer:\n",
    "                    print(f\"| DEBUG: Confusion Matrix - Before add_figure step {state.global_step}.\")\n",
    "                    explicit_writer.add_figure(\n",
    "                        f\"{metrics.get('epoch', 'final'):.2f}/Confusion_Matrix\",\n",
    "                        fig,\n",
    "                        global_step=state.global_step\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"[CM Callback Error] Failed to generate/log confusion matrix: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1c5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainerCallback\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "class CovarianceLoggingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Logs the covariance of model predictions/features to TensorBoard.\n",
    "    \"\"\"\n",
    "    def __init__(self, writer):\n",
    "        # Store the provided SummaryWriter instance\n",
    "        self.writer = writer\n",
    "        \n",
    "        # You may want to initialize other internal variables here\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # NOTE: You must also ensure the logic in on_evaluate \n",
    "        # uses self.writer instead of a global tb_writer.\n",
    "        \n",
    "        # Call the parent TrainerCallback init (optional, but good practice)\n",
    "        super().__init__()\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        # Only proceed if TensorBoard logging is active\n",
    "        if args.report_to and \"tensorboard\" in args.report_to:\n",
    "            tb_writer = None\n",
    "            # Find the active TensorBoard writer instance\n",
    "            for callback in kwargs.get(\"callbacks\", []):\n",
    "                if isinstance(callback, TensorBoardCallback):\n",
    "                    tb_writer = callback.tb_writer\n",
    "                    break\n",
    "            \n",
    "            if tb_writer:\n",
    "                # --- Compute Covariance Here ---\n",
    "                # This is highly context-dependent. A simplified example\n",
    "                # using random data. Replace with your actual calculation \n",
    "                # (e.g., from validation dataset logits/features).\n",
    "                \n",
    "                # Example: Compute covariance of a dummy feature vector\n",
    "                # In a real scenario, you'd extract logits or features \n",
    "                # during evaluation somehow.\n",
    "                # For this example, we log a sample matrix:\n",
    "                \n",
    "                sample_data = np.random.randn(100, 5) # 100 samples, 5 features\n",
    "                cov_matrix = np.cov(sample_data, rowvar=False) # Features in columns\n",
    "                cov_mean = np.mean(cov_matrix)\n",
    "                cov_std = np.std(cov_matrix)\n",
    "\n",
    "                print(f\"| DEBUG: Logging Covariance for step {state.global_step}. Shape: {cov_matrix.shape}\")\n",
    "                tb_writer.add_histogram(\n",
    "                    tag=\"Evaluation/Covariance_Matrix_Values_Distribution\",\n",
    "                    values=cov_matrix.flatten(), # Log all elements as a distribution\n",
    "                    global_step=state.global_step\n",
    "                )\n",
    "                \n",
    "                # You might also want to log the max/min of the covariance\n",
    "                tb_writer.add_scalar(\"Evaluation/Covariance_Max_Value\", np.max(cov_matrix), state.global_step)\n",
    "                tb_writer.add_scalar(\"Evaluation/Covariance_Mean_Value\", cov_mean, state.global_step)\n",
    "                tb_writer.add_scalar(\"Evaluation/Covariance_STD_Deviation\", cov_std, state.global_step)\n",
    "                tb_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67baf2",
   "metadata": {},
   "source": [
    "#### 1. Loss Analysis Diagnosis Table\n",
    "\n",
    "| Training Loss | Validation Loss | Diagnosis | Action Required |\n",
    "|---------------|-----------------|-----------|-----------------|\n",
    "| High | High | **Underfitting** | Increase model complexity, train longer |\n",
    "| Low | High (Increasing) | **Overfitting** | Add regularization (Dropout/L2), get more data, Early Stopping |\n",
    "| Low | Low | **Good Fit** | Save model weights |\n",
    "| High | Low | **Data Issue** | Check regularization, data augmentation, or data split |\n",
    "\n",
    "#### 2. Loss Value Interpretation Table\n",
    "\n",
    "| Value Range | Meaning | Status |\n",
    "|-------------|---------|--------|\n",
    "| > 0.60 | **High** | Underfitting / Starting: Model is guessing |\n",
    "| 0.40 - 0.60 | **Medium** | Learning: Model is picking up patterns |\n",
    "| < 0.30 | **Low** | Good Fit: Model is confident in its predictions |\n",
    "\n",
    "#### 3. Additional Metrics for Multi-Label Classification\n",
    "\n",
    "| Metric | Good Range | Interpretation |\n",
    "|--------|------------|----------------|\n",
    "| **Per-class Accuracy** | > 0.70 | Percentage of correct predictions per condition |\n",
    "| **Macro F1-Score** | > 0.60 | Average F1 across all classes (handles imbalance) |\n",
    "| **Hamming Loss** | < 0.20 | Fraction of wrong labels (lower is better) |\n",
    "| **Subset Accuracy** | > 0.40 | Exact match of all labels (strict metric) |\n",
    "\n",
    "#### 4. SigLip Fine-tuning Specific Guidelines\n",
    "\n",
    "| Training Stage | Expected Train Loss | Expected Val Loss | What to Look For |\n",
    "|----------------|-------------------|-------------------|------------------|\n",
    "| **Early (Epoch 1-3)** | 0.80 - 1.20 | 0.70 - 1.10 | Both should decrease rapidly |\n",
    "| **Mid (Epoch 4-8)** | 0.40 - 0.70 | 0.45 - 0.75 | Val loss should track train loss closely |\n",
    "| **Late (Epoch 9-15)** | 0.20 - 0.45 | 0.30 - 0.55 | Gap should be small (<0.10) |\n",
    "| **Converged** | < 0.30 | < 0.40 | Both stable, gap <0.10 means good generalization |\n",
    "\n",
    "#### 5. Warning Signs During Training\n",
    "\n",
    "| Observation | Problem | Solution |\n",
    "|-------------|---------|----------|\n",
    "| Val loss increases after epoch 5 | **Overfitting** | Enable early stopping, increase dropout |\n",
    "| Both losses stuck > 0.70 | **Underfitting** | Decrease LoRA rank, increase learning rate |\n",
    "| Val loss oscillates wildly | **Unstable training** | Reduce learning rate, increase batch size |\n",
    "| Train loss = 0.0, Val loss > 1.0 | **Severe overfitting** | Add strong regularization, reduce model capacity |\n",
    "| Gap between train/val > 0.20 | **Poor generalization** | More data augmentation, check data leakage |\n",
    "\n",
    "#### 6. Recommended Actions Based on Your Dataset\n",
    "\n",
    "Given your characteristics:\n",
    "- **79.4% multi-label samples** ‚Üí Focus on Hamming Loss and Macro F1\n",
    "- **758:1 class imbalance** ‚Üí Monitor per-class metrics, not just overall accuracy\n",
    "- **9 rare classes (<10 samples)** ‚Üí These will have high loss; group to coarse categories\n",
    "\n",
    "| Target Metrics (Epoch 15) | Realistic Goal | Stretch Goal |\n",
    "|---------------------------|----------------|--------------|\n",
    "| **Validation Loss** | < 0.40 | < 0.30 |\n",
    "| **Macro F1-Score** | > 0.55 | > 0.65 |\n",
    "| **Top-3 Accuracy** | > 0.80 | > 0.90 |\n",
    "| **Hamming Loss** | < 0.25 | < 0.15 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f468d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2580' max='2580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2580/2580 32:36, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.341094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.278660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.270648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.267424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.265739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.264728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.264016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.263557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.263117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.262965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.262889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.262836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.262824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.262825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/29 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_layered_loss_data_collator(features):\n",
    "    \"\"\"\n",
    "    Custom collator to handle the custom 'condition_indices' tensor alongside \n",
    "    standard Hugging Face processor outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extract the custom indices (Tensor)\n",
    "    condition_indices = [f['condition_indices'] for f in features]\n",
    "    \n",
    "    # 2. Remove the custom key so the default collator handles the rest\n",
    "    for f in features:\n",
    "        del f['condition_indices']\n",
    "        \n",
    "    # 3. Use the default collator for the standard keys \n",
    "    #    (This correctly handles padding/stacking for pixel_values, input_ids, etc.)\n",
    "    batch = default_data_collator(features)\n",
    "    \n",
    "    # 4. Add the custom indices back as a single stacked tensor\n",
    "    # Note: torch.stack is required here because they are single tensors from __getitem__\n",
    "    batch['condition_indices'] = torch.stack(condition_indices)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def debug_icd_map_integrity(trainer_instance: Any):\n",
    "    \"\"\"Prints the loaded ICD index map keys and their raw string representation.\"\"\"\n",
    "    \n",
    "    # Access the map stored in the trainer instance\n",
    "    icd_map: Dict[str, int] = trainer_instance.condition_to_idx \n",
    "    \n",
    "    if not icd_map:\n",
    "        print(\"\\n--- ICD MAP INTEGRITY CHECK ---\")\n",
    "        print(\"‚ùå ERROR: self.condition_to_idx is empty. Check file loading in __init__.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- ICD MAP INTEGRITY CHECK ---\")\n",
    "    print(f\"‚úÖ Map Size: {len(icd_map)} conditions.\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Sample Keys (First 25 for detailed verification):\")\n",
    "    \n",
    "    # Iterate through the first 25 keys\n",
    "    for i, (condition, index) in enumerate(list(icd_map.items())[:25]):\n",
    "        # The repr() function shows invisible characters like spaces (' ') or newlines ('\\n')\n",
    "        print(f\"  [{i:2d} | Index {index}] Key: '{condition}' (Raw: {repr(condition)})\")\n",
    "    \n",
    "    print(\"\\n--- Failing Input Check ---\")\n",
    "    print(f\"Input Check: 'Eczema' in map? {('Eczema' in icd_map)}\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # predictions (logits) are tuple: (logits_per_image, logits_per_text)\n",
    "    logits = eval_pred.predictions \n",
    "    # Robustly handle tuple unpacking\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # For evaluation, we treat the diagonal as the ground truth class\n",
    "    # (retrieval task: given image i, find text i)\n",
    "    batch_size = logits.shape[0]\n",
    "    labels = np.arange(batch_size)\n",
    "    \n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# Define your custom hyperparameters for tracking (outside TrainingArguments if needed)\n",
    "CUSTOM_HP_DROPOUT = 0.1\n",
    "CUSTOM_HP_LAYERS = 3\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(config.output_dir),\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    lr_scheduler_type=\"cosine\", # Controls lerning rate behavior.\n",
    "    fp16=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[\"tensorboard\"], # Tell the Trainer to send all logs to TensorBoard\n",
    "    logging_dir=config.log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    run_name=f\"run_lr3e4_dp{CUSTOM_HP_DROPOUT}\", # Great for fine-tuning experiment tracking\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2, #Was 3\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    prediction_loss_only=False, # CRITICAL for compute_metrics to run!\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "        r=config.lora_alpha,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "# Apply LoRA if specified\n",
    "if config.finetune_mode == \"LoRA\":\n",
    "    print(\"\\nApplying LoRA fine-tuning...\")\n",
    "    model = get_peft_model(model, lora_config)  \n",
    "\n",
    "explicit_writer = SummaryWriter(log_dir=config.log_dir)\n",
    "# 1. Create the callback instance\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)\n",
    "# Instantiate the custom callback\n",
    "cov_callback = CovarianceLoggingCallback(writer=explicit_writer)\n",
    "conf_matrix_callback = ConfusionMatrixCallback(writer=explicit_writer)\n",
    "\n",
    "explicit_writer = SummaryWriter(log_dir=config.log_dir)\n",
    "\n",
    "trainer = SigLipTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # Your standard metrics function\n",
    "    callbacks=[early_stopping_callback, cov_callback, conf_matrix_callback], # Your custom covariance logger\n",
    "    data_collator=custom_layered_loss_data_collator, # Custom collator to handle condition indices\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ICD MAP INTEGRITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "debug_icd_map_integrity(trainer)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING FINE-GRAINED TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Training with fine-grained condition labels...\")\n",
    "print(\"Focus on discriminating between 66 specific conditions\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(f\"\\n‚úì Training complete! Final loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_results['eval_loss']:.4f}\") # (This runs successfully)\n",
    "\n",
    "# Required for Tensor Board HParams logging\n",
    "if explicit_writer:\n",
    "    # Log final metrics with the 'hparam/' prefix for the HParams tab.\n",
    "    explicit_writer.add_hparams(\n",
    "        hparam_dict={\n",
    "            'custom_dropout_rate': CUSTOM_HP_DROPOUT,\n",
    "            'another_custom_hp': CUSTOM_HP_LAYERS,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'batch_size': config.batch_size,\n",
    "        },\n",
    "        metric_dict={\n",
    "            # Now these will be populated with real, non-zero values\n",
    "            'hparam/loss': test_results.get('eval_loss', 0.0),\n",
    "            'hparam/accuracy': test_results.get('eval_accuracy', 0.0),\n",
    "        }\n",
    "    )\n",
    "    explicit_writer.flush()\n",
    "\n",
    "print(f\"‚úì Test loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Save\n",
    "final_path = config.output_dir / 'final_model'\n",
    "trainer.save_model(str(final_path))\n",
    "processor.save_pretrained(str(final_path))\n",
    "print(f\"‚úì Model saved to: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d565dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval(model, processor, test_df, images_dir, n_samples=20, debug_limit=5):\n",
    "    \"\"\"\n",
    "    Test fine-grained retrieval capabilities with detailed debugging.\n",
    "    \n",
    "    Args:\n",
    "        debug_limit (int): Number of samples to print detailed logs for.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure we don't sample more than available\n",
    "    n_samples = min(n_samples, len(test_df))\n",
    "    sample_indices = np.random.choice(len(test_df), n_samples, replace=False)\n",
    "\n",
    "    accuracies = []\n",
    "    condition_accuracies = {}\n",
    "    debug_counter = 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"New Testing retrieval on {n_samples} samples...\")\n",
    "    print(f\"Detailed debug info will be printed for the first {debug_limit} samples.\\n\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    for idx in tqdm(sample_indices, desc=\"Testing retrieval\"):\n",
    "        row = test_df.iloc[idx]\n",
    "\n",
    "        # 1. Load Image\n",
    "        image_path = images_dir / row['image_path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        # 2. Prepare Candidates\n",
    "        desc = str(row['description'])\n",
    "        if desc.lower() == 'nan' or not desc:\n",
    "            correct_text = f\"A dermatological image of {row['condition']}\"\n",
    "        else:\n",
    "            correct_text = f\"A dermatological image of {row['condition']}: {desc}\"\n",
    "            \n",
    "        correct_condition = row['condition']\n",
    "\n",
    "        # Get distractors\n",
    "        other_conditions = test_df[test_df['condition'] != correct_condition]['condition'].unique()\n",
    "        \n",
    "        n_distractors = min(9, len(other_conditions))\n",
    "        if n_distractors < 1: continue\n",
    "\n",
    "        distractor_conditions = np.random.choice(other_conditions, n_distractors, replace=False)\n",
    "\n",
    "        distractor_texts = []\n",
    "        for dist_cond in distractor_conditions:\n",
    "            dist_rows = test_df[test_df['condition'] == dist_cond]\n",
    "            if len(dist_rows) > 0:\n",
    "                dist_row = dist_rows.sample(1).iloc[0]\n",
    "                # Clean description logic for distractors too\n",
    "                d_desc = str(dist_row['description'])\n",
    "                if d_desc.lower() == 'nan' or not d_desc:\n",
    "                     d_text = f\"A dermatological image of {dist_row['condition']}\"\n",
    "                else:\n",
    "                     d_text = f\"A dermatological image of {dist_row['condition']}: {d_desc}\"\n",
    "                distractor_texts.append(d_text)\n",
    "\n",
    "        # Index 0 is ALWAYS the correct answer in this list\n",
    "        all_texts = [correct_text] + distractor_texts\n",
    "\n",
    "        # 3. Process Inputs\n",
    "        inputs = processor(\n",
    "            text=all_texts,\n",
    "            images=[image] * len(all_texts),\n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=64, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(config.device)\n",
    "\n",
    "        # 4. Get Predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if isinstance(outputs, dict):\n",
    "                logits_per_image = outputs['logits_per_image']\n",
    "            else:\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "            \n",
    "            # Get logits for the single image against all texts\n",
    "            logits = logits_per_image[0]\n",
    "            \n",
    "            # Apply Sigmoid to get readable probabilities (0.0 to 1.0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "        # 5. Check accuracy\n",
    "        predicted_idx = probs.argmax().item()\n",
    "        correct = (predicted_idx == 0)\n",
    "        accuracies.append(correct)\n",
    "\n",
    "        # Track per-condition accuracy\n",
    "        if correct_condition not in condition_accuracies:\n",
    "            condition_accuracies[correct_condition] = []\n",
    "        condition_accuracies[correct_condition].append(correct)\n",
    "\n",
    "        # --- DEBUGGING OUTPUT ---\n",
    "        if debug_counter < debug_limit:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"DEBUG SAMPLE {debug_counter + 1} | Condition: {correct_condition}\")\n",
    "            print(f\"Image Path: {image_path}\")\n",
    "            print(f\"Result: {'‚úÖ CORRECT' if correct else '‚ùå INCORRECT'}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "            # Get Top 5 predictions for this image\n",
    "            top_k = min(5, len(all_texts))\n",
    "            top_probs, top_indices = torch.topk(probs, top_k)\n",
    "            \n",
    "            print(f\"Model's Top {top_k} Guesses:\")\n",
    "            for score, idx in zip(top_probs, top_indices):\n",
    "                idx = idx.item()\n",
    "                score = score.item()\n",
    "                \n",
    "                is_ground_truth = (idx == 0)\n",
    "                marker = \"üëà TRUE LABEL\" if is_ground_truth else \"\"\n",
    "                \n",
    "                # Truncate text for cleaner printing\n",
    "                display_text = all_texts[idx][:100] + \"...\" if len(all_texts[idx]) > 100 else all_texts[idx]\n",
    "                \n",
    "                print(f\"  [{score:.4f}] {display_text} {marker}\")\n",
    "\n",
    "            # If correct answer wasn't in top 5, print it explicitly\n",
    "            if 0 not in top_indices.tolist():\n",
    "                true_score = probs[0].item()\n",
    "                print(f\"  ...\\n  [{true_score:.4f}] {correct_text} üëà TRUE LABEL (Ranked > {top_k})\")\n",
    "                \n",
    "            print(f\"{'='*80}\\n\")\n",
    "            debug_counter += 1\n",
    "\n",
    "    if not accuracies:\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = np.mean(accuracies)\n",
    "    print(f\"\\n‚úì Fine-grained retrieval accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4337e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# NOTE: This function assumes config, processor, model, and test_df are in scope.\n",
    "\n",
    "def test_retrieval(model, processor, test_df, images_dir, n_samples=20, k_accuracy=5, debug_limit=5):\n",
    "    \"\"\"\n",
    "    Test fine-grained retrieval capabilities with detailed logging and True Rank calculation.\n",
    "    \n",
    "    Args:\n",
    "        k_accuracy (int): The 'K' value for Top-K accuracy (the final metric).\n",
    "        debug_limit (int): Number of samples to print detailed logs for (shows Top 5).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    n_samples = min(n_samples, len(test_df))\n",
    "    sample_indices = np.random.choice(len(test_df), n_samples, replace=False)\n",
    "\n",
    "    accuracies = []\n",
    "    condition_accuracies = {}\n",
    "    debug_counter = 0\n",
    "\n",
    "    print(f\"Testing retrieval on {n_samples} samples for R@{k_accuracy} accuracy...\")\n",
    "\n",
    "    for idx in tqdm(sample_indices, desc=\"Testing retrieval\"):\n",
    "        row = test_df.iloc[idx]\n",
    "        image_path = Path(images_dir) / row['image_path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Prepare Candidates (Assuming previous logic)\n",
    "        desc = str(row['description'])\n",
    "        if desc.lower() == 'nan' or not desc:\n",
    "            correct_text = f\"A dermatological image of {row['condition']}\"\n",
    "        else:\n",
    "            correct_text = f\"A dermatological image of {row['condition']}: {desc}\"\n",
    "            \n",
    "        correct_condition = row['condition']\n",
    "\n",
    "        # Get distractors (logic omitted for brevity)\n",
    "        other_conditions = test_df[test_df['condition'] != correct_condition]['condition'].unique()\n",
    "        n_distractors = min(9, len(other_conditions))\n",
    "        if n_distractors < 1: continue\n",
    "\n",
    "        distractor_conditions = np.random.choice(other_conditions, n_distractors, replace=False)\n",
    "        distractor_texts = []\n",
    "        for dist_cond in distractor_conditions:\n",
    "            dist_rows = test_df[test_df['condition'] == dist_cond]\n",
    "            if len(dist_rows) > 0:\n",
    "                dist_row = dist_rows.sample(1).iloc[0]\n",
    "                d_desc = str(dist_row['description'])\n",
    "                d_text = f\"A dermatological image of {dist_row['condition']}: {d_desc}\" if not d_desc.lower() == 'nan' else f\"A dermatological image of {dist_row['condition']}\"\n",
    "                distractor_texts.append(d_text)\n",
    "\n",
    "        all_texts = [correct_text] + distractor_texts\n",
    "\n",
    "        # 3. Process Inputs\n",
    "        inputs = processor(\n",
    "            text=all_texts,\n",
    "            images=[image] * len(all_texts),\n",
    "            padding=True, truncation=True, max_length=64, return_tensors=\"pt\"\n",
    "        ).to(config.device)\n",
    "\n",
    "        # 4. Get Predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            if isinstance(outputs, dict):\n",
    "                logits_per_image = outputs['logits_per_image']\n",
    "            else:\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "            \n",
    "            similarities = logits_per_image[0]\n",
    "            probs = torch.sigmoid(similarities)\n",
    "\n",
    "        # 5. Check accuracy (R@K) and Calculate Rank\n",
    "        K = min(k_accuracy, len(all_texts)) \n",
    "        \n",
    "        # Calculate Rank\n",
    "        true_score = probs[0].item()\n",
    "        # Rank is 1 + the number of scores that are strictly greater than the true score\n",
    "        rank = (probs > true_score).sum().item() + 1 \n",
    "        \n",
    "        top_k_indices = torch.topk(probs, k=K).indices\n",
    "        correct = (0 in top_k_indices.tolist()) \n",
    "        \n",
    "        accuracies.append(correct)\n",
    "\n",
    "        # 6. --- DEBUGGING OUTPUT ---\n",
    "        if debug_counter < debug_limit:\n",
    "            top_k_debug = min(5, len(all_texts))\n",
    "            top_probs, top_indices = torch.topk(probs, top_k_debug)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"DEBUG SAMPLE {debug_counter + 1} | Condition: {correct_condition}\")\n",
    "            print(f\"Image Path: {image_path}\")\n",
    "            print(f\"Result (R@{K}): {'‚úÖ CORRECT' if correct else '‚ùå INCORRECT'}\") \n",
    "            print(f\"TRUE LABEL RANK: {rank}th\") # <--- NEW RANK OUTPUT\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "            print(f\"Model's Top {top_k_debug} Guesses:\")\n",
    "            for score, idx in zip(top_probs, top_indices):\n",
    "                idx = idx.item()\n",
    "                score = score.item()\n",
    "                \n",
    "                is_ground_truth = (idx == 0)\n",
    "                marker = \"üëà TRUE LABEL\" if is_ground_truth else \"\"\n",
    "                \n",
    "                display_text = all_texts[idx][:100] + \"...\" if len(all_texts[idx]) > 100 else all_texts[idx]\n",
    "                \n",
    "                print(f\"  [{score:.4f}] {display_text} {marker}\")\n",
    "\n",
    "            if 0 not in top_indices.tolist():\n",
    "                print(f\"  ...\\n  [{true_score:.4f}] {correct_text} üëà TRUE LABEL (Actual Rank: {rank}th)\") # <--- NEW RANK OUTPUT\n",
    "                \n",
    "            print(f\"{'='*80}\\n\")\n",
    "            debug_counter += 1\n",
    "\n",
    "    if not accuracies:\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = np.mean(accuracies)\n",
    "    print(f\"\\n‚úì Fine-grained retrieval accuracy (R@{k_accuracy}): {accuracy:.2%}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb22c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing retrieval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Fine-Grained Retrieval\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "retrieval_acc = test_retrieval(model, processor, test_df, config.images_dir)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'approach': 'fine_grained',\n",
    "    'num_conditions': int(df['condition'].nunique()),\n",
    "    'train_samples': len(train_df),\n",
    "    'final_loss': float(train_result.training_loss),\n",
    "    'test_loss': float(test_results['eval_loss']),\n",
    "    'retrieval_accuracy': float(retrieval_acc),\n",
    "}\n",
    "\n",
    "with open(config.output_dir / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINE-GRAINED TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Test loss: {test_results['eval_loss']:.4f}\")\n",
    "print(f\"  Retrieval accuracy: {retrieval_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
