{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddaa1b5",
   "metadata": {},
   "source": [
    "### **Save the Sample for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947471ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Loading SCIN dataset (streaming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading and sanitizing 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eval samples: 100%|██████████| 1000/1000 [01:03<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 747 valid samples out of 1000.\n",
      "Saving 747 samples to eval_data.pt...\n",
      "Data preparation complete.\n",
      "\n",
      "You can now run 'test_baseline_eval.py' repeatedly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import traceback\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- Configuration --- #\n",
    "N_VAL_SAMPLES = 1000 # Number of samples to save\n",
    "OUTPUT_FILE = \"eval_data.pt\" # The file we will save\n",
    "\n",
    "def prepare_and_save_data():\n",
    "    \"\"\"\n",
    "    Downloads, sanitizes, and saves the validation dataset.\n",
    "    Run this script ONCE.\n",
    "    \"\"\"\n",
    "    print(\"Starting data preparation...\")\n",
    "\n",
    "    # --- STEP 1: LOAD DATASET STREAM ---\n",
    "    print(\"Loading SCIN dataset (streaming)...\")\n",
    "    try:\n",
    "        base_iterable = load_dataset(\"google/scin\", split=\"train\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset: {e}. Check dataset name/internet.\")\n",
    "        return\n",
    "\n",
    "    eval_data_list = []\n",
    "    image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
    "\n",
    "    print(f\"Pre-loading and sanitizing {N_VAL_SAMPLES} samples...\")\n",
    "\n",
    "    # --- STEP 2: SANITIZE AND COLLECT DATA ---\n",
    "    for item in tqdm(base_iterable.take(N_VAL_SAMPLES), total=N_VAL_SAMPLES, desc=\"Loading eval samples\"):\n",
    "        text = item.get(\"related_category\")\n",
    "\n",
    "        # [FIX] Check for None AND empty strings\n",
    "        if not text or text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    # [FIX] Force convert to RGB and store the sanitized item\n",
    "                    eval_data_list.append({\"image\": image.convert(\"RGB\"), \"text\": text.strip()})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                # Found a valid image, break from inner loop\n",
    "                break\n",
    "\n",
    "    print(f\"Collected {len(eval_data_list)} valid samples out of {N_VAL_SAMPLES}.\")\n",
    "\n",
    "    # --- STEP 3: SAVE TO FILE ---\n",
    "    if not eval_data_list:\n",
    "        print(\"ERROR: No data was collected. Aborting save.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Saving {len(eval_data_list)} samples to {OUTPUT_FILE}...\")\n",
    "        torch.save(eval_data_list, OUTPUT_FILE)\n",
    "        print(\"Data preparation complete.\")\n",
    "        print(f\"\\nYou can now run 'test_baseline_eval.py' repeatedly.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_and_save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3287572",
   "metadata": {},
   "source": [
    "### **Test the eval_data.pl file for correctness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66edc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INSPECTING eval_data.pt ---\n",
      "Type of loaded data: <class 'list'>\n",
      "Total items in list: 747\n",
      "\n",
      "--- FIRST 5 ITEMS ---\n",
      "[{'image': <PIL.Image.Image image mode=RGB size=810x779 at 0x70A8451BE050>, 'text': 'RASH'}, {'image': <PIL.Image.Image image mode=RGB size=810x1080 at 0x70A8451BFED0>, 'text': 'OTHER_ISSUE_DESCRIPTION'}, {'image': <PIL.Image.Image image mode=RGB size=810x1080 at 0x70A8451BE010>, 'text': 'OTHER_ISSUE_DESCRIPTION'}, {'image': <PIL.Image.Image image mode=RGB size=810x1080 at 0x70A8451BD090>, 'text': 'RASH'}, {'image': <PIL.Image.Image image mode=RGB size=810x1080 at 0x70A8451BED90>, 'text': 'RASH'}]\n",
      "---------------------\n",
      "Count of 'None' items: 0 / 747\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "EVAL_DATA_FILE = \"eval_data.pt\"\n",
    "\n",
    "print(f\"--- INSPECTING {EVAL_DATA_FILE} ---\")\n",
    "\n",
    "try:\n",
    "    # Load the data\n",
    "    eval_data_list = torch.load(EVAL_DATA_FILE, weights_only=False)\n",
    "    \n",
    "    # Check what was loaded\n",
    "    print(f\"Type of loaded data: {type(eval_data_list)}\")\n",
    "    \n",
    "    if isinstance(eval_data_list, list):\n",
    "        print(f\"Total items in list: {len(eval_data_list)}\")\n",
    "        print(\"\\n--- FIRST 5 ITEMS ---\")\n",
    "        print(eval_data_list[:5])\n",
    "        print(\"---------------------\")\n",
    "\n",
    "        # Check for 'None'\n",
    "        none_count = sum(1 for item in eval_data_list if item is None)\n",
    "        print(f\"Count of 'None' items: {none_count} / {len(eval_data_list)}\")\n",
    "    else:\n",
    "        print(\"ERROR: Data file is not a list as expected.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load or inspect file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f1bb6",
   "metadata": {},
   "source": [
    "### **Run baseline Evaluation on eval_data.pt file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ea8abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model and processor from: google/siglip-base-patch16-224\n",
      "DEBUG: Model parameters are on device: cuda:0\n",
      "DEBUG: Model parameters are dtype: torch.float32\n",
      "Loading pre-prepared data from eval_data.pt...\n",
      "Initializing SCIN_List_Dataset with 747 pre-loaded samples.\n",
      "Loaded 747 valid samples.\n",
      "Setting up training arguments and trainer...\n",
      "\n",
      "==================================================\n",
      "RUNNING BASELINE EVALUATION...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BATCH STATISTICS:\n",
      "  Total batches attempted: 47\n",
      "  Successful batches: 47\n",
      "  Skipped batches: 0\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TEST COMPLETE. BASELINE METRICS:\n",
      "==================================================\n",
      "  eval_loss: 0.6112768650054932\n",
      "  eval_model_preparation_time: 0.0024\n",
      "  eval_runtime: 6.6985\n",
      "  eval_samples_per_second: 111.518\n",
      "  eval_steps_per_second: 7.017\n",
      "\n",
      "\n",
      "✓ SUCCESS: 'eval_loss' and other metrics were successfully calculated.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration --- #\n",
    "MODEL_ID = \"google/siglip-base-patch16-224\"\n",
    "BATCH_SIZE = 16\n",
    "EVAL_DATA_FILE = \"eval_data.pt\" # The file created by prepare_data.py\n",
    "\n",
    "# We will use it to clear the logs just once\n",
    "global_batch_counter = 0\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 1: The Dataset & Collate Code\n",
    "# ===================================================================\n",
    "\n",
    "class SCIN_List_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that wraps a simple list.\n",
    "    This is used for the VALIDATION set to ensure it's reusable\n",
    "    and works correctly with trainer.evaluate().\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        print(f\"Initializing SCIN_List_Dataset with {len(data_list)} pre-loaded samples.\")\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    \"\"\"\n",
    "    [Robust, Item-by-Item] Data collator.\n",
    "    This version includes explicit checks for None or empty content\n",
    "    to prevent the processor from crashing.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    processed_texts_input_ids = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            if item is None:\n",
    "                print(f\"WARNING (collate_fn): Skipping item {i} because it is 'None'.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            img = item.get(\"image\")\n",
    "            txt = item.get(\"text\")\n",
    "            \n",
    "            # --- THIS IS THE FIX ---\n",
    "            # Explicitly check for bad content *before* calling the processor.\n",
    "            # The processor will crash on None or empty strings.\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"WARNING (collate_fn): Skipping item {i} due to 'None' image.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "                \n",
    "            if txt is None or txt.strip() == \"\":\n",
    "                print(f\"WARNING (collate_fn): Skipping item {i} due to 'None' or empty text.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            # -------------------------\n",
    "\n",
    "            # If we get here, img and txt are valid\n",
    "            inputs = processor(\n",
    "                text=[txt],\n",
    "                images=[img],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=64\n",
    "            )\n",
    "            \n",
    "            processed_images.append(inputs[\"pixel_values\"])\n",
    "            processed_texts_input_ids.append(inputs[\"input_ids\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            # This now catches *other* errors, like a truly corrupt image file\n",
    "            print(f\"WARNING (collate_fn): Skipping item {i} due to UNEXPECTED error: {e}\")\n",
    "            skipped_count += 1\n",
    "            \n",
    "    if not processed_images:\n",
    "        # This will still happen if an entire batch is bad\n",
    "        print(f\"ERROR: Entire batch was skipped! ({skipped_count} items failed)\")\n",
    "        return {}\n",
    "\n",
    "    if skipped_count > 0:\n",
    "        print(f\"Collate: Skipped {skipped_count}/{len(batch)} items in this batch\")\n",
    "\n",
    "    try:\n",
    "        batch_pixel_values = torch.cat(processed_images, dim=0)\n",
    "        batch_input_ids = torch.cat(processed_texts_input_ids, dim=0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": batch_pixel_values,\n",
    "            \"input_ids\": batch_input_ids\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during final batch collation: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 2: The Corrected Logic\n",
    "# ===================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Standard Hugging Face method to calculate metrics.\n",
    "    This runs ONCE at the end of evaluation on the accumulated predictions.\n",
    "    \"\"\"\n",
    "    # eval_pred.predictions is a tuple of (logits_per_image, logits_per_text)\n",
    "    # We care about image-to-text matching (logits_per_image)\n",
    "    logits = eval_pred.predictions\n",
    "    \n",
    "    # Handle tuple case\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # In CLIP/SigLIP, the standard task is: given an image, find the correct text in the batch.\n",
    "    # So labels are the diagonal (0, 1, 2, 3...).\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Generate ground truth labels (diagonal alignment)\n",
    "    true_labels = np.arange(len(predictions))\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "class EvalTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    This is the final debug trainer.\n",
    "    It includes a check for NaN/Inf values from the model's output.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.batch_count = 0\n",
    "        self.successful_batches = 0\n",
    "        self.skipped_batches = 0\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        self.batch_count += 1\n",
    "        \n",
    "        print(f\"\\n=== Batch {self.batch_count} ===\")\n",
    "        print(f\"Inputs keys: {inputs.keys() if inputs else 'NONE'}\")\n",
    "        \n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "             print(f\"WARNING: Empty batch received in compute_loss (batch {self.batch_count})\")\n",
    "             self.skipped_batches += 1\n",
    "             # CRITICAL: Don't return dummy loss, raise exception to truly skip\n",
    "             raise ValueError(\"Empty batch - skipping\")\n",
    "\n",
    "        print(f\"Pixel values shape: {inputs['pixel_values'].shape}\")\n",
    "        print(f\"Input ids shape: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        print(f\"Model output keys: {outputs.keys() if hasattr(outputs, 'keys') else type(outputs)}\")\n",
    "        \n",
    "        # --- FINAL NaN/Inf CHECK ---\n",
    "        if torch.isnan(outputs.logits_per_image).any() or torch.isinf(outputs.logits_per_image).any():\n",
    "            print(\"DEBUG: NaN or Inf DETECTED in model logits.\")\n",
    "            self.skipped_batches += 1\n",
    "            raise ValueError(\"NaN/Inf in logits - skipping\")\n",
    "        # ---------------------------\n",
    "\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        batch_size = logits_per_image.shape[0]\n",
    "        \n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Logits_per_image shape: {logits_per_image.shape}\")\n",
    "        \n",
    "        if batch_size <= 1:\n",
    "             print(f\"WARNING: Batch size is {batch_size}, cannot compute contrastive loss\")\n",
    "             self.skipped_batches += 1\n",
    "             raise ValueError(f\"Batch size too small: {batch_size}\")\n",
    "\n",
    "        labels = torch.eye(batch_size, device=model.device)\n",
    "        loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "        loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "        loss = (loss_images + loss_text) / 2.0\n",
    "        \n",
    "        # print(f\"✓ Computed loss: {loss.item():.4f}\")\n",
    "        self.successful_batches += 1\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Override to ensure we compute loss AND return predictions for compute_metrics.\n",
    "        This is called during evaluation, NOT compute_loss!\n",
    "        \"\"\"\n",
    "        self.batch_count += 1\n",
    "        \n",
    "        # print(f\"\\n=== Batch {self.batch_count} (prediction_step) ===\")\n",
    "        # print(f\"Inputs keys: {inputs.keys() if inputs else 'NONE'}\")\n",
    "        \n",
    "        # Handle empty batches\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            print(f\"WARNING: Empty batch in prediction_step (batch {self.batch_count})\")\n",
    "            self.skipped_batches += 1\n",
    "            return (None, None, None)\n",
    "        \n",
    "        # print(f\"Pixel values shape: {inputs['pixel_values'].shape}\")\n",
    "        # print(f\"Input ids shape: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        # We need to manually compute loss and get outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # print(f\"Model output type: {type(outputs)}\")\n",
    "            \n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            batch_size = logits_per_image.shape[0]\n",
    "            \n",
    "            # print(f\"Batch size: {batch_size}\")\n",
    "            # print(f\"Logits_per_image shape: {logits_per_image.shape}\")\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "                print(\"WARNING: NaN or Inf DETECTED in model logits.\")\n",
    "                self.skipped_batches += 1\n",
    "                return (None, None, None)\n",
    "            \n",
    "            # Compute loss\n",
    "            if batch_size <= 1:\n",
    "                print(f\"WARNING: Batch size is {batch_size}, cannot compute contrastive loss\")\n",
    "                self.skipped_batches += 1\n",
    "                # Still return outputs for metrics, but no loss\n",
    "                loss = None\n",
    "            else:\n",
    "                labels = torch.eye(batch_size, device=model.device)\n",
    "                loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "                loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "                loss = (loss_images + loss_text) / 2.0\n",
    "                # print(f\"✓ Computed loss: {loss.item():.4f}\")\n",
    "                self.successful_batches += 1\n",
    "        \n",
    "        # Return (loss, logits, labels)\n",
    "        # For contrastive learning, we don't have explicit labels, so return None\n",
    "        # The logits are what compute_metrics will use\n",
    "        logits_tuple = (logits_per_image.cpu(), logits_per_text.cpu())\n",
    "        \n",
    "        return (loss, logits_tuple, None)\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 3: The Main Test Logic\n",
    "# ===================================================================\n",
    "\n",
    "def run_baseline_test():\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- STEP 1: LOAD MODEL AND PROCESSOR ---\n",
    "    print(f\"Loading base model and processor from: {MODEL_ID}\")\n",
    "    \n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "        \n",
    "        # Load the model in its default precision\n",
    "        base_model = AutoModel.from_pretrained(MODEL_ID)\n",
    "        \n",
    "        # NOW, explicitly cast the ENTIRE model to float32 and move to device\n",
    "        base_model = base_model.to(device).float()\n",
    "        \n",
    "        # This debug line will PROVE the model is in float32\n",
    "        print(f\"DEBUG: Model parameters are on device: {next(base_model.parameters()).device}\")\n",
    "        print(f\"DEBUG: Model parameters are dtype: {next(base_model.parameters()).dtype}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}. Check internet connection/model ID.\")\n",
    "        return\n",
    "\n",
    "    # --- STEP 2: LOAD PRE-PREPARED DATA ---\n",
    "    print(f\"Loading pre-prepared data from {EVAL_DATA_FILE}...\")\n",
    "    \n",
    "    if not os.path.exists(EVAL_DATA_FILE):\n",
    "        print(f\"ERROR: {EVAL_DATA_FILE} not found.\")\n",
    "        print(\"Please run 'prepare_data.py' first to create this file.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        eval_data_list = torch.load(EVAL_DATA_FILE, weights_only=False)\n",
    "        eval_dataset = SCIN_List_Dataset(data_list=eval_data_list)\n",
    "        print(f\"Loaded {len(eval_dataset)} valid samples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {EVAL_DATA_FILE}: {e}\")\n",
    "        return\n",
    "    \n",
    "    if len(eval_dataset) == 0:\n",
    "        print(\"ERROR: No valid data was loaded for evaluation. Stopping.\")\n",
    "        return\n",
    "\n",
    "    # --- STEP 3: SET UP TRAINER ---\n",
    "    print(\"Setting up training arguments and trainer...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./temp_eval_test\",\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,  # Enable tqdm to see progress\n",
    "        # IMPORTANT: These ensure predictions are computed\n",
    "        prediction_loss_only=False,\n",
    "    )\n",
    "\n",
    "    trainer = EvalTrainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=lambda data: collate_fn(data, processor),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # --- STEP 4: RUN BASELINE EVALUATION ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING BASELINE EVALUATION...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # This calls trainer.evaluate()\n",
    "    baseline_metrics = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"BATCH STATISTICS:\")\n",
    "    print(f\"  Total batches attempted: {trainer.batch_count}\")\n",
    "    print(f\"  Successful batches: {trainer.successful_batches}\")\n",
    "    print(f\"  Skipped batches: {trainer.skipped_batches}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST COMPLETE. BASELINE METRICS:\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in baseline_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if \"eval_loss\" in baseline_metrics and baseline_metrics[\"eval_loss\"] > 0:\n",
    "        print(\"✓ SUCCESS: 'eval_loss' and other metrics were successfully calculated.\")\n",
    "    else:\n",
    "        print(\"✗ FAILURE: 'eval_loss' is missing or zero. Batches were likely skipped.\")\n",
    "        print(\"\\nDEBUG TIPS:\")\n",
    "        print(\"1. Check if any 'WARNING' messages appeared above\")\n",
    "        print(\"2. Verify your eval_data.pt file has valid image/text pairs\")\n",
    "        print(\"3. Try reducing BATCH_SIZE to see if smaller batches work\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set multiprocessing context if needed\n",
    "    try:\n",
    "        torch.multiprocessing.set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        pass \n",
    "\n",
    "    run_baseline_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
