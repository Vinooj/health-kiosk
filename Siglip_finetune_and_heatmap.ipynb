{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb00da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoo/projects/health-kiosk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model and processor from: google/siglip-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n",
      "Model configured with LoRA. Trainable parameters:\n",
      "trainable params: 1,179,648 || all params: 204,335,618 || trainable%: 0.5773\n",
      "Loading SCIN dataset (streaming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 'train' stream: first 1000 for eval, rest for train.\n",
      "Pre-loading 1000 samples for validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eval samples: 100%|██████████| 1000/1000 [01:06<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SCIN_Iterable_Dataset with provided iterable...\n",
      "Initializing SCIN_List_Dataset with 747 pre-loaded samples.\n",
      "Dataset iterators created. Train (streaming), Eval (747 samples).\n",
      "Setting up training arguments...\n",
      "CustomTrainer initialized with loss_type: sigmoid\n",
      "\n",
      "==================================================\n",
      "RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 07:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Evaluation Metrics:\n",
      "{'eval_loss': 0.6139736771583557, 'eval_model_preparation_time': 0.0025, 'eval_runtime': 5.6339, 'eval_samples_per_second': 132.59, 'eval_steps_per_second': 8.342}\n",
      "\n",
      "==================================================\n",
      "STARTING FINE-TUNING...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 15:07, Epoch 2/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.239409</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.236773</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RUNNING FINAL EVALUATION (AFTER FINE-TUNING)...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Metrics:\n",
      "{'eval_loss': 0.23677285015583038, 'eval_model_preparation_time': 0.0025, 'eval_runtime': 5.5861, 'eval_samples_per_second': 133.726, 'eval_steps_per_second': 8.414, 'epoch': 2.24}\n",
      "\n",
      "==================================================\n",
      "SAVING MODEL AND GENERATING REPORT...\n",
      "==================================================\n",
      "Training complete. LoRA adapter saved to: ./siglip-scin-lora/final-adapter\n",
      "\n",
      "Generating final gradient heatmaps...\n",
      "Heatmap saved to: ./siglip-scin-lora/gradient_impact_heatmap.png\n",
      "Generating final report...\n",
      "\n",
      "Running qualitative similarity test on image: https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Seborrhoeic_keratosis_-_close-up.jpg/800px-Seborrhoeic_keratosis_-_close-up.jpg\n",
      "[ERROR] Failed to generate similarity plot: cannot identify image file <_io.BytesIO object at 0x7b82941f62f0>\n",
      "Final report saved to: ./siglip-scin-lora/final_report.md\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm # Import tqdm for the progress bar\n",
    "\n",
    "# --- Configuration --- #\n",
    "MODEL_ID = \"google/siglip-base-patch16-224\"\n",
    "OUTPUT_DIR = \"./siglip-scin-lora\"\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 16\n",
    "MAX_STEPS = 500\n",
    "\n",
    "# --- [NEW] Experiment Toggle ---\n",
    "# Set this to \"contrastive\" or \"sigmoid\"\n",
    "LOSS_TYPE = \"sigmoid\" # or \"contrastive\"\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 1: The Dataset Loader Code\n",
    "# ===================================================================\n",
    "\n",
    "class SCIN_Iterable_Dataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch IterableDataset for the google/scin dataset.\n",
    "    This class STREAMS the dataset to prevent OOM errors.\n",
    "\n",
    "    This is used for the TRAINING set.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_iterable):\n",
    "        print(f\"Initializing SCIN_Iterable_Dataset with provided iterable...\")\n",
    "        self.dataset = dataset_iterable\n",
    "        self.image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields dictionaries containing processed image and text data.\n",
    "        \"\"\"\n",
    "        for item in self.dataset:\n",
    "            text = item.get(\"related_category\")\n",
    "            if not text or not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            for img_col in self.image_columns:\n",
    "                image = item.get(img_col)\n",
    "                if image and isinstance(image, Image.Image):\n",
    "                    # [FIX] Force convert to RGB to handle PNGs with alpha\n",
    "                    # or other non-standard image modes.\n",
    "                    try:\n",
    "                        yield {\n",
    "                            \"image\": image.convert(\"RGB\"),\n",
    "                            \"text\": text\n",
    "                        }\n",
    "                        break # Move to the next item once an image is found\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting image, skipping: {e}\")\n",
    "                        break\n",
    "\n",
    "\n",
    "# --- [NEW] Map-style Dataset for Evaluation ---\n",
    "class SCIN_List_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that wraps a simple list.\n",
    "    This is used for the VALIDATION set to ensure it's reusable\n",
    "    and works correctly with trainer.evaluate().\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        print(f\"Initializing SCIN_List_Dataset with {len(data_list)} pre-loaded samples.\")\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    \"\"\"\n",
    "    [Robust, Item-by-Item] Data collator.\n",
    "    This version includes explicit checks for None or empty content\n",
    "    to prevent the processor from crashing.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    processed_texts_input_ids = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            if item is None:\n",
    "                # This should be rare, but good to check\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            img = item.get(\"image\")\n",
    "            txt = item.get(\"text\")\n",
    "\n",
    "            # --- THIS IS THE FIX ---\n",
    "            # Explicitly check for bad content *before* calling the processor.\n",
    "            # The processor will crash on None or empty strings.\n",
    "            if img is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            if txt is None or txt.strip() == \"\":\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            # -------------------------\n",
    "\n",
    "            # If we get here, img and txt are valid\n",
    "            inputs = processor(\n",
    "                text=[txt],\n",
    "                images=[img],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=64\n",
    "            )\n",
    "\n",
    "            processed_images.append(inputs[\"pixel_values\"])\n",
    "            processed_texts_input_ids.append(inputs[\"input_ids\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            # This now catches *other* errors, like a truly corrupt image file\n",
    "            print(f\"WARNING (collate_fn): Skipping item {i} due to UNEXPECTED error: {e}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    if not processed_images:\n",
    "        if len(batch) > 0:\n",
    "            print(f\"ERROR: Entire batch was skipped! ({skipped_count} items failed)\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        batch_pixel_values = torch.cat(processed_images, dim=0)\n",
    "        batch_input_ids = torch.cat(processed_texts_input_ids, dim=0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": batch_pixel_values,\n",
    "            \"input_ids\": batch_input_ids\n",
    "            # No attention_mask needed for SigLIP\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final batch collation: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 2: The Custom Trainer (with Heatmap & Metrics)\n",
    "# ===================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Standard Hugging Face method to calculate metrics.\n",
    "    This runs ONCE at the end of evaluation on the accumulated predictions.\n",
    "    \"\"\"\n",
    "    # eval_pred.predictions is the tuple (logits_per_image, logits_per_text)\n",
    "    logits = eval_pred.predictions[0]\n",
    "\n",
    "    # Handle case where it might not be a tuple (though it should be)\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    # In CLIP/SigLIP, the standard task is: given an image, find the correct text in the batch.\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Generate ground truth labels (diagonal alignment)\n",
    "    true_labels = np.arange(len(predictions))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # IMPORTANT: The keys MUST start with \"eval_\"\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1,\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer to:\n",
    "    1. Compute the switchable loss (Contrastive or Sigmoid).\n",
    "    2. Accumulate gradient norms for heatmap generation.\n",
    "    3. Correctly compute loss AND metrics during evaluation by overriding prediction_step.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, loss_type=\"contrastive\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        print(f\"CustomTrainer initialized with loss_type: {self.loss_type}\")\n",
    "\n",
    "        # --- Heatmap Data ---\n",
    "        self.gradient_accumulator = defaultdict(float)\n",
    "        self.step_count = 0\n",
    "\n",
    "        # --- Track batch stats ---\n",
    "        self.successful_batches = 0\n",
    "        self.skipped_batches_eval = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        This is now ONLY used for the TRAINING loss calculation.\n",
    "        Evaluation loss is handled in prediction_step.\n",
    "        \"\"\"\n",
    "        # --- Handle empty batches from collate_fn ---\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "             dummy_loss = torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "             return (dummy_loss, {}) if return_outputs else dummy_loss\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        batch_size = logits_per_image.shape[0]\n",
    "\n",
    "        if batch_size <= 1:\n",
    "             dummy_loss = torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "             return (dummy_loss, {}) if return_outputs else dummy_loss\n",
    "\n",
    "        # --- Switchable Loss Calculation ---\n",
    "        if self.loss_type == \"contrastive\":\n",
    "            labels = torch.arange(batch_size, device=model.device)\n",
    "            loss_images = F.cross_entropy(logits_per_image, labels)\n",
    "            loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "            loss = (loss_images + loss_text) / 2.0\n",
    "        elif self.loss_type == \"sigmoid\":\n",
    "            labels = torch.eye(batch_size, device=model.device)\n",
    "            loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "            loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "            loss = (loss_images + loss_text) / 2.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss_type: {self.loss_type}\")\n",
    "\n",
    "        # Metric calculation is REMOVED from here.\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        \"\"\"\n",
    "        Overrides the training step to hook into gradient accumulation.\n",
    "        \"\"\"\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "        if loss is not None:\n",
    "            self.step_count += 1\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and param.requires_grad:\n",
    "                        self.gradient_accumulator[name] += param.grad.norm().item()\n",
    "        return loss\n",
    "\n",
    "    # --- THIS IS THE CRITICAL NEW FUNCTION ---\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        [CRITICAL FIX]\n",
    "        This function is called by trainer.evaluate() when compute_metrics is set.\n",
    "        We MUST override it to manually compute and return the loss.\n",
    "        \"\"\"\n",
    "        # Handle empty batches from our robust collate_fn\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            self.skipped_batches_eval += 1\n",
    "            return (None, None, None) # Return Nones to skip\n",
    "\n",
    "        # We need to manually compute loss and get outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            batch_size = logits_per_image.shape[0]\n",
    "\n",
    "            # Check for NaN/Inf to prevent crashes (good practice)\n",
    "            if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "                print(\"WARNING: NaN or Inf DETECTED in model logits during eval.\")\n",
    "                self.skipped_batches_eval += 1\n",
    "                return (None, None, None)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = None\n",
    "            if batch_size <= 1:\n",
    "                # Can't compute contrastive loss, so we skip\n",
    "                self.skipped_batches_eval += 1\n",
    "            else:\n",
    "                # --- Re-calculate loss just like in compute_loss ---\n",
    "                if self.loss_type == \"contrastive\":\n",
    "                    labels = torch.arange(batch_size, device=model.device)\n",
    "                    loss_images = F.cross_entropy(logits_per_image, labels)\n",
    "                    loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "                    loss = (loss_images + loss_text) / 2.0\n",
    "                elif self.loss_type == \"sigmoid\":\n",
    "                    labels = torch.eye(batch_size, device=model.device)\n",
    "                    loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "                    loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "                    loss = (loss_images + loss_text) / 2.0\n",
    "\n",
    "                self.successful_batches += 1\n",
    "\n",
    "        # Return (loss, logits, labels)\n",
    "        # We return the logits tuple for compute_metrics\n",
    "        # We return None for labels, as compute_metrics generates them\n",
    "        logits_tuple = (logits_per_image.cpu(), logits_per_text.cpu())\n",
    "\n",
    "        return (loss, logits_tuple, None)\n",
    "\n",
    "    # --- Heatmap Helper Functions (Keep these as they are) ---\n",
    "    def _extract_layer_index(self, name_parts):\n",
    "        for part in name_parts:\n",
    "            if part.isdigit():\n",
    "                return int(part)\n",
    "        return None\n",
    "\n",
    "    def _extract_component_name(self, name_parts):\n",
    "        name_str = \".\".join(name_parts)\n",
    "        if \"lora_A\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA A (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA A (Value)\"\n",
    "        elif \"lora_B\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA B (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA B (Value)\"\n",
    "\n",
    "        if \"q_proj\" in name_str: return \"Query Proj\"\n",
    "        if \"v_proj\" in name_str: return \"Value Proj\"\n",
    "        if \"k_proj\" in name_str: return \"Key Proj\"\n",
    "        if \"fc1\" in name_str: return \"MLP Layer 1\"\n",
    "        if \"fc2\" in name_str: return \"MLP Layer 2\"\n",
    "        return None\n",
    "\n",
    "    def _process_gradients_for_heatmap(self):\n",
    "        if self.step_count == 0:\n",
    "            print(\"No training steps recorded. Skipping heatmap.\")\n",
    "            return None, None, []\n",
    "\n",
    "        vision_data = defaultdict(lambda: defaultdict(float))\n",
    "        text_data = defaultdict(lambda: defaultdict(float))\n",
    "        skipped_params = []\n",
    "\n",
    "        for name, avg_grad_norm in self.gradient_accumulator.items():\n",
    "            avg_norm = avg_grad_norm / self.step_count\n",
    "            parts = name.split('.')\n",
    "            layer_idx = self._extract_layer_index(parts)\n",
    "            component = self._extract_component_name(parts)\n",
    "\n",
    "            if layer_idx is None or component is None:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "                continue\n",
    "\n",
    "            if \"vision_model\" in name:\n",
    "                vision_data[layer_idx][component] = avg_norm\n",
    "            elif \"text_model\" in name:\n",
    "                text_data[layer_idx][component] = avg_norm\n",
    "            else:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "\n",
    "        vision_df = pd.DataFrame.from_dict(vision_data, orient='index').sort_index()\n",
    "        text_df = pd.DataFrame.from_dict(text_data, orient='index').sort_index()\n",
    "\n",
    "        return vision_df, text_df, skipped_params\n",
    "\n",
    "    def plot_final_heatmap(self, save_path):\n",
    "        print(\"\\nGenerating final gradient heatmaps...\")\n",
    "        vision_df, text_df, skipped = self._process_gradients_for_heatmap()\n",
    "\n",
    "        if vision_df is None or (vision_df.empty and text_df.empty):\n",
    "            print(\"No gradient data collected. Skipping heatmap file.\")\n",
    "            return\n",
    "\n",
    "        if skipped:\n",
    "            print(f\"[WARN] Skipped {len(skipped)} LoRA params (couldn't parse name):\")\n",
    "            for i, name in enumerate(skipped[:5]):\n",
    "                print(f\"  ... {name}\")\n",
    "            if len(skipped) > 5: print(f\"  ... and {len(skipped)-5} more.\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "        vmin = 0.0\n",
    "        vmax = max(vision_df.max().max() if not vision_df.empty else 0,\n",
    "                   text_df.max().max() if not text_df.empty else 0)\n",
    "        if vmax == 0:\n",
    "            vmax = 1.0\n",
    "\n",
    "        if not vision_df.empty:\n",
    "            sns.heatmap(vision_df, ax=ax1, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax1.set_title(\"Vision Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax1.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax1.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"No Vision Gradients Found\", ha='center', va='center')\n",
    "            ax1.set_title(\"Vision Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "\n",
    "        if not text_df.empty:\n",
    "            sns.heatmap(text_df, ax=ax2, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax2.set_title(\"Text Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax2.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax2.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No Text Gradients Found\", ha='center', va='center')\n",
    "            ax2.set_title(\"Text Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Heatmap saved to: {save_path}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 3: The Main Training Logic\n",
    "# ===================================================================\n",
    "\n",
    "def main_training():\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- STEP 1: LOAD MODEL AND PROCESSOR ---\n",
    "    print(f\"Loading base model and processor from: {MODEL_ID}\")\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "    # --- [NEW] Load TWO models: one for baseline, one to tune ---\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=dtype\n",
    "    ).to(device)\n",
    "\n",
    "    model_to_tune = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=dtype\n",
    "    ) # LoRA will be applied to this one\n",
    "\n",
    "    # --- STEP 2: CONFIGURE AND APPLY LORA ---\n",
    "    print(\"Applying LoRA configuration...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"], # Target Q and V projections\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    model_to_tune = get_peft_model(model_to_tune, lora_config)\n",
    "    model_to_tune = model_to_tune.to(device)\n",
    "\n",
    "    print(\"Model configured with LoRA. Trainable parameters:\")\n",
    "    model_to_tune.print_trainable_parameters()\n",
    "\n",
    "    # --- STEP 3: LOAD DATASET ---\n",
    "    print(\"Loading SCIN dataset (streaming)...\")\n",
    "\n",
    "    # --- [FIX] Load 'train' split ONCE and split it ---\n",
    "    # We must do this because IterableDataset can't be reused.\n",
    "    try:\n",
    "        base_iterable = load_dataset(\"google/scin\", split=\"train\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset. Error: {e}\")\n",
    "        print(\"Please ensure you have an internet connection and have accepted 'google/scin' terms if any.\")\n",
    "        return\n",
    "\n",
    "    # Split the stream: 1000 for validation, the rest for training\n",
    "    N_VAL_SAMPLES = 1000\n",
    "    print(f\"Splitting 'train' stream: first {N_VAL_SAMPLES} for eval, rest for train.\")\n",
    "\n",
    "    # --- [NEW] Build the eval_data_list in memory ---\n",
    "    eval_data_list = []\n",
    "    image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
    "\n",
    "    print(f\"Pre-loading {N_VAL_SAMPLES} samples for validation set...\")\n",
    "    # Use tqdm for a progress bar\n",
    "    for item in tqdm(base_iterable.take(N_VAL_SAMPLES), total=N_VAL_SAMPLES, desc=\"Loading eval samples\"):\n",
    "        text = item.get(\"related_category\")\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                # [FIX] Force convert to RGB to handle alpha channels, etc.\n",
    "                try:\n",
    "                    eval_data_list.append({\"image\": image.convert(\"RGB\"), \"text\": text})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                break # Move to next item\n",
    "\n",
    "    # Create the training iterable (skipping the ones we took)\n",
    "    train_iterable = base_iterable.skip(N_VAL_SAMPLES)\n",
    "\n",
    "    # --- [NEW] Use the correct Dataset classes ---\n",
    "    train_dataset = SCIN_Iterable_Dataset(dataset_iterable=train_iterable)\n",
    "    eval_dataset = SCIN_List_Dataset(data_list=eval_data_list)\n",
    "\n",
    "    print(f\"Dataset iterators created. Train (streaming), Eval ({len(eval_dataset)} samples).\")\n",
    "\n",
    "\n",
    "    # --- STEP 4: SET UP TRAINING ---\n",
    "    print(\"Setting up training arguments...\")\n",
    "    use_fp16 = True if device == \"cuda\" else False\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        max_steps=MAX_STEPS,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=50,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=250,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        load_best_model_at_end=False,\n",
    "        fp16=use_fp16,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        prediction_loss_only=False,\n",
    "    )\n",
    "\n",
    "    # Initialize our CustomTrainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model_to_tune,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=lambda data: collate_fn(data, processor),\n",
    "        loss_type=LOSS_TYPE,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # --- STEP 5: [NEW] RUN BASELINE EVALUATION ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    trainer.model = base_model # Temporarily swap to the base model\n",
    "    baseline_metrics = trainer.evaluate()\n",
    "    print(\"Baseline Evaluation Metrics:\")\n",
    "    print(baseline_metrics)\n",
    "\n",
    "    # Swap back to the model we actually want to tune\n",
    "    trainer.model = model_to_tune.to(device) # Ensure it's on the device\n",
    "\n",
    "    # --- STEP 6: RUN FINE-TUNING ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING FINE-TUNING...\")\n",
    "    print(\"=\"*50)\n",
    "    trainer.train()\n",
    "\n",
    "    # --- STEP 7: RUN FINAL EVALUATION ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING FINAL EVALUATION (AFTER FINE-TUNING)...\")\n",
    "    print(\"=\"*50)\n",
    "    final_metrics = trainer.evaluate()\n",
    "    print(\"Final Evaluation Metrics:\")\n",
    "    print(final_metrics)\n",
    "\n",
    "    # --- STEP 8: SAVE FINAL MODEL & GENERATE REPORT ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAVING MODEL AND GENERATING REPORT...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    final_adapter_path = os.path.join(OUTPUT_DIR, \"final-adapter\")\n",
    "    model_to_tune.save_pretrained(final_adapter_path)\n",
    "    processor.save_pretrained(final_adapter_path)\n",
    "    print(f\"Training complete. LoRA adapter saved to: {final_adapter_path}\")\n",
    "\n",
    "    # Save heatmap\n",
    "    heatmap_path = os.path.join(OUTPUT_DIR, \"gradient_impact_heatmap.png\")\n",
    "    trainer.plot_final_heatmap(save_path=heatmap_path)\n",
    "\n",
    "    # Generate and save the final text report\n",
    "    generate_final_report(\n",
    "        baseline_metrics,\n",
    "        final_metrics,\n",
    "        base_model,\n",
    "        model_to_tune.to(device), # Ensure tuned model is on device\n",
    "        processor,\n",
    "        OUTPUT_DIR,\n",
    "        device\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 4: Final Report Generation\n",
    "# ===================================================================\n",
    "\n",
    "def generate_final_report(baseline_metrics, final_metrics, base_model, tuned_model, processor, output_dir, device):\n",
    "    \"\"\"\n",
    "    Generates a final markdown report with quantitative and qualitative results.\n",
    "    \"\"\"\n",
    "    print(\"Generating final report...\")\n",
    "    report_path = os.path.join(output_dir, \"final_report.md\")\n",
    "\n",
    "    # --- Part 1: Quantitative Metrics Table ---\n",
    "    report_content = \"# Fine-Tuning Experiment Report\\n\\n\"\n",
    "    report_content += \"## 1. Quantitative Metrics\\n\\n\"\n",
    "    report_content += \"Comparison of model performance on the validation set before and after fine-tuning.\\n\\n\"\n",
    "\n",
    "    report_content += \"| Metric | Baseline (Before) | Fine-Tuned (After) | Change |\\n\"\n",
    "    report_content += \"| :--- | :--- | :--- | :--- |\\n\"\n",
    "\n",
    "    # Helper to format and get metrics\n",
    "    def get_metric(metrics, key, precision=4):\n",
    "        val = metrics.get(key)\n",
    "        if val is None:\n",
    "            return \"N/A\"\n",
    "        return f\"{val:.{precision}f}\"\n",
    "\n",
    "    def get_change(baseline, final, key, precision=4):\n",
    "        b = baseline.get(key)\n",
    "        f = final.get(key)\n",
    "        if b is None or f is None:\n",
    "            return \"N/A\"\n",
    "        change = f - b\n",
    "        sign = \"+\" if change >= 0 else \"\"\n",
    "        return f\"{sign}{change:.{precision}f}\"\n",
    "\n",
    "    metric_keys = [\n",
    "        (\"eval_loss\", \"Eval Loss\"),\n",
    "        (\"eval_accuracy\", \"Accuracy\"),\n",
    "        (\"eval_precision\", \"Precision (Macro)\"),\n",
    "        (\"eval_recall\", \"Recall (Macro)\"),\n",
    "        (\"eval_f1\", \"F1-Score (Macro)\"),\n",
    "        (\"eval_runtime\", \"Eval Runtime (s)\"),\n",
    "    ]\n",
    "\n",
    "    for key, name in metric_keys:\n",
    "        b_val = get_metric(baseline_metrics, key)\n",
    "        f_val = get_metric(final_metrics, key)\n",
    "        c_val = get_change(baseline_metrics, final_metrics, key)\n",
    "        report_content += f\"| **{name}** | {b_val} | {f_val} | {c_val} |\\n\"\n",
    "\n",
    "    # --- Part 2: Qualitative Similarity Test ---\n",
    "    report_content += \"\\n## 2. Qualitative Analysis (Similarity Test)\\n\\n\"\n",
    "    report_content += \"This test shows how the model's understanding of specific concepts changed. We feed a test image and several text probes to both models and compare their similarity scores.\\n\\n\"\n",
    "\n",
    "    # Define a sample image and probes (relevant to SCIN dataset)\n",
    "    # [FIX] Changed to a domain-relevant image (benign keratosis)\n",
    "    SAMPLE_IMG_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Seborrhoeic_keratosis_-_close-up.jpg/800px-Seborrhoeic_keratosis_-_close-up.jpg\"\n",
    "    # [FIX] Use the exact label from the dataset\n",
    "    SAMPLE_IMG_CATEGORY = \"benign keratosis\" # The correct label\n",
    "\n",
    "    # [FIX] Changed to domain-relevant, dataset-matching text probes\n",
    "    TEXT_PROBES = [\n",
    "        SAMPLE_IMG_CATEGORY,\n",
    "        \"melanoma\",\n",
    "        \"nevus\",\n",
    "        \"eczema\",\n",
    "        \"basal cell carcinoma\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nRunning qualitative similarity test on image: {SAMPLE_IMG_URL}\")\n",
    "    sim_report_path = \"N/A\"\n",
    "    try:\n",
    "        # Get similarity scores\n",
    "        response = requests.get(SAMPLE_IMG_URL)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        # [FIX] Must convert this image to RGB as well\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        base_scores = get_similarity_scores(base_model, processor, image, TEXT_PROBES, device)\n",
    "        tuned_scores = get_similarity_scores(tuned_model, processor, image, TEXT_PROBES, device)\n",
    "\n",
    "        # Plot and save the bar chart\n",
    "        sim_report_path = os.path.join(output_dir, \"similarity_report.png\")\n",
    "        plot_similarity_scores(base_scores, tuned_scores, TEXT_PROBES, SAMPLE_IMG_CATEGORY, sim_report_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to generate similarity plot: {e}\")\n",
    "        report_content += f\"**Failed to generate similarity plot:** `{e}`\\n\"\n",
    "\n",
    "    if sim_report_path != \"N/A\":\n",
    "        report_content += f\"**Test Image URL:** {SAMPLE_IMG_URL}\\n\"\n",
    "        report_content += f\"**Test Image Category:** `{SAMPLE_IMG_CATEGORY}`\\n\\n\"\n",
    "        report_content += \"![Similarity Plot](similarity_report.png)\\n\"\n",
    "        report_content += \"\\n**Interpretation:** Ideally, the 'Baseline' model is confused (low, flat scores), while the 'Fine-Tuned' model shows a clear, high score for the correct category.\\n\"\n",
    "\n",
    "    # --- Part 3: Gradient Impact Heatmap ---\n",
    "    report_content += \"\\n## 3. Gradient Impact Heatmap\\n\\n\"\n",
    "    report_content += \"This heatmap shows which parts of the model were changed the most during fine-tuning (average L2 norm of gradients).\\n\\n\"\n",
    "    report_content += \"![Gradient Impact Heatmap](gradient_impact_heatmap.png)\\n\"\n",
    "    report_content += \"\\n**Interpretation:** Brighter colors (e.g., yellow) indicate layers and components that were heavily modified by the fine-tuning process. Darker colors (e.g., purple) indicate parts of the model that were left mostly unchanged.\\n\"\n",
    "\n",
    "    # --- Save the final report ---\n",
    "    try:\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "        print(f\"Final report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to save final report: {e}\")\n",
    "\n",
    "def get_similarity_scores(model, processor, image, text_probes, device):\n",
    "    \"\"\"Helper function to get model similarity scores for a single image and text list.\"\"\"\n",
    "    inputs = processor(\n",
    "        text=text_probes,\n",
    "        images=[image], # Pass image as a list\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval() # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # We use logits_per_image. We have 1 image, N texts. Shape (1, N)\n",
    "        # Sigmoid gives a 0-1 \"probability\"\n",
    "        scores = torch.sigmoid(outputs.logits_per_image).cpu().numpy().flatten()\n",
    "    return scores\n",
    "\n",
    "def plot_similarity_scores(base_scores, tuned_scores, probes, true_category, save_path):\n",
    "    \"\"\"Generates a bar chart comparing baseline and fine-tuned scores.\"\"\"\n",
    "\n",
    "    df_data = {\n",
    "        \"Text Probe\": probes * 2,\n",
    "        \"Similarity Score\": np.concatenate([base_scores, tuned_scores]),\n",
    "        \"Model\": [\"Baseline\"] * len(probes) + [\"Fine-Tuned\"] * len(probes)\n",
    "    }\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Set colors\n",
    "    colors = []\n",
    "    for probe in probes:\n",
    "        colors.append(\"red\" if probe == true_category else \"grey\")\n",
    "    colors = colors * 2 # Apply to both baseline and tuned\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"Text Probe\",\n",
    "        y=\"Similarity Score\",\n",
    "        hue=\"Model\",\n",
    "        palette={\"Baseline\": \"lightblue\", \"Fine-Tuned\": \"darkblue\"}\n",
    "    )\n",
    "\n",
    "    # Add highlighting to the correct category\n",
    "    ax = plt.gca()\n",
    "    for i, probe in enumerate(probes):\n",
    "        if probe == true_category:\n",
    "            ax.get_xticklabels()[i].set_color(\"red\")\n",
    "            ax.get_xticklabels()[i].set_fontweight(\"bold\")\n",
    "\n",
    "    plt.title(f\"Qualitative Similarity Test (True Category: {true_category})\", fontsize=16)\n",
    "    plt.ylabel(\"Similarity Score (Sigmoid)\", fontsize=12)\n",
    "    plt.xlabel(\"Text Probes\", fontsize=12)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.legend(title=\"Model\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity plot saved to: {save_path}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 5: Run the Code\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        torch.multiprocessing.set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    main_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
