{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using custom loss function**\n",
        "\n",
        "https://www.youtube.com/watch?v=Hm8_PgVTFuc"
      ],
      "metadata": {
        "id": "tE33S9rZp8So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch torchvision transformers datasets peft accelerate Pillow"
      ],
      "metadata": {
        "id": "0w0-kRIDha7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModel,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# --- Configuration --- #\n",
        "# This section defines key parameters that control the model, training, and LoRA configuration.\n",
        "# Adjusting these values can significantly impact performance, training time, and resource usage.\n",
        "MODEL_ID = \"google/siglip-base-patch16-224\" # The base pre-trained SigLIP model to fine-tune.\n",
        "                                           # Importance: This is the foundation of our fine-tuned model.\n",
        "                                           # Rationale: SigLIP models are good for vision-language tasks.\n",
        "OUTPUT_DIR = \"./siglip-scin-lora\"         # Directory to save the fine-tuned LoRA adapter and processor.\n",
        "                                           # Importance: Essential for model persistence and later inference.\n",
        "BATCH_SIZE = 16                            # Number of samples processed in parallel during training and evaluation.\n",
        "                                           # Impact (Higher): Faster training per epoch, but higher memory usage.\n",
        "                                           #                  Can lead to poorer generalization if too large.\n",
        "                                           # Impact (Lower): Slower training, less memory usage.\n",
        "                                           #                 Can lead to more noisy gradients but potentially better generalization.\n",
        "LEARNING_RATE = 1e-4                       # The initial learning rate for the optimizer.\n",
        "                                           # Importance: Controls the step size during model weight updates.\n",
        "                                           # Impact (Higher): Model may converge faster but risk overshooting the optimum (divergence).\n",
        "                                           # Impact (Lower): Slower convergence, but potentially more stable training and a better optimum.\n",
        "LORA_RANK = 16                             # The rank (r) of the low-rank matrices in LoRA.\n",
        "                                           # Importance: Determines the capacity of the LoRA adapter. Higher rank allows more expressiveness.\n",
        "                                           # Impact (Higher): More trainable parameters, higher memory usage, potentially better performance\n",
        "                                           #                  but increased risk of overfitting and slower training.\n",
        "                                           # Impact (Lower): Fewer trainable parameters, lower memory usage, faster training,\n",
        "                                           #                 but might not capture complex relationships (underfitting).\n",
        "LORA_ALPHA = 16                            # Scaling factor for the LoRA update. Often `lora_alpha = lora_rank`.\n",
        "                                           # Importance: Scales the impact of the LoRA weights.\n",
        "                                           # Impact (Higher): LoRA adapters have a stronger influence on the base model.\n",
        "                                           # Impact (Lower): LoRA adapters have a weaker influence.\n",
        "# --------------------- #\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "#  Part 1: The Dataset Loader Code\n",
        "# ===================================================================\n",
        "\n",
        "class SCIN_Dataset(torch.utils.data.IterableDataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch IterableDataset for the google/scin dataset.\n",
        "    This class STREAMS the dataset to prevent OOM errors.\n",
        "\n",
        "    Importance: Streaming datasets are crucial for large datasets that cannot fit into memory,\n",
        "                preventing Out-Of-Memory (OOM) errors and allowing training on resource-constrained systems.\n",
        "    Rationale: The `IterableDataset` does not preload the entire dataset. Instead, it loads items\n",
        "               one by one as requested, making it memory-efficient.\n",
        "    \"\"\"\n",
        "    def __init__(self, split=\"train\"):\n",
        "        print(f\"Loading 'google/scin' dataset in STREAMING mode: {split}...\")\n",
        "        try:\n",
        "            # Loads the 'google/scin' dataset in streaming mode.\n",
        "            # 'streaming=True' ensures that data is loaded on-the-fly, not all at once.\n",
        "            self.dataset = load_dataset(\"google/scin\", split=split, streaming=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load dataset 'google/scin'. Error: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Defines the columns in the dataset that contain image paths.\n",
        "        # The iterator will loop through these to find valid image data.\n",
        "        self.image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Yields dictionaries containing processed image and text data.\n",
        "        Importance: This method defines how individual data samples are provided to the DataLoader.\n",
        "        \"\"\"\n",
        "        for item in self.dataset:\n",
        "            # Extracts the 'related_category' as the text label.\n",
        "            text = item.get(\"related_category\")\n",
        "            # Skips items without a valid text label.\n",
        "            if not text or not isinstance(text, str):\n",
        "                continue\n",
        "\n",
        "            # Iterates through potential image columns to find an actual image.\n",
        "            for img_col in self.image_columns:\n",
        "                image = item.get(img_col)\n",
        "                # If a valid PIL Image is found, yield it along with the text.\n",
        "                if image and isinstance(image, Image.Image):\n",
        "                    yield {\n",
        "                        \"image\": image,\n",
        "                        \"text\": text\n",
        "                    }\n",
        "\n",
        "def collate_fn(batch, processor):\n",
        "    \"\"\"\n",
        "    Data collator for batching.\n",
        "    Importance: This function takes a list of individual samples (output of __iter__)\n",
        "                and combines them into a single batch suitable for model input.\n",
        "    Rationale: Models typically process data in batches for efficiency. This function\n",
        "               handles tasks like tokenization, image preprocessing, and padding to create uniform batches.\n",
        "    \"\"\"\n",
        "    # Separates images and texts from the incoming batch.\n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "\n",
        "    try:\n",
        "        # Uses the AutoProcessor to prepare both text and images for the model.\n",
        "        # `text`: List of text labels.\n",
        "        # `images`: List of PIL images.\n",
        "        # `return_tensors=\"pt\"`: Returns PyTorch tensors.\n",
        "        # `padding=\"max_length\"`: Pads sequences to the `max_length`.\n",
        "        # `truncation=True`: Truncates sequences longer than `max_length`.\n",
        "        # `max_length=64`: The maximum sequence length for tokenization.\n",
        "        #                  Importance: Matches the model's expected input size.\n",
        "        #                  Impact (Higher): Can capture more context but increases memory and computation.\n",
        "        #                  Impact (Lower): Faster, less memory, but might lose important information if text is long.\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=64 # Correct max length for SigLIP\n",
        "        )\n",
        "        return inputs\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing batch: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "#  Part 2: The Custom Trainer\n",
        "# ===================================================================\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom Trainer to compute the contrastive loss (SigLIP loss).\n",
        "\n",
        "    Importance: Overriding `compute_loss` allows us to implement specific loss functions\n",
        "                that are not directly provided by the standard Hugging Face `Trainer`.\n",
        "    Rationale: SigLIP models are trained using a contrastive loss, specifically a symmetric\n",
        "               cross-entropy loss between image and text embeddings. This custom trainer\n",
        "               implements that specific loss formulation.\n",
        "    \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Get the model's outputs (logits).\n",
        "        # The model takes processed inputs (image and text features) and outputs\n",
        "        # similarity scores (logits) between images and texts.\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # `logits_per_image` are the similarity scores where each row corresponds to an image\n",
        "        # and columns correspond to texts.\n",
        "        # `logits_per_text` are the similarity scores where each row corresponds to a text\n",
        "        # and columns correspond to images.\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        logits_per_text = outputs.logits_per_text\n",
        "\n",
        "        # Create the contrastive labels.\n",
        "        # For contrastive learning, the diagonal elements of the similarity matrix represent\n",
        "        # positive pairs (e.g., image[i] with text[i]), and off-diagonal are negative pairs.\n",
        "        # `labels` is an identity matrix implicitly created by `torch.arange`, where `labels[i]`\n",
        "        # corresponds to the correct text for `image[i]` (and vice-versa).\n",
        "        batch_size = logits_per_image.shape[0]\n",
        "        labels = torch.arange(batch_size, device=model.device) # e.g., [0, 1, 2, ..., batch_size-1]\n",
        "\n",
        "        # Calculate the symmetric cross-entropy loss.\n",
        "        # `F.cross_entropy` computes the loss between the logits and the true labels.\n",
        "        # `loss_images`: Measures how well images predict their corresponding texts.\n",
        "        # `loss_text`: Measures how well texts predict their corresponding images.\n",
        "        loss_images = F.cross_entropy(logits_per_image, labels)\n",
        "        loss_text = F.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "        # The total loss is the average of the two cross-entropy losses, making it symmetric.\n",
        "        loss = (loss_images + loss_text) / 2.0\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "#  Part 3: The Main Training Logic\n",
        "# ===================================================================\n",
        "\n",
        "def main_training():\n",
        "\n",
        "    # Determine the available device (GPU or CPU) for computation.\n",
        "    # Importance: Utilizing a GPU (CUDA or MPS) significantly speeds up training for deep learning models.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- STEP 1: LOAD MODEL AND PROCESSOR ---\n",
        "    print(f\"Loading base model and processor from: {MODEL_ID}\")\n",
        "\n",
        "    # The processor handles image transformations (e.g., resizing, normalization) and text tokenization.\n",
        "    # Importance: Ensures input data is in the correct format for the model.\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "    # Set torch_dtype based on device to leverage mixed-precision training on CUDA devices.\n",
        "    # Importance: `float16` (half-precision) reduces memory usage and speeds up computations on compatible hardware\n",
        "    #             (like NVIDIA GPUs with Tensor Cores), while maintaining sufficient accuracy.\n",
        "    # Rationale: Training in `float32` (full-precision) for non-CUDA devices to ensure compatibility and stability.\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    # Loads the pre-trained SigLIP model from the Hugging Face Hub.\n",
        "    # Importance: Provides a strong starting point for fine-tuning, leveraging knowledge learned from massive datasets.\n",
        "    model = AutoModel.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=dtype # Applies the selected precision.\n",
        "    )\n",
        "\n",
        "    # --- STEP 2: CONFIGURE AND APPLY LORA ---\n",
        "    print(\"Applying LoRA configuration...\")\n",
        "\n",
        "    # Configures Low-Rank Adaptation (LoRA).\n",
        "    # Importance: LoRA significantly reduces the number of trainable parameters during fine-tuning,\n",
        "    #             making it more memory-efficient and faster than full fine-tuning, while achieving comparable performance.\n",
        "    # Rationale: Instead of updating all model weights, LoRA injects small, trainable low-rank matrices into the model.\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_RANK,                         # Rank of the update matrices. See `LORA_RANK` above.\n",
        "        lora_alpha=LORA_ALPHA,               # LoRA scaling factor. See `LORA_ALPHA` above.\n",
        "        target_modules=[\"q_proj\", \"v_proj\"], # Modules within the base model to apply LoRA to.\n",
        "                                           # Importance: These are typically attention projection layers where LoRA is effective.\n",
        "        lora_dropout=0.1,                    # Dropout probability for the LoRA layers.\n",
        "                                           # Importance: Helps prevent overfitting by randomly setting some LoRA weights to zero during training.\n",
        "                                           # Impact (Higher): More regularization, less likely to overfit but can underfit.\n",
        "                                           # Impact (Lower): Less regularization, more likely to overfit.\n",
        "        bias=\"none\",                         # How to handle bias terms. \"none\" means no bias is added to LoRA layers.\n",
        "                                           # Importance: Simplifies the LoRA layers and often sufficient.\n",
        "    )\n",
        "\n",
        "    # Applies the LoRA configuration to the base model, creating a PeftModel.\n",
        "    # This wraps the original model with trainable LoRA layers.\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model = model.to(device) # Moves the model to the selected device.\n",
        "\n",
        "    print(\"Model configured with LoRA. Trainable parameters:\")\n",
        "    # Prints a summary of trainable parameters, highlighting the significant reduction due to LoRA.\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # --- STEP 3: LOAD DATASET ---\n",
        "    print(\"Loading SCIN dataset (streaming)...\")\n",
        "\n",
        "    # Initializes the streaming datasets for training and evaluation.\n",
        "    # Rationale: Using the custom `SCIN_Dataset` ensures memory-efficient loading.\n",
        "    # Note: For this example, eval_dataset also uses 'train' split for simplicity,\n",
        "    #       but in a real scenario, you'd use a separate 'validation' or 'test' split.\n",
        "    train_dataset = SCIN_Dataset(split=\"train\")\n",
        "    eval_dataset = SCIN_Dataset(split=\"train\") # Using train for eval as per original notebook\n",
        "\n",
        "    print(\"Dataset iterators created.\")\n",
        "\n",
        "    # --- STEP 4: SET UP TRAINING ---\n",
        "    print(\"Setting up training arguments...\")\n",
        "\n",
        "    # Enable fp16 only if on CUDA to utilize mixed-precision benefits.\n",
        "    use_fp16 = True if device == \"cuda\" else False\n",
        "\n",
        "    # Defines the training arguments using Hugging Face's TrainingArguments.\n",
        "    # Importance: Configures various aspects of the training loop.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,                     # Directory for model checkpoints and logs.\n",
        "        per_device_train_batch_size=BATCH_SIZE,    # Batch size per GPU/CPU for training. See `BATCH_SIZE` above.\n",
        "        per_device_eval_batch_size=BATCH_SIZE,     # Batch size per GPU/CPU for evaluation. See `BATCH_SIZE` above.\n",
        "        max_steps=500,                             # Total number of training steps.\n",
        "                                                   # Importance: Controls the duration of training.\n",
        "                                                   # Impact (Higher): More training, potentially better performance, but longer time and risk of overfitting.\n",
        "                                                   # Impact (Lower): Faster training, but potential underfitting if model doesn't learn enough.\n",
        "        weight_decay=0.01,                         # Strength of L2 regularization.\n",
        "                                                   # Importance: Helps prevent overfitting by penalizing large weights.\n",
        "                                                   # Impact (Higher): Stronger regularization, can lead to underfitting.\n",
        "                                                   # Impact (Lower): Weaker regularization, can lead to overfitting.\n",
        "        learning_rate=LEARNING_RATE,               # Initial learning rate. See `LEARNING_RATE` above.\n",
        "        warmup_steps=50,                           # Number of steps for linear learning rate warmup.\n",
        "                                                   # Importance: Gradually increases learning rate from zero, helping stabilize training at the beginning.\n",
        "        logging_steps=10,                          # Log training metrics every N steps.\n",
        "        save_strategy=\"steps\",                     # Save model checkpoint based on steps.\n",
        "        save_steps=250,                            # Save a checkpoint every N steps.\n",
        "        eval_strategy=\"steps\",                     # Evaluate model performance based on steps.\n",
        "        eval_steps=250,                            # Run evaluation every N steps.\n",
        "\n",
        "        # --- [THE FIX] --- #\n",
        "        # We set this to False because our IterableDataset doesn't produce the 'eval_loss' metric\n",
        "        # in a way that `load_best_model_at_end` can reliably track. This prevents errors.\n",
        "        # Importance: Essential when using iterable datasets or custom evaluation metrics.\n",
        "        load_best_model_at_end=False,\n",
        "\n",
        "        fp16=use_fp16,                             # Enables mixed-precision training if CUDA is available.\n",
        "        report_to=\"none\",                          # Disables integration with experiment tracking tools (e.g., Weights & Biases).\n",
        "                                                   # Rationale: Simplifies the example by not requiring external logging setup.\n",
        "        remove_unused_columns=False,               # Prevents the Trainer from removing columns not used by the model's forward pass.\n",
        "                                                   # Importance: Necessary when your dataset has extra columns you might need later or for custom processing.\n",
        "    )\n",
        "\n",
        "    # Initializes the custom trainer with the LoRA-enabled model, arguments, datasets, and collator.\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=lambda data: collate_fn(data, processor), # Uses our custom collate function.\n",
        "    )\n",
        "\n",
        "    # --- STEP 5: RUN FINE-TUNING ---\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    trainer.train() # Initiates the training loop.\n",
        "\n",
        "    # Define the path to save the final LoRA adapter.\n",
        "    final_adapter_path = os.path.join(OUTPUT_DIR, \"final-adapter\")\n",
        "    # Saves only the trainable LoRA weights, not the entire base model.\n",
        "    # Importance: This is a key advantage of LoRA, as it produces a small, portable adapter file.\n",
        "    model.save_pretrained(final_adapter_path)\n",
        "    # Saves the processor used during training, ensuring consistency during inference.\n",
        "    processor.save_pretrained(final_adapter_path)\n",
        "\n",
        "    print(f\"Training complete. LoRA adapter saved to: {final_adapter_path}\")\n",
        "\n",
        "# ===================================================================\n",
        "#  Part 4: Run the Code\n",
        "# ===================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Sets the start method for multiprocessing to 'spawn'.\n",
        "    # Importance: This is often necessary in PyTorch when using CUDA with multiprocessing\n",
        "    #             to prevent issues like deadlocks or unexpected behavior.\n",
        "    # Rationale: 'spawn' creates fresh, independent child processes, avoiding resource conflicts.\n",
        "    try:\n",
        "        torch.multiprocessing.set_start_method('spawn')\n",
        "    except RuntimeError:\n",
        "        pass # Ignore if it's already set or not needed.\n",
        "\n",
        "    main_training() # Calls the main training function to start the process."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944,
          "referenced_widgets": [
            "36afcec51da4473fa81fd63fd3e4cd52",
            "c0a85a899aa74d33b4fb081694d436fd",
            "5b1585fdf9a748d7915a78e378168dea",
            "4f95b6a16e2c46568e6a0a10af99af71",
            "559fd6a05eb94541bccf390ba1f7ae28",
            "c6be8dc47fbe4f688b6ec640659faebf",
            "99bc48baf36449698043c01698ff4a24",
            "3bc96660588246cd9f4c29b1db77edc7",
            "7a497314f7404ced9dd84fd76519d94c",
            "5e528c8720924ad38b933bec9b92fd2e",
            "96963ca7272c45f1ae0dd22b7d0ccab5",
            "8aeaceb1359c48bc9fd89a1b051475a0",
            "3f49df9069e84b898072cfe48c649215",
            "e732f79993f346c89310e8d8e5b7bab0",
            "df716521e4704546b17afb75a26cc2f5",
            "2259134e187440c99a8205659fee5f25",
            "5bdd8b62a7fa45cb9764aff4117926b4",
            "ced952ec4ecc4157abe85addd049c801",
            "7ddd333a09c84a0daa671c2a4ea1355a",
            "9314b57205a3466085d11fe15f962e34",
            "abf9c190ea44495e958571893befebb0",
            "633b7d454308402984743b1dad766fb2",
            "b93e007979cf4f68a570f091a486d730",
            "07a83933c5b44e51aa7ca61ef76ba46f",
            "2d20df4ac8b44c4f92db6c8c6cb4b67e",
            "52d8d156d4ba43578e7c2c804f3d832b",
            "343de7fa4d524658b9d480168f1517bd",
            "f31c632bc1854b8da7efd6a452c13ac0",
            "023733e8b84a47f1b54aa9fb2ed2f395",
            "71212542375c48e783c65658234a8b01",
            "c3d7f16be2f6415ba97d65e68a306a41",
            "2eeed661a9b048858b875fd2938fc936",
            "94720683c5be4a9ba41d1a4454d8a40e",
            "6a381836cfba42439dca437ec7c98be9",
            "ec8f929e4fcf42ffa127d621501dde83",
            "87e468342c934a81a17eaf53e44ecbae",
            "fe43724deebe4ee88c5e18212ad9636c",
            "a0fe80ebc11b4c34824308cac1ba3e84",
            "9fb302a89dd44a6d8bd16555be1b170e",
            "2a49169c7a364dfebfbdd54f0a6a62e5",
            "5d995d6a92fe48b0b2b82f812cfae71c",
            "c3d2c83e97a148cf809abe36d7153d77",
            "db29f4267d284fe28137e51a5ad01140",
            "6c3e15a3c9cc482a9a65bbb090291352",
            "f3e9ad5ff55e4139b2ed38c302ae2e4c",
            "a2776667febc4be6a6fe0521b2b111b2",
            "609e684ba60f4167a31f189ade17ce07",
            "8718c849e91c4ff1bb786e3204f77965",
            "246dc17f22b349bea3e789dc5bbc49c1",
            "56b7529b0c064b728b213f0dcef0a1b1",
            "04dcea6c4a934962aced3d0da3dd1f39",
            "ed7653debb364e0791760fffdd781302",
            "36a333da1e9b45b38fa1a593cc88eb50",
            "b3d9e775b98047389844159afab58e73",
            "fd319e97691449439360233f002bff0b",
            "76cc3263e66d44a5afa02cdc98e66c62",
            "25cfedea5b1f47c09b6e32a1a05f9e9e",
            "72447a62cfa2426f8bf5fb6d3277fad7",
            "16651a32ed8544899e1402a38182f65f",
            "3ebaa2bd58864c6ea0e98aaa327f5ed8",
            "0e0be3eacf6b4b20931ac0dd712aa752",
            "f94086a5b6ba48dfacea37546f20c1e2",
            "f9613822b69b40799da43245c3047986",
            "bca68475b067426782d55e1b59ae1322",
            "aad7db07d41e4c649e108e48a60f5778",
            "a05e8996ddf9436697c6e62e471797cb",
            "d0ff152a961b4524979924ca2b8b306a",
            "075e4e83defd4708a24ae922efb1b05b",
            "6748dfa08d3042cd9520252f046cfd07",
            "65bc63eecf97456d8d9e2bae9bf62ed0",
            "f50ff498c1a34363b4a854c8ef76d131",
            "53c18b99b9794279858212443513b21f",
            "c9de54c62793457db82b9650df1d2867",
            "d542a7ae4b104284a7222c97dbb7bedf",
            "0ff9a9f4ef3941a7a35f37ee9938d0cf",
            "7d4381163b304b0ab554476435bbb539",
            "c819fa7e92ee4bdb8d71117d0a6555e3",
            "342e82eaab0d43b8a1fba8ba6fc131c7",
            "4ba757378fb04d89a5b8d1921b5d219c",
            "86facc7601f544b8b872353ae41540ca",
            "b08c46d692d34032bac26cc958f87e78",
            "1399a2c7ef5b49c3b29e98cf1bbe070b",
            "9c141caece1f4ac8b4d79e37b66d8e0b",
            "b51396531c3648dcb1de1b23b085d96b",
            "de85b3c540424ea5afb78b2ffcb95695",
            "0d1b14c826f1475f9dc0d82def1b653b",
            "f8890aff2c944f8aa2d9df0435a56a87",
            "68ad725e8dfd4baebf8517c40816aaad",
            "06695fda2c6242239771c680a2a91a99",
            "67543c215fb04af98034c73acec28be7",
            "3ba6debc758b4bddb63456c1b516e6dd",
            "86d3cea3b77844a49b6c2962bd38336c",
            "c3e7b350d2dc4b928b98087bf14221ca",
            "0eaf8808cd28480a9b51a75afa9a134e",
            "58369405aaf64f7ba18e6742ef9be31c",
            "3203bf6abd2047b4844b9c24c44c8976",
            "b93a8487bda64af083e6040af5c6bb84",
            "e1f4069379b24e0292e2bb169da9b731",
            "1fcdcfdb469545f5be3543c4193df0d0",
            "c6dd7083762e44528b4b863114938e67",
            "c68d6fee3e574508b3c2b849d9dd4282",
            "5eac1c7cd7054f17b025b3a237254e87",
            "f6db960942354730b3558328b87b0698",
            "1a646270b56146b2b940ba4992521f7f",
            "bd4b19d05b454d2d89b0df8d57519788",
            "9b3170140fc0400f9a0c3d2aeb4b660c",
            "defb3f646db74e6c9ae827c82556cfd6",
            "6e1e41362e5d4634b3f19e82663406ce",
            "a43e88e8e25f4bae9ce132e0bc316a77",
            "5778bde9842b4103a5480ff6dae9e528",
            "91c72177466f4519abc29cd4d91855da",
            "6599ea776b2446a4bac7bc99236ec9af",
            "96f4b07dd0f04ec097c4db2478b6d6c9",
            "a057e1f126664374bc2e92b9d20d1129",
            "ca63d55f45924eeba97d76917baae716",
            "fbe8c18bf985402e9fa8c30e9f5eed83",
            "b3b9238fca2c47b89eee2d3c61490084",
            "fccfccda69d748c898f1502aca775310",
            "381796ae04f44d188c4ef9f4bd73b9f2",
            "4213b6059d78409087d9218bdde519c8",
            "278e39d39eb049968bf0cedd93de7226",
            "e5010e0114fb4464b9e3811395521521",
            "69fb669f8a8e4b919ec3ac0c581f0790",
            "6aebab09c10d44daab8ba8584e774f6e",
            "8fc1585349b94d26961d3b0c2ba2e597",
            "30909da88f194ae1afa09e57bdd70fe4",
            "b63581576a884ef5a78f95a97f4ac2be",
            "58abee31bf8042fdbb8df0eb9aae604d",
            "3e389c51dbca48a19d34b850a9053431",
            "1ff911073f624e74afe927287a62be83",
            "bd2943cb0a434e019a3e6f67cb96478e",
            "4fda8723a96940958d3c516ab68879d9"
          ]
        },
        "id": "noE2JxXopyNc",
        "outputId": "3e4b7811-fbea-4b93-b837-579f0a859759"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading base model and processor from: google/siglip-base-patch16-224\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36afcec51da4473fa81fd63fd3e4cd52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8aeaceb1359c48bc9fd89a1b051475a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b93e007979cf4f68a570f091a486d730",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a381836cfba42439dca437ec7c98be9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3e9ad5ff55e4139b2ed38c302ae2e4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76cc3263e66d44a5afa02cdc98e66c62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0ff152a961b4524979924ca2b8b306a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/813M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA configuration...\n",
            "Model configured with LoRA. Trainable parameters:\n",
            "trainable params: 1,179,648 || all params: 204,335,618 || trainable%: 0.5773\n",
            "Loading SCIN dataset (streaming)...\n",
            "Loading 'google/scin' dataset in STREAMING mode: train...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "342e82eaab0d43b8a1fba8ba6fc131c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06695fda2c6242239771c680a2a91a99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "WARNING:datasets.load:Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6dd7083762e44528b4b863114938e67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'google/scin' dataset in STREAMING mode: train...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c72177466f4519abc29cd4d91855da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "WARNING:datasets.load:Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5010e0114fb4464b9e3811395521521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset iterators created.\n",
            "Setting up training arguments...\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='358' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [358/500 13:17 < 05:18, 0.45 it/s, Epoch 0.71/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.743900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 23:05, Epoch 1/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.743900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.791500</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. LoRA adapter saved to: ./siglip-scin-lora/final-adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\ud83d\ude80 How to Load the Model for Inference"
      ],
      "metadata": {
        "id": "rh4IKowZr-Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from peft import PeftModel\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Define your models and device\n",
        "base_model_id = \"google/siglip-base-patch16-224\"\n",
        "adapter_path = \"./siglip-scin-lora/final-adapter\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# 2. Load the base model (the original, large model)\n",
        "base_model = AutoModel.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ")\n",
        "\n",
        "# 3. Load the processor (this was saved with your adapter)\n",
        "processor = AutoProcessor.from_pretrained(adapter_path)\n",
        "\n",
        "# 4. Load and apply the LoRA adapter\n",
        "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# 5. [Recommended] Merge for faster inference\n",
        "# This combines the LoRA weights back into the base model.\n",
        "# After this, it's just like a regular model.\n",
        "model = model.merge_and_unload()\n",
        "model = model.to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "print(\"Model loaded and ready for inference!\")"
      ],
      "metadata": {
        "id": "nBiw2zKZr8Yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cbe40a-b6bd-4b84-dcb1-1a1367fe8d04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LoRA adapter from: ./siglip-scin-lora/final-adapter\n",
            "Model loaded and ready for inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\u2705 How to Test If It Trained Correctly"
      ],
      "metadata": {
        "id": "nEe7hU8xsCjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from peft import PeftModel\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "# Suppress harmless warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "@torch.no_grad() # We don't need to calculate gradients for testing\n",
        "def test_model():\n",
        "    # --- 1. Load the fine-tuned model (same as above) ---\n",
        "    base_model_id = \"google/siglip-base-patch16-224\"\n",
        "    adapter_path = \"./siglip-scin-lora/final-adapter\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    base_model = AutoModel.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(adapter_path)\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    model = model.merge_and_unload()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(\"Fine-tuned model loaded.\")\n",
        "\n",
        "    # --- 2. Get a test image ---\n",
        "    # We'll stream one item from the dataset to use as a test\n",
        "    # This is an image the model *might* have seen, but it's a good first check.\n",
        "    # For a *real* test, you should use a completely new, unseen image.\n",
        "    try:\n",
        "        test_data = next(iter(load_dataset(\"google/scin\", split=\"train\", streaming=True)))\n",
        "\n",
        "        # Find the first valid image in the test item\n",
        "        test_image = None\n",
        "        for col in [\"image_1_path\", \"image_2_path\", \"image_3_path\"]:\n",
        "            if test_data[col] and isinstance(test_data[col], Image.Image):\n",
        "                test_image = test_data[col]\n",
        "                break\n",
        "\n",
        "        if test_image is None:\n",
        "            print(\"Error: Could not load a test image from the dataset.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Test image loaded. The correct label is: {test_data['related_category']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load test image: {e}\")\n",
        "        print(\"Please provide your own image by using: test_image = Image.open('path/to/your/image.jpg')\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Define your text labels ---\n",
        "    # These are the \"classes\" we want to choose from\n",
        "    text_labels = [\n",
        "        \"an image of ACNE\",\n",
        "        \"an image of ECZEMA\",\n",
        "        \"an image of a RASH\",\n",
        "        \"a photo of a MOLE\",\n",
        "        \"a picture of PSORIASIS\",\n",
        "        \"a photo of healthy, normal skin\"\n",
        "    ]\n",
        "    print(f\"Testing against labels: {text_labels}\")\n",
        "\n",
        "    # --- 4. Process the image and text ---\n",
        "    # Note: We process the image once and the text labels all at once\n",
        "    inputs = processor(\n",
        "        text=text_labels,\n",
        "        images=[test_image], # Pass the image as a list\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\", # Pad text to the max length\n",
        "        truncation=True,\n",
        "        max_length=64 # Use the 64-token limit\n",
        "    ).to(device)\n",
        "\n",
        "    # --- 5. Get model predictions ---\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # This gives us the similarity scores\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "\n",
        "    # We apply softmax to turn scores into probabilities (0% to 100%)\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # --- 6. Show the results ---\n",
        "    print(\"\\n--- Test Results ---\")\n",
        "\n",
        "    # Get the top 3 predictions\n",
        "    top_k_values, top_k_indices = torch.topk(probs, 3)\n",
        "\n",
        "    for i in range(top_k_values.shape[1]):\n",
        "        value = top_k_values[0, i].item() * 100 # as percentage\n",
        "        label_index = top_k_indices[0, i].item()\n",
        "        label_name = text_labels[label_index]\n",
        "        print(f\"{i+1}. Predicted Label: {label_name:<25} | Confidence: {value:2.2f}%\")\n",
        "\n",
        "    print(\"\\nTest complete. If the top prediction matches the 'correct label', the model is working!\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()"
      ],
      "metadata": {
        "id": "ON8VRGIasF8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "8bf99dfd31964209bd461d3a0fe3d245",
            "6a14fe029b7e40fabbb1892b7f91de54",
            "5921fc3ec26a450e8a96c6b8b2ac8749",
            "7fb0e9e1019949769139bef89debe9a2",
            "d9f7ea0c82f54af9a47b5fffb1a3e70e",
            "663027e4b12c4c039e26677aa730156b",
            "ad9d9024277640cfa6aaece6b308f9b5",
            "b0ad3f034ab94c64991bb87ce9d5e825",
            "0212008e01a0481cb607a5480b1142a4",
            "0295a4e376eb4428b4634a838183a8de",
            "fc34d3c671164ec6ac861334aed3d85d",
            "6e3615bf62a0434e991265014e0d82ce",
            "1fc16d067d0c40f7a274317316d6e4ed",
            "e8a5da43d387417a81965228d74069a0",
            "3ea5c54712bb4d0095fdf093651e224f",
            "4bd5924eb7214a848f056c84d73c17e7",
            "2ecb60d88a7a436a8c396a74dfeb3db9",
            "ead85a1249de4e9f9af79771e90a0c07",
            "3610fd042c55463eb4674578788c9261",
            "c539399ca837444ebc339e813e4bc7b9",
            "85f510c6aed5467dadad30ecf658bc11",
            "8bbbcb7361b54e97a5dfb2b593a5c876"
          ]
        },
        "outputId": "6f6cff4e-5ed1-4329-9eed-07ed5e7bfa5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Fine-tuned model loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bf99dfd31964209bd461d3a0fe3d245"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "WARNING:datasets.load:Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e3615bf62a0434e991265014e0d82ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test image loaded. The correct label is: RASH\n",
            "Testing against labels: ['an image of ACNE', 'an image of ECZEMA', 'an image of a RASH', 'a photo of a MOLE', 'a picture of PSORIASIS', 'a photo of healthy, normal skin']\n",
            "\n",
            "--- Test Results ---\n",
            "1. Predicted Label: an image of ECZEMA        | Confidence: 43.38%\n",
            "2. Predicted Label: a picture of PSORIASIS    | Confidence: 40.45%\n",
            "3. Predicted Label: an image of a RASH        | Confidence: 12.62%\n",
            "\n",
            "Test complete. If the top prediction matches the 'correct label', the model is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\ud83e\uddea \"Before\" SigLip Training (Pre-Training)**"
      ],
      "metadata": {
        "id": "s_OabGDUauwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "# Note: We don't import PeftModel, as we are not loading an adapter\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "# Suppress harmless warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "@torch.no_grad() # We don't need to calculate gradients for testing\n",
        "def test_base_model():\n",
        "    # --- 1. Load the ORIGINAL base model ---\n",
        "    base_model_id = \"google/siglip-base-patch16-224\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # We load the original model and processor directly from the Hugging Face Hub\n",
        "    model = AutoModel.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(base_model_id)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    print(\"Original BASE model loaded.\")\n",
        "\n",
        "    # --- 2. Get a test image ---\n",
        "    # This section is identical to your previous test\n",
        "    try:\n",
        "        test_data = next(iter(load_dataset(\"google/scin\", split=\"train\", streaming=True)))\n",
        "\n",
        "        # Find the first valid image in the test item\n",
        "        test_image = None\n",
        "        for col in [\"image_1_path\", \"image_2_path\", \"image_3_path\"]:\n",
        "            if test_data[col] and isinstance(test_data[col], Image.Image):\n",
        "                test_image = test_data[col]\n",
        "                break\n",
        "\n",
        "        if test_image is None:\n",
        "            print(\"Error: Could not load a test image from the dataset.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Test image loaded. The correct label is: {test_data['related_category']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load test image: {e}\")\n",
        "        print(\"Please provide your own image by using: test_image = Image.open('path/to/your/image.jpg')\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Define your text labels ---\n",
        "    # Identical to your previous test\n",
        "    text_labels = [\n",
        "        \"an image of ACNE\",\n",
        "        \"an image of ECZEMA\",\n",
        "        \"an image of a RASH\",\n",
        "        \"a photo of a MOLE\",\n",
        "        \"a picture of PSORIASIS\",\n",
        "        \"a photo of healthy, normal skin\"\n",
        "    ]\n",
        "    print(f\"Testing against labels: {text_labels}\")\n",
        "\n",
        "    # --- 4. Process the image and text ---\n",
        "    # Identical to your previous test\n",
        "    inputs = processor(\n",
        "        text=text_labels,\n",
        "        images=[test_image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64\n",
        "    ).to(device)\n",
        "\n",
        "    # --- 5. Get model predictions ---\n",
        "    # Identical to your previous test\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # --- 6. Show the results ---\n",
        "    # Identical to your previous test\n",
        "    print(\"\\n--- 'BEFORE' Test Results (Original Model) ---\")\n",
        "\n",
        "    top_k_values, top_k_indices = torch.topk(probs, 3)\n",
        "\n",
        "    for i in range(top_k_values.shape[1]):\n",
        "        value = top_k_values[0, i].item() * 100 # as percentage\n",
        "        label_index = top_k_indices[0, i].item()\n",
        "        label_name = text_labels[label_index]\n",
        "        print(f\"{i+1}. Predicted Label: {label_name:<25} | Confidence: {value:2.2f}%\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_base_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "72e8f2f50ba74fc28c6f8dc183d0c436",
            "186e3809da6940b0914e75c459f4bb78",
            "20dc817494934c75b1562e315857f55d",
            "56ed7ab01c6d4bbbad587973e997d0a9",
            "eb2a645959204895955fd94e26bfcabf",
            "0fd6671e76334a57bcd0698236124e36",
            "3fcc5ceaeaac42738cce4b79a60f0165",
            "a391d7bdbe7e4dcda22c37f987453dff",
            "b853ca4079424a90bd056e8dada6d4f2",
            "c777320773734b40982528c2efefea2b",
            "fedd315467354062bac6b9b6d149097a",
            "c90f2bccb7384ab1b61bfdcffd5e0c83",
            "a68d92486da04f9ebcde37e9fb090105",
            "46b2416e410f42dbafd1c4fb20f30a0c",
            "ef9ca2d042414b80aae0837b9300dc7c",
            "6375afbbfe2d4ce3a4649d071a89021c",
            "6b086a6d48cc47c19aeada6a7da119af",
            "ce98f3cbe4f34c3eb921bc2a7ea9d852",
            "7530ed125a4b4aa6a1cbe50c1c2ff1cb",
            "59e41a0f9d6849aaa29c82da0b1f9110",
            "ca880595641a48fd8b3bc707ede814c4",
            "f4b1cf6282c84a01ae79c0d84c7b0bf6"
          ]
        },
        "id": "kUrVcQ-LZU9q",
        "outputId": "a604b207-11e7-4425-fe24-ab78e7c6aed9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Original BASE model loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72e8f2f50ba74fc28c6f8dc183d0c436"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "WARNING:datasets.load:Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c90f2bccb7384ab1b61bfdcffd5e0c83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test image loaded. The correct label is: RASH\n",
            "Testing against labels: ['an image of ACNE', 'an image of ECZEMA', 'an image of a RASH', 'a photo of a MOLE', 'a picture of PSORIASIS', 'a photo of healthy, normal skin']\n",
            "\n",
            "--- 'BEFORE' Test Results (Original Model) ---\n",
            "1. Predicted Label: an image of a RASH        | Confidence: 88.77%\n",
            "2. Predicted Label: an image of ECZEMA        | Confidence: 8.00%\n",
            "3. Predicted Label: a picture of PSORIASIS    | Confidence: 2.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\ud83e\ude7a **\"MedSigLIP\" Test Script (For Comparison)**"
      ],
      "metadata": {
        "id": "-gzxOgtIavwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "# Note: We don't import PeftModel\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "# Suppress harmless warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "@torch.no_grad() # We don't need to calculate gradients for testing\n",
        "def test_medsiglip_model():\n",
        "\n",
        "    # --- 1. Load the MedSigLIP model ---\n",
        "\n",
        "    # [THE ONLY CHANGE IS HERE]\n",
        "    # We're now loading the pre-trained medical specialist model\n",
        "    base_model_id = \"google/medsiglip-448\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Loading base model: {base_model_id}\")\n",
        "\n",
        "    # We load the original model and processor directly from the Hugging Face Hub\n",
        "    model = AutoModel.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(base_model_id)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    print(\"Original MedSigLIP model loaded.\")\n",
        "\n",
        "    # --- 2. Get a test image ---\n",
        "    # This section is identical\n",
        "    try:\n",
        "        test_data = next(iter(load_dataset(\"google/scin\", split=\"train\", streaming=True)))\n",
        "\n",
        "        test_image = None\n",
        "        for col in [\"image_1_path\", \"image_2_path\", \"image_3_path\"]:\n",
        "            if test_data[col] and isinstance(test_data[col], Image.Image):\n",
        "                test_image = test_data[col]\n",
        "                break\n",
        "\n",
        "        if test_image is None:\n",
        "            print(\"Error: Could not load a test image from the dataset.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Test image loaded. The correct label is: {test_data['related_category']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load test image: {e}\")\n",
        "        print(\"Please provide your own image by using: test_image = Image.open('path/to/your/image.jpg')\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Define your text labels ---\n",
        "    # Identical\n",
        "    text_labels = [\n",
        "        \"an image of ACNE\",\n",
        "        \"an image of ECZEMA\",\n",
        "        \"an image of a RASH\",\n",
        "        \"a photo of a MOLE\",\n",
        "        \"a picture of PSORISASIS\", # Corrected spelling for accuracy\n",
        "        \"a photo of healthy, normal skin\"\n",
        "    ]\n",
        "    print(f\"Testing against labels: {text_labels}\")\n",
        "\n",
        "    # --- 4. Process the image and text ---\n",
        "    # Identical, max_length=64 is still correct\n",
        "    inputs = processor(\n",
        "        text=text_labels,\n",
        "        images=[test_image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64\n",
        "    ).to(device)\n",
        "\n",
        "    # --- 5. Get model predictions ---\n",
        "    # Identical\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # --- 6. Show the results ---\n",
        "    print(\"\\n--- 'MedSigLIP' Test Results (Original Model) ---\")\n",
        "\n",
        "    top_k_values, top_k_indices = torch.topk(probs, 3)\n",
        "\n",
        "    for i in range(top_k_values.shape[1]):\n",
        "        value = top_k_values[0, i].item() * 100 # as percentage\n",
        "        label_index = top_k_indices[0, i].item()\n",
        "        label_name = text_labels[label_index]\n",
        "        print(f\"{i+1}. Predicted Label: {label_name:<25} | Confidence: {value:2.2f}%\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_medsiglip_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "3fb25727fce74cd4816a131b56a8f50a",
            "8bae541a6f2f47c196d908341498e8ad",
            "e07f381ee3354e429d4962b86c814192",
            "b79b16fb60174c7783a7193a57df2a99",
            "bfc4a7fcd25640e0a3dc945827c4c903",
            "f6a3cd7f4d2847d3997d6e55767b4337",
            "a061187eb4a948b1b9cf6be311bc65ef",
            "d3e5545b8ba54a7f976d6f6e4479b2ed",
            "5852832fd43b46e7897b88f4fb5a2468",
            "3e2c752b7dcb410aa182990a67949919",
            "c12da41de52b4c8389067b0b3ae44da3",
            "4f0595e9aa084802b48787837e14796f",
            "f241a048f7a242c5804571eb622ba69b",
            "353a1c6a83ca4ae69ebe9933d6aa87d0",
            "d7e8e8b9baa049f095c9fbcd2ca53aec",
            "26baa35787e542fdb7fb5cac963d3bd7",
            "ac1cef1dc9d04de4a48fd794ad096f73",
            "a08223150bbe44328bba36cfbffbd3b4",
            "2182e2f3747e421fb4b9f7aa6bd0f251",
            "1140a0152536427f8f120575c1c4bd6a",
            "8dae06b1be56477cb2605b3a3ae05399",
            "e2d3d243531b490d9e8e01709abb8dfe",
            "f05a8c8c9f1a4140a5cb8cfaf46fa969",
            "30a8f35a62c040dda1d101b075a76204",
            "59632137e26848de83276c0e35e162f7",
            "2c6fc1dada7d48bb90b9a03dafb0cfd3",
            "2241e73ad0cd4bbcb67195728fb0d95e",
            "6b08c5e4f2d6450bab0161d3edd3d594",
            "a92cc784a875450494ade8ba3151e888",
            "3373250cbc4b4689bf6cca2349087bfd",
            "a87bcfea46204cabbe10e74a24ccab3c",
            "5092f9e3fffb40838fdc65a5549549b0",
            "13495e0a78924519b6905d2897c4523c",
            "f93e3a5c4d03477eaa87f0927558f5ac",
            "7a3044ced2ee41a8a5a519cc44a4db6c",
            "71ebbd694d7c4e80a42ae7670a683db8",
            "bfd57e1090684d2e84ca7cbe2c53c3a2",
            "bcd0235efc0b40f782b8e8a186269bee",
            "917c2b0d4ed34d47ad2d20536510b7c7",
            "ee51a0a2e4f44163a02e0811383119a0",
            "d19ce42f0c474aefa04e12fd6e9da798",
            "a7afed9d13b842f2ac88596bf658c5a6",
            "d6cb5517b286474ab315f768198a5181",
            "f5b04e1d8c5c481aab04968e591c4855",
            "aca130f2e3274c298358a334fbbcc420",
            "fcd3590a5bad4b13adbc4c54b84ef9f7",
            "17e8508a2fc440288382b1de0c587475",
            "6fb37d6bd0d04dadbc2758739cfef1c3",
            "118da4bc3db241c299bf7b57da2385df",
            "3894b227412944b9b922731090735c2b",
            "e447ab200d5f49089c1e22911fde67de",
            "8e5fb207e8014882aae54064db4e637f",
            "b4e24436b14b4d6ab435e284e758599b",
            "3944fbb7ddc646c99110cbe718f33dc0",
            "7293b78dd8ef4b9cab17a20dc8c847ed",
            "e528f9e2f07648b7b4b1b00a099427bc",
            "b9046cebb6ab4c9db390ecdb5989f149",
            "44ab3a8180774d0f89943b2ef713b7ef",
            "03f20e2437e44effac41d8fccdd63655",
            "b3fcdb12564746be9352d42cf5c3a208",
            "7cb18b9c74444014904f91c834686cc9",
            "60effb446d1b445c934de0f220d90607",
            "5e61f387f61f4f29857de8c74ffb4031",
            "1b8a0e9d3ba24752bf4ca6b67ca9a6a0",
            "3140d3a07a8d42b7927d2be0829d9f12",
            "3b79f54a7d7145c5bf065d2d6709fb49",
            "fcf0030cd5214e399a6db94e7c9aa6ee",
            "949a02329c6945f9b9da8e866cef8123",
            "70f7144881ea47a7b865582285263eae",
            "ef0ce20c98e84dbe96b7ff84c9ab8b6c",
            "5a3604547fba4f5e99bb649840198245",
            "5ff2c7b475b4462684308514ed02faf6",
            "e050ab4a598b43568ef825b6b0fa6ac0",
            "f164b9e9fe4e4adfb20e8cb97e3c5cf7",
            "c99bdfe55f67443088b488c926536518",
            "8a9f9227536440f196c21b8214b955f7",
            "8393887d86a24fad8f0abc71dc3a4c51",
            "e9fe9a89fcdb406b990c20725ac01289",
            "5f8a9e00e9994ee194ba1faa904901d8",
            "09ea4af802b344db8adb7ef8d5791c09",
            "6af037a9d25d46b2aeff7e85107716fa",
            "91cd8846f4fb434a935a775840dd436a",
            "843720e15a2546258342f8e8cadb3d74",
            "216a68fc3bf241e29f3f444d1a52193f",
            "e1939ef42d2949cdba3af7fa10681efd",
            "cc11b60674164a208ccc312a30272d95",
            "bb54b72133a34d0498a1ee3454ad9268",
            "1e34913468e247c1bcd901b49d4dfa60",
            "9225670fd7a14c0f9d9f43b6e18e2628",
            "bab096571d3d4542a3a09e2ee97fed2f",
            "aad7a67ff6e04e09875c028bf2b1aa00",
            "526a7c150f214a10910fd89590987435",
            "97d89557612c4cb0ac6f2e058df60085",
            "9709851226ab4d8a8c35d9f4576c0d4b",
            "6b78333c84d0423a8000eb69c244f1fb",
            "f10b7f811f674834ae46df14b72d63f6",
            "e96322da42d94e42acd98c9a41320090",
            "46081b3ebf174e268261dfa8c4ab7959",
            "72607d7ef7504197a0b21b2795b185e2"
          ]
        },
        "id": "3aIiOhn7azDe",
        "outputId": "2377500f-8d5d-43b0-e67a-5747e3066d21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading base model: google/medsiglip-448\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fb25727fce74cd4816a131b56a8f50a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0595e9aa084802b48787837e14796f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/360 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f05a8c8c9f1a4140a5cb8cfaf46fa969"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f93e3a5c4d03477eaa87f0927558f5ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aca130f2e3274c298358a334fbbcc420"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e528f9e2f07648b7b4b1b00a099427bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcf0030cd5214e399a6db94e7c9aa6ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original MedSigLIP model loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9fe9a89fcdb406b990c20725ac01289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "WARNING:datasets.load:Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9225670fd7a14c0f9d9f43b6e18e2628"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test image loaded. The correct label is: RASH\n",
            "Testing against labels: ['an image of ACNE', 'an image of ECZEMA', 'an image of a RASH', 'a photo of a MOLE', 'a picture of PSORISASIS', 'a photo of healthy, normal skin']\n",
            "\n",
            "--- 'MedSigLIP' Test Results (Original Model) ---\n",
            "1. Predicted Label: an image of a RASH        | Confidence: 32.91%\n",
            "2. Predicted Label: an image of ECZEMA        | Confidence: 28.37%\n",
            "3. Predicted Label: an image of ACNE          | Confidence: 15.06%\n"
          ]
        }
      ]
    }
  ]
}