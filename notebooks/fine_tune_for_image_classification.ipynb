{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinooj/health-kiosk/blob/main/notebooks/fine_tune_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZF3SXooHNwZ"
      },
      "source": [
        "## Fintuning SigLIP with SCIN dataset\n",
        "\n",
        "### Get access to SigLIP\n",
        "\n",
        "Before you get started, make sure that you have access to MedSigLIP models on\n",
        "Hugging Face:\n",
        "\n",
        "1.  If you don't already have a Hugging Face account, you can create one for\n",
        "    free by clicking [here](https://huggingface.co/join).\n",
        "2.  Head over to the\n",
        "    [SigLIP model page](https://huggingface.co/google/siglip-base-patch16-224) and\n",
        "    accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1-QR0nHbXK"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ool8XypHcx4"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet accelerate datasets evaluate tensorboard transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxRAlUiyJE6b"
      },
      "source": [
        "## Prepare fine-tuning dataset\n",
        "\n",
        "This notebook uses the\n",
        "[SCIN dataset](https://github.com/google-research-datasets/scin) to fine-tune MedSigLIP to classify the ten most common dermatology conditions:\n",
        "\n",
        "```\n",
        "[\"Eczema\", \"Allergic Contact Dermatitis\", \"Insect Bite\", \"Urticaria\", \"Psoriasis\", \"Folliculitis\", \"Irritant Contact Dermatitis\", \"Tinea\", \"Herpes Zoster\", \"Drug Rash\"]\n",
        "```\n",
        "\n",
        "**Note:** The SCIN dataset was included in MedSigLIP's training data. It is used as a fine-tuning dataset in this notebook for demonstration purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-eQ6WOqmGjX"
      },
      "source": [
        "Download the dataset from\n",
        "[Cloud Storage](https://console.cloud.google.com/storage/browser/dx-scin-public-data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tapKfG67nqkb"
      },
      "outputs": [],
      "source": [
        "# Skip authentication since this dataset is public\n",
        "! gcloud config set auth/disable_credentials True && gcloud config set user_output_enabled False\n",
        "\n",
        "! mkdir dataset\n",
        "! gcloud storage cp -R gs://dx-scin-public-data/dataset/* dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgTwR2Lpoo63"
      },
      "source": [
        "Load the dataset as a `pandas.DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSSIAI_rLNiG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "SCIN_GCS_CASES_CSV = \"dataset/scin_cases.csv\"\n",
        "SCIN_GCS_LABELS_CSV = \"dataset/scin_labels.csv\"\n",
        "\n",
        "\n",
        "def initialize_df_with_metadata(csv_path: str):\n",
        "    df = pd.read_csv(csv_path, dtype={\"case_id\": str})\n",
        "    df[\"case_id\"] = df[\"case_id\"].astype(str)\n",
        "    return df\n",
        "\n",
        "\n",
        "def augment_metadata_with_labels(df: pd.DataFrame, csv_path: str):\n",
        "    labels_df = pd.read_csv(csv_path, dtype={\"case_id\": str})\n",
        "    print(f\"Loaded labels with {len(labels_df)} rows.\")\n",
        "    labels_df[\"case_id\"] = labels_df[\"case_id\"].astype(str)\n",
        "    merged_df = pd.merge(df, labels_df, on=\"case_id\")\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "scin_no_label_df = initialize_df_with_metadata(SCIN_GCS_CASES_CSV)\n",
        "scin_df = augment_metadata_with_labels(scin_no_label_df, SCIN_GCS_LABELS_CSV)\n",
        "scin_df.set_index(\"case_id\", inplace=True)\n",
        "\n",
        "scin_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVDuSovpeUz"
      },
      "source": [
        "Process the DataFrame so that each row corresponds to a training example with an `image` and `label`.\n",
        "\n",
        "Filter examples with insufficient image quality and\n",
        "low-confidence labels. Additionally, in the original dataset each data donator contributes up to three images. Make each individual image and its corresponding label into separate examples in the resulting training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJA08zNDLQtR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "CONDITIONS = [\"Eczema\", \"Allergic Contact Dermatitis\", \"Insect Bite\", \"Urticaria\", \"Psoriasis\", \"Folliculitis\", \"Irritant Contact Dermatitis\", \"Tinea\", \"Herpes Zoster\", \"Drug Rash\"]\n",
        "MINIMUM_CONFIDENCE = 0\n",
        "\n",
        "\n",
        "def remove_low_confidence_labels(row: pd.Series):\n",
        "    labels = eval(row.dermatologist_skin_condition_on_label_name)\n",
        "    confidences = eval(row.dermatologist_skin_condition_confidence)\n",
        "\n",
        "    row_labels = []\n",
        "    for label, confidence in zip(labels, confidences):\n",
        "        if label in CONDITIONS and confidence >= MINIMUM_CONFIDENCE:\n",
        "            row_labels.append(label)\n",
        "    return row_labels\n",
        "\n",
        "\n",
        "# Filter examples with insufficient image quality\n",
        "scin_df = scin_df[scin_df.dermatologist_gradable_for_skin_condition_1 == \"DEFAULT_YES_IMAGE_QUALITY_SUFFICIENT\"]\n",
        "\n",
        "# Remove labels that are below a minimum confidence\n",
        "scin_df[\"label\"] = scin_df.apply(remove_low_confidence_labels, axis=1)\n",
        "\n",
        "# Make each image (if it exists) into a separate example.\n",
        "# Also create a new dataframe with only images and labels.\n",
        "image_cols = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
        "scin_df = pd.melt(\n",
        "    scin_df, id_vars=[\"label\"], value_vars=image_cols, value_name=\"image\"\n",
        ").drop(columns=[\"variable\"]).dropna(subset=[\"image\"]).set_index(\"image\")\n",
        "\n",
        "# Convert labels from e.g. [[\"Eczema\"], [\"Urticaria\", \"Insect Bite\"]] to\n",
        "# [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]]\n",
        "mlb = MultiLabelBinarizer(classes=CONDITIONS)\n",
        "scin_df[\"label\"] = mlb.fit_transform(scin_df[\"label\"]).tolist()\n",
        "\n",
        "# Drop missing image (https://github.com/google-research-datasets/scin/issues/1)\n",
        "scin_df.drop(index=[\"dataset/images/-2243186711511406658.png\"], inplace=True)\n",
        "\n",
        "scin_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYPjh8RUynAM"
      },
      "source": [
        "Load the DataFrame as a Hugging Face `Dataset` with `from_pandas()`. Then, create train, validation, and test splits.\n",
        "\n",
        "Initial scin_df after merging labels had 5033 rows (cases), but the number of samples in the dataset for training became 6451. This is because:\n",
        "\n",
        "1. **Multiple Images per Case:** The original scin_df had image_1_path, image_2_path, and image_3_path columns. The code used pd.melt to transform this, creating a separate row for each image associated with a case. So, if a single case had 3 images, it became 3 separate image examples for the model.\n",
        "2. **Filtering:** Some examples were filtered out due to insufficient image quality or missing images, which brought the total number of valid image examples down to 6451.\n",
        "\n",
        "From these** 6451 total image samples**, the split was as follows:\n",
        "\n",
        "- Initial Split (test_size=0.2):\n",
        "  - Training Set: 80% of 6451 ≈ 5160 samples.\n",
        "  - Temporary Test/Validation Pool: 20% of 6451 ≈ 1291 samples.\n",
        "- Second Split (test_size=0.5) of the Temporary Pool:\n",
        "  - Validation Set: 50% of 1291 = 645 samples.\n",
        "  - Test Set: 50% of 1291 = 646 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d10f934"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Re-initializing for accurate counts\n",
        "SCIN_GCS_CASES_CSV = \"dataset/scin_cases.csv\"\n",
        "SCIN_GCS_LABELS_CSV = \"dataset/scin_labels.csv\"\n",
        "\n",
        "def initialize_df_with_metadata(csv_path: str):\n",
        "    df = pd.read_csv(csv_path, dtype={\"case_id\": str})\n",
        "    df[\"case_id\"] = df[\"case_id\"].astype(str)\n",
        "    return df\n",
        "\n",
        "def augment_metadata_with_labels(df: pd.DataFrame, csv_path: str):\n",
        "    labels_df = pd.read_csv(csv_path, dtype={\"case_id\": str})\n",
        "    labels_df[\"case_id\"] = labels_df[\"case_id\"].astype(str)\n",
        "    merged_df = pd.merge(df, labels_df, on=\"case_id\")\n",
        "    return merged_df\n",
        "\n",
        "# Step 1: Initial load of cases and labels\n",
        "scin_no_label_df_temp = initialize_df_with_metadata(SCIN_GCS_CASES_CSV)\n",
        "scin_df_temp = augment_metadata_with_labels(scin_no_label_df_temp, SCIN_GCS_LABELS_CSV)\n",
        "scin_df_temp.set_index(\"case_id\", inplace=True)\n",
        "print(f\"Initial number of cases (rows) after merging labels: {len(scin_df_temp)}\")\n",
        "\n",
        "# Step 2: Filter examples with insufficient image quality\n",
        "original_cases_count = len(scin_df_temp)\n",
        "scin_df_temp = scin_df_temp[scin_df_temp.dermatologist_gradable_for_skin_condition_1 == \"DEFAULT_YES_IMAGE_QUALITY_SUFFICIENT\"]\n",
        "filtered_cases_count = len(scin_df_temp)\n",
        "print(f\"Number of cases (rows) after filtering for image quality: {filtered_cases_count}\")\n",
        "print(f\"Number of cases filtered out by image quality: {original_cases_count - filtered_cases_count}\")\n",
        "\n",
        "# Step 3: Remove low confidence labels and melt to individual image examples\n",
        "CONDITIONS = [\"Eczema\", \"Allergic Contact Dermatitis\", \"Insect Bite\", \"Urticaria\", \"Psoriasis\", \"Folliculitis\", \"Irritant Contact Dermatitis\", \"Tinea\", \"Herpes Zoster\", \"Drug Rash\"]\n",
        "MINIMUM_CONFIDENCE = 0\n",
        "\n",
        "def remove_low_confidence_labels(row: pd.Series):\n",
        "    labels = eval(row.dermatologist_skin_condition_on_label_name)\n",
        "    confidences = eval(row.dermatologist_skin_condition_confidence)\n",
        "\n",
        "    row_labels = []\n",
        "    for label, confidence in zip(labels, confidences):\n",
        "        if label in CONDITIONS and confidence >= MINIMUM_CONFIDENCE:\n",
        "            row_labels.append(label)\n",
        "    return row_labels\n",
        "\n",
        "scin_df_temp[\"label\"] = scin_df_temp.apply(remove_low_confidence_labels, axis=1)\n",
        "\n",
        "image_cols = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
        "initial_image_rows_after_melt = len(pd.melt(scin_df_temp, id_vars=[\"label\"], value_vars=image_cols, value_name=\"image\").drop(columns=[\"variable\"]))\n",
        "\n",
        "scin_df_temp = pd.melt(\n",
        "    scin_df_temp, id_vars=[\"label\"], value_vars=image_cols, value_name=\"image\"\n",
        ").drop(columns=[\"variable\"])\n",
        "\n",
        "# Count rows after melt, before dropna\n",
        "print(f\"Number of image rows after melting (before dropping NaN images): {len(scin_df_temp)}\")\n",
        "\n",
        "scin_df_temp_before_dropna = len(scin_df_temp)\n",
        "scin_df_temp = scin_df_temp.dropna(subset=[\"image\"])\n",
        "print(f\"Number of image rows after dropping NaN images: {len(scin_df_temp)}\")\n",
        "print(f\"Number of image rows filtered out by dropping NaN images: {scin_df_temp_before_dropna - len(scin_df_temp)}\")\n",
        "\n",
        "# Step 4: Drop specific missing image\n",
        "scin_df_temp_before_specific_drop = len(scin_df_temp)\n",
        "# Check if the specific image path exists in the index before attempting to drop\n",
        "if \"dataset/images/-2243186711511406658.png\" in scin_df_temp.set_index(\"image\").index:\n",
        "    scin_df_temp = scin_df_temp.set_index(\"image\").drop(index=[\"dataset/images/-2243186711511406658.png\"], errors='ignore')\n",
        "    # Reset index if it was set for the drop operation\n",
        "    if isinstance(scin_df_temp, pd.DataFrame):\n",
        "        scin_df_temp = scin_df_temp.reset_index()\n",
        "else:\n",
        "    print(\"Specific missing image 'dataset/images/-2243186711511406658.png' not found in DataFrame for explicit drop.\")\n",
        "\n",
        "print(f\"Final number of image rows before `from datasets import Dataset`: {len(scin_df_temp)}\")\n",
        "print(f\"Number of image rows filtered out by specific image drop: {scin_df_temp_before_specific_drop - len(scin_df_temp)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mse9FtlLTk8"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, Image\n",
        "\n",
        "data = Dataset.from_pandas(scin_df)\n",
        "# Decode image paths as PIL images\n",
        "data = data.cast_column(\"image\", Image())\n",
        "data = data.train_test_split(\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "validation_test_data = data.pop(\"test\").train_test_split(\n",
        "    test_size=0.5,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "data[\"validation\"] = validation_test_data[\"train\"]\n",
        "data[\"test\"] = validation_test_data[\"test\"]\n",
        "\n",
        "# Display dataset details\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiIHKYBrOcpy"
      },
      "source": [
        "Inspect a sample data point, which contains:\n",
        "\n",
        "* `image`: dermatology image as a `PIL` image object\n",
        "* `label`: corresponding multiple labels as a one-hot encoded vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwogC69qLWKT"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][1][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N7_FAQMLZip"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][1][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG72XmYZPvHJ"
      },
      "source": [
        "Preprocess the input images.\n",
        "\n",
        "The model expects the input images to be resized to 448x448 with pixel values rescaled to the range [-1, 1].\n",
        "\n",
        "Note that the input images are also zero-padded to square before resizing to preserve aspect ratio. This step is included for consistency with the original MedSigLIP training data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKluNn4dA_4-"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor, Normalize, InterpolationMode\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "\n",
        "size = image_processor.size[\"height\"]  # 448\n",
        "mean = image_processor.image_mean  # 0.5\n",
        "std = image_processor.image_std  # 0.5\n",
        "\n",
        "_transform = Compose([\n",
        "    Resize((size, size), interpolation=InterpolationMode.BILINEAR),\n",
        "    # Convert PIL image to PyTorch tensor and rescale pixel values from the\n",
        "    # range [0, 255] to [0, 1]\n",
        "    ToTensor(),\n",
        "    # Scale pixel values to the range [-1, 1]\n",
        "    Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess(examples):\n",
        "    examples[\"pixel_values\"] = [\n",
        "        # CenterCrop effectively zero pads the image to a square with size equal\n",
        "        # to the larger dimension\n",
        "        _transform(CenterCrop(max(image.size))(image.convert(\"RGB\")))\n",
        "        for image in examples[\"image\"]\n",
        "    ]\n",
        "    return examples\n",
        "\n",
        "\n",
        "train_data = data[\"train\"].map(preprocess, batched=True, remove_columns=[\"image\"])\n",
        "validation_data = data[\"validation\"].map(preprocess, batched=True, remove_columns=[\"image\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1hLQNJOJjl"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "This notebook demonstrates fine-tuning the MedSigLIP vision encoder for a multi-label image classification task on image and structured label data using the `Trainer` from the Hugging Face `Transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWcKpZr0iXA"
      },
      "source": [
        "Load the MedSigLIP vision encoder with an image classification head on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NOAPt2RIN9Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model_id = \"google/siglip-base-patch16-224\"\n",
        "\n",
        "# Define the label mappings for the classification task\n",
        "id2label = {i: label for i, label in enumerate(CONDITIONS)}\n",
        "label2id = {label: i for i, label in enumerate(CONDITIONS)}\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_id,\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    num_labels=len(CONDITIONS),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vimnYAdBiUn"
      },
      "source": [
        "Define a data collator to prepare batches of training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsK16bROOM_D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.tensor([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples], dtype=torch.float)\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jah4UAuM1hQn"
      },
      "source": [
        "Define evaluation metrics to be computed during training. The function takes in an [`EvalPrediction`](https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction) which contains the model predictions (logits) and labels.\n",
        "\n",
        "Similar to MedSigLIP's reported metrics, this example uses the macro-averaged One-vs-rest ROC AUC (Area Under the Receiver Operating Characteristic Curve) score to evaluate multi-class classification performance. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwSmhofc1gnK"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "roc_auc_score = evaluate.load(\"roc_auc\", \"multilabel\")\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def metrics_fn(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    scores = sigmoid(logits)\n",
        "    return roc_auc_score.compute(\n",
        "        prediction_scores=scores,\n",
        "        references=labels,\n",
        "        average=\"macro\",\n",
        "        multi_class=\"ovr\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXAtL-4QRxwe"
      },
      "source": [
        "Define a weighted loss function to address class imbalance within the dataset.\n",
        "\n",
        "This is the [default loss](https://github.com/huggingface/transformers/blob/cd74917ffc3e8f84e4a886052c5ab32b7ac623cc/src/transformers/models/siglip/modeling_siglip.py#L1205) used for multi-label classification but utilizes `pos_weight` to assign a per-class weight to positive examples, effectively treating minority positive classes with greater importance in the loss calculation. Refer to the [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) for more details.\n",
        "\n",
        "**Note:** There are other balancing methods such as oversampling that may be used depending on your dataset and classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhW7rtRrRxW7"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "labels = torch.tensor(train_data[\"label\"])\n",
        "num_samples = labels.shape[0]\n",
        "num_positive = labels.sum(axis=0)\n",
        "num_negative = num_samples - num_positive\n",
        "POS_WEIGHT = num_negative / num_positive\n",
        "\n",
        "\n",
        "def loss_fn(outputs, labels, num_items_in_batch):\n",
        "    logits = outputs.get(\"logits\")\n",
        "    pos_weight = POS_WEIGHT.to(logits.device)\n",
        "    loss_fct = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    return loss_fct(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcFR2ObAj3qy"
      },
      "source": [
        "Configure training parameters in\n",
        "[`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WJBo5w0xgqu"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "num_train_epochs = 3  # @param {type: \"number\"}\n",
        "learning_rate = 5e-5  # @param {type: \"number\"}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"siglip-224-scin-classification\",  # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,               # Number of training epochs\n",
        "    per_device_train_batch_size=8,                   # Batch size per device during training\n",
        "    per_device_eval_batch_size=8,                    # Batch size per device during evaluation\n",
        "    gradient_accumulation_steps=8,                   # Number of steps before performing a backward/update pass\n",
        "    logging_steps=40,                                # Number of steps between logs\n",
        "    save_strategy=\"epoch\",                           # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",                           # Evaluate every `eval_steps`\n",
        "    eval_steps=40,                                   # Number of steps between evaluations\n",
        "    learning_rate=learning_rate,                     # Learning rate\n",
        "    weight_decay=0.01,                               # Weight decay to apply\n",
        "    warmup_steps=5,                                  # Number of steps for linear warmup from 0 to learning rate\n",
        "    lr_scheduler_type=\"cosine\",                      # Use cosine learning rate scheduler\n",
        "    push_to_hub=False,                                # Push model to Hub\n",
        "    report_to=\"tensorboard\",                         # Report metrics to tensorboard\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiaBMdnskE64"
      },
      "source": [
        "Construct a [`Trainer`](https://huggingface.co/docs/transformers/trainer) using the previously defined training parameters, data collator, metrics function, and weighted loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOFyE_KXxm9q"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=validation_data,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=metrics_fn,\n",
        "    compute_loss_func=loss_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h9ZNG6KkMZv"
      },
      "source": [
        "Launch the fine-tuning process.\n",
        "\n",
        "**Note:** This may take around 3 hours to run using the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFlo50QUgFhF"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYqBi93gi4Fx"
      },
      "source": [
        "Save the final model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K03yYO6Mi6Ff"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the link below to navigate to your model repository and click on the \"Training metrics\" tab to view training curves."
      ],
      "metadata": {
        "id": "LbsuGyUrgAm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "username = api.whoami()[\"name\"]\n",
        "print(f\"https://huggingface.co/{username}/{training_args.output_dir}\")"
      ],
      "metadata": {
        "id": "Ocgzvj1PfV8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN-Eav-lkUKw"
      },
      "source": [
        "## Evaluate the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAi_f6mlNT1b"
      },
      "source": [
        "### Set up for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSUsXjcRsAM"
      },
      "source": [
        "Load the ROC AUC (Area Under the Receiver Operating Characteristic Curve) and additional accuracy metrics to evaluate the model's performance on the classification task.\n",
        "\n",
        "You can use other accuracy metrics based on your use case and performance requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELAViyT1OZev"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "roc_auc_score = evaluate.load(\"roc_auc\", \"multilabel\")\n",
        "\n",
        "# Ground-truth labels\n",
        "REFERENCES = data[\"test\"][\"label\"]\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    prediction_scores: np.ndarray,\n",
        "    threshold: float,\n",
        ") -> dict[str, float]:\n",
        "    metrics = {}\n",
        "    metrics.update(roc_auc_score.compute(\n",
        "        prediction_scores=prediction_scores,\n",
        "        references=REFERENCES,\n",
        "        average=\"macro\",\n",
        "        multi_class=\"ovr\",\n",
        "    ))\n",
        "    predictions = (prediction_scores > threshold).astype(int)\n",
        "    mcm = multilabel_confusion_matrix(\n",
        "        y_true=REFERENCES,\n",
        "        y_pred=predictions,\n",
        "    )\n",
        "    tn = mcm[:, 0, 0]\n",
        "    tp = mcm[:, 1, 1]\n",
        "    fn = mcm[:, 1, 0]\n",
        "    fp = mcm[:, 0, 1]\n",
        "    metrics.update({\n",
        "        \"sensitivity\": tp / (tp + fn),\n",
        "        \"specificity\": tn / (tn + fp),\n",
        "    })\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def print_metrics(metrics: dict[str, Union[float, np.ndarray]]) -> None:\n",
        "    print(f\"Macro-averaged one-vs-rest ROC AUC: {metrics['roc_auc']:.2f}\")\n",
        "    for metric in [\"sensitivity\", \"specificity\"]:\n",
        "        print(f\"\\n{metric.capitalize()}:\")\n",
        "        for i, condition in enumerate(CONDITIONS):\n",
        "            print(f\"{condition}: {metrics[metric][i]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFZ_hrNmon3l"
      },
      "source": [
        "### Compute metrics on the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUFQrheRotcR"
      },
      "source": [
        "Load the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7gvIDnTopvj"
      },
      "outputs": [],
      "source": [
        "ft_model = AutoModelForImageClassification.from_pretrained(\n",
        "    training_args.output_dir,\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    num_labels=len(CONDITIONS),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7rXeJ5dpGF2"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Followig Code will pick a random image from the training set, run it through the fine-tuned model, and then display the model's predicted probabilities, the conditions it identified (based on a 0.5 threshold), and the original ground-truth labels for that image.**\n",
        "\n",
        "--- Verification with Top 5 Predicted Conditions ---"
      ],
      "metadata": {
        "id": "zwfLaVSq5s2u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a587442d"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Choose a random index from the training data\n",
        "random_index = random.randint(0, len(train_data) - 1)\n",
        "\n",
        "# Get the original image path and label from the raw data before preprocessing\n",
        "original_data_point = data[\"train\"][random_index]\n",
        "original_image_path = original_data_point[\"image\"]\n",
        "original_label = original_data_point[\"label\"]\n",
        "\n",
        "# Preprocess the image for the model\n",
        "image = original_image_path.convert(\"RGB\") # Corrected line\n",
        "inputs = image_processor(images=[image], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = ft_model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "probabilities = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "\n",
        "# Define a threshold for identifying conditions (same as used for evaluation)\n",
        "threshold = 0.5\n",
        "identified_conditions_binary = (probabilities > threshold).astype(int)\n",
        "\n",
        "print(f\"--- Analysis for Random Image (Index: {random_index}) ---\")\n",
        "print(f\"Image Path: {original_image_path}\") # Changed print statement for clarity\n",
        "print(\"\\nOriginal Labels (Ground Truth):\")\n",
        "original_condition_names = []\n",
        "for i, val in enumerate(original_label):\n",
        "    if val == 1:\n",
        "        original_condition_names.append(CONDITIONS[i])\n",
        "        print(f\"- {CONDITIONS[i]}\")\n",
        "\n",
        "print(\"\\nModel Predicted Probabilities and Identified Conditions (Threshold=0.5) (Sorted by Probability):\")\n",
        "\n",
        "# Combine probabilities, condition names, and identified status\n",
        "prediction_details = []\n",
        "for i, prob in enumerate(probabilities):\n",
        "    condition_name = CONDITIONS[i]\n",
        "    is_identified = bool(identified_conditions_binary[i])\n",
        "    prediction_details.append((prob, condition_name, is_identified))\n",
        "\n",
        "# Sort by probability in descending order\n",
        "prediction_details.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "identified_conditions_list = []\n",
        "for prob, condition_name, is_identified in prediction_details:\n",
        "    print(f\"- {condition_name}: {prob:.4f} (Identified: {is_identified})\")\n",
        "    if is_identified:\n",
        "        identified_conditions_list.append(condition_name)\n",
        "\n",
        "if not identified_conditions_list:\n",
        "    print(\"  No conditions identified by the model for this image.\")\n",
        "else:\n",
        "    print(f\"\\nModel identified the following conditions: {', '.join(identified_conditions_list)}\")\n",
        "\n",
        "# --- Additional Verification: Check for intersection with top 5 predicted ---\n",
        "print(\"\\n--- Verification with Top 5 Predicted Conditions ---\")\n",
        "\n",
        "top_5_predicted_conditions = [detail[1] for detail in prediction_details[:5]]\n",
        "\n",
        "print(f\"Top 5 Model Predicted Conditions: {', '.join(top_5_predicted_conditions)}\")\n",
        "print(f\"Original Labels: {', '.join(original_condition_names)}\")\n",
        "\n",
        "# Find the intersection\n",
        "intersection = set(original_condition_names).intersection(set(top_5_predicted_conditions))\n",
        "\n",
        "if intersection:\n",
        "    print(f\"Match Successful! Common conditions found: {', '.join(intersection)}\")\n",
        "else:\n",
        "    print(\"Match Failed. No common conditions between original labels and top 5 predicted.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's break down the line outputs = ft_model(**inputs) bold text when you have a batch of 64 images, using a simple analogy.\n",
        "\n",
        "Imagine your ft_model is a highly specialized team of ten doctors, one for each of the dermatology conditions you are trying to classify (Eczema, Insect Bite, etc.).\n",
        "\n",
        "inputs: This is like taking a stack of 64 patient folders. Each folder contains one image that has already been prepared and cleaned up (resized, normalized, etc.) so the doctors can understand it perfectly.\n",
        "\n",
        "ft_model(**inputs): The ** before inputs is just a technical way of saying, \"Hey, model! Here's this whole stack of 64 patient folders.\" Instead of handing them one by one, you give the entire stack to the doctor team at once.\n",
        "\n",
        "How the doctors (model) process the batch: The team of doctors doesn't just look at one image, finish their diagnosis, and then move to the next. Instead, they efficiently look at all 64 images almost simultaneously. Each doctor (representing a specific condition) quickly scans all 64 images and forms an initial opinion about whether their condition is present in each image.\n",
        "\n",
        "outputs: This is the collective report from all ten doctors for all 64 patients. It's a structured collection of their findings.\n",
        "\n",
        "logits: Within that outputs report, logits are the raw initial suspicion scores from each doctor. For every one of the 64 images, and for every one of the ten conditions, each doctor provides a number. This number isn't yet a neat probability (like \"80% chance of Eczema\"). Instead, it's a raw, unscaled score that indicates how strongly that specific doctor suspects their condition is present in that specific image. A higher positive number means stronger suspicion, a negative number means less suspicion, and zero means neutral. These scores will later be converted into actual probabilities (0 to 1) using the sigmoid function, which is like turning the doctors' raw suspicions into a final percentage chance."
      ],
      "metadata": {
        "id": "aC1mBVG43_tv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PchtnMRmpHxV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "prediction_scores = []\n",
        "for batch in data[\"test\"].batch(batch_size=64):\n",
        "    images = [Image.open(image[\"path\"]) for image in batch[\"image\"]]\n",
        "    inputs = image_processor(images=images, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = ft_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    scores = torch.sigmoid(logits)\n",
        "    prediction_scores.extend(scores)\n",
        "\n",
        "prediction_scores = torch.stack(prediction_scores).cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMmPD1aWpBsJ"
      },
      "source": [
        "Compute metrics.\n",
        "\n",
        "**Note:** This notebook demonstrates a sample training run and the metrics below have not been optimized. Further tuning will be needed to achieve desired performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkpCjDhgf-ZK"
      },
      "outputs": [],
      "source": [
        "# Default threshold used to convert probability scores into class predictions.\n",
        "# Note that optimal threshold selection is not demonstrated in this notebook.\n",
        "threshold = 0.5\n",
        "\n",
        "metrics = compute_metrics(prediction_scores, threshold)\n",
        "print_metrics(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZCmEq3m1pJC"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medsiglip/blob/main/notebooks) to learn what else you can do with the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf94a27"
      },
      "source": [
        "# Task\n",
        "Evaluate the fine-tuned model on the `data['test']` dataset by performing predictions for each image, extracting original labels and the top 5 predicted conditions. Calculate the 'Top 5 Intersection Accuracy' by determining if any of the original labels are present in the top 5 predicted conditions. Additionally, compute and print the macro-averaged one-vs-rest ROC AUC, sensitivity, and specificity for all conditions using the `compute_metrics` and `print_metrics` functions, and finally, summarize the model's overall performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f77aa4a"
      },
      "source": [
        "## Evaluate Model on Test Set\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the entire `data['test']` dataset. For each image, perform the prediction using the fine-tuned model. Extract the original labels and the top 5 predicted conditions. Compare these two sets to check for any intersection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dd27e2c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires iterating through the test dataset, making predictions, extracting top 5 predicted conditions, and comparing them with ground truth labels to count successful matches. This code block will implement these steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a749228c"
      },
      "source": [
        "successful_matches = 0\n",
        "\n",
        "for i in range(len(data[\"test\"])):\n",
        "    example = data[\"test\"][i]\n",
        "    image = example[\"image\"]\n",
        "    original_label_onehot = example[\"label\"]\n",
        "\n",
        "    # Preprocess the image\n",
        "    preprocessed_image = image_processor(images=[image.convert(\"RGB\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = ft_model(**preprocessed_image)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "\n",
        "    # Get top 5 predicted conditions\n",
        "    top_5_indices = probabilities.argsort()[-5:][::-1] # Get indices of top 5 probabilities, in descending order\n",
        "    top_5_predicted_conditions = [CONDITIONS[idx] for idx in top_5_indices]\n",
        "\n",
        "    # Convert original label to condition names\n",
        "    original_condition_names = [CONDITIONS[idx] for idx, val in enumerate(original_label_onehot) if val == 1]\n",
        "\n",
        "    # Check for intersection\n",
        "    if set(original_condition_names).intersection(set(top_5_predicted_conditions)):\n",
        "        successful_matches += 1\n",
        "\n",
        "print(f\"Total test examples: {len(data['test'])}\")\n",
        "print(f\"Successful matches (at least one original label in top 5 predictions): {successful_matches}\")\n",
        "\n",
        "top_5_intersection_accuracy = (successful_matches / len(data[\"test\"])) * 100\n",
        "print(f\"Top 5 Intersection Accuracy: {top_5_intersection_accuracy:.2f}%\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "fine_tune_for_image_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}