{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506affb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf99a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"google/medsiglip-448\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"google/medsiglip-448\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample image\n",
    "imgs = [Image.open(\"dataset/images/3445096909671059178.png\").convert(\"RGB\"), Image.open(\"dataset/images/-5669089898008966381.png\").convert(\"RGB\")]\n",
    "\n",
    "texts = [\n",
    "    \"a photo of an arm with no rash\",\n",
    "    \"a photo of an arm with a rash\",\n",
    "    \"a photo of a leg with no rash\",\n",
    "    \"a photo of a leg with a rash\"\n",
    "]\n",
    "\n",
    "inputs = processor(text=texts, images=imgs, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = torch.softmax(logits_per_image, dim=1)\n",
    "\n",
    "for n_img, img in enumerate(imgs):\n",
    "    display(img)  # Note this is an IPython function that will only work in a Jupyter notebook environment\n",
    "    for i, label in enumerate(texts):\n",
    "        print(f\"{probs[n_img][i]:.2%} that image is '{label}'\")\n",
    "\n",
    "# Get the image and text embeddings\n",
    "print(f\"image embeddings: {outputs.image_embeds}\")\n",
    "print(f\"text embeddings: {outputs.text_embeds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the seconf image so the \n",
    "image2 = Image.open(\"dataset/images/-5669089898008966381.png\").convert(\"RGB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    image_embedding = outputs.image_embeds  # Shape: [1, embedding_dim]\n",
    "    text_embeddings = outputs.text_embeds   # Shape: [num_texts, embedding_dim]\n",
    "\n",
    "print(f\"Image embedding shape: {image_embedding.shape}\")\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"\\nImage embedding (first 10 values): {image_embedding[0][:10]}\")\n",
    "\n",
    "# Compute similarity\n",
    "image_embedding_norm = torch.nn.functional.normalize(image_embedding, p=2, dim=-1)\n",
    "embedding_list = image_embedding_norm[0].cpu().tolist()\n",
    "\n",
    "text_embeddings_norm = torch.nn.functional.normalize(text_embeddings, p=2, dim=-1)\n",
    "\n",
    "similarities = torch.matmul(image_embedding_norm, text_embeddings_norm.T)\n",
    "print(f\"\\nSimilarity scores:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"  '{text}': {similarities[0][i]:.4f}\")\n",
    "\n",
    "# If you want JUST the image embedding for storage/comparison later\n",
    "# You can extract it from this output\n",
    "print(f\"\\nImage embedding can be saved: {image_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e672b5a",
   "metadata": {},
   "source": [
    "**Call Gemini with Text Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a99dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyDXjCnAAfPT4C6fgEqT6GFZFInyPV7zHow\")\n",
    "\n",
    "print(f\"Image embedding shape: {image_embedding.shape}\")\n",
    "print(f\"Embedding vector length: {len(embedding_list)}\")\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"Analyze this image and its SigLIP embedding.\n",
    "\n",
    "Embedding Statistics:\n",
    "- Dimensions: {len(embedding_list)}\n",
    "- Range: [{min(embedding_list):.4f}, {max(embedding_list):.4f}]\n",
    "- Mean: {sum(embedding_list)/len(embedding_list):.4f}\n",
    "- First 20 values: {embedding_list[:20]}\n",
    "\n",
    "Please:\n",
    "1. Describe what you see in the image in detail\n",
    "2. Generate a natural language caption for this image\n",
    "3. Explain what visual features these embedding values might represent\n",
    "\"\"\"\n",
    "\n",
    "# Call Gemini API\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash-exp',\n",
    "        contents=[prompt, image2]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GEMINI ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(response.text)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error calling Gemini: {e}\")\n",
    "    print(f\"Error type: {type(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "model_name = \"puyangwang/medgemma-27b-it:q8\"  # Try the simpler model name first\n",
    "\n",
    "ollama_client = ollama.Client()\n",
    "\n",
    "# Verify the model exists and check its details\n",
    "try:\n",
    "    model_info = ollama_client.show(model_name)\n",
    "    print(f\"Model info: {model_info}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model info error: {e}\")\n",
    "\n",
    "# Convert PIL Image to base64 - try JPEG format\n",
    "buffered = io.BytesIO()\n",
    "image2.save(buffered, format=\"JPEG\")  # Try JPEG instead of PNG\n",
    "image_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Alternative: Pass the raw bytes instead of base64\n",
    "buffered2 = io.BytesIO()\n",
    "image2.save(buffered2, format=\"JPEG\")\n",
    "image_bytes = buffered2.getvalue()\n",
    "\n",
    "ollama_prompt = \"Describe this image in detail and generate a natural language caption for it.\"\n",
    "\n",
    "# Try method 1: with base64 string\n",
    "try:\n",
    "    print(\"Attempting with base64 string...\")\n",
    "    response = ollama_client.chat(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': ollama_prompt,\n",
    "                'images': [image_base64]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(\"Success!\")\n",
    "    print(response['message']['content'])\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Base64 method failed: {e}\")\n",
    "    \n",
    "    # Try method 2: with raw bytes\n",
    "    try:\n",
    "        print(\"\\nAttempting with raw bytes...\")\n",
    "        response = ollama_client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': ollama_prompt,\n",
    "                    'images': [image_bytes]\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        print(\"Success!\")\n",
    "        print(response['message']['content'])\n",
    "    except Exception as e2:\n",
    "        print(f\"Raw bytes method also failed: {e2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
