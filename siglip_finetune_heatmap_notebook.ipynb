{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa946627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoo/projects/health-kiosk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration: {'MODEL_ID': 'google/siglip-base-patch16-224', 'OUTPUT_DIR': './siglip-scin-lora', 'DATA_DIR': './data/scin_cache', 'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'LORA_RANK': 16, 'LORA_ALPHA': 16, 'MAX_STEPS': 500, 'LOSS_TYPE': 'sigmoid', 'N_VAL_SAMPLES': 1000, 'N_TRAIN_SAMPLES': 5000}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'MODEL_ID': \"google/siglip-base-patch16-224\",\n",
    "    'OUTPUT_DIR': \"./siglip-scin-lora\",\n",
    "    'DATA_DIR': \"./data/scin_cache\",  # Local cache directory\n",
    "    'BATCH_SIZE': 16,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'LORA_RANK': 16,\n",
    "    'LORA_ALPHA': 16,\n",
    "    'MAX_STEPS': 500,\n",
    "    'LOSS_TYPE': \"sigmoid\",  # or \"contrastive\"\n",
    "    'N_VAL_SAMPLES': 1000,\n",
    "    'N_TRAIN_SAMPLES': 5000,  # Set to None to use all available\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "os.makedirs(CONFIG['DATA_DIR'], exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0dc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading val samples: 100%|██████████| 1000/1000 [01:06<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training samples (max: 5000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train samples:  81%|████████  | 4033/5000 [04:32<01:05, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 3032 training samples to cache...\n",
      "Saving 747 validation samples to cache...\n",
      "Dataset cached to ./data/scin_cache\n",
      "\n",
      "Dataset loaded: 3032 train, 747 val samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading and Caching Functions\n",
    "\n",
    "def download_and_cache_dataset(n_train=5000, n_val=1000, force_redownload=False):\n",
    "    \"\"\"\n",
    "    Download dataset from HuggingFace and cache locally.\n",
    "    \n",
    "    Args:\n",
    "        n_train: Number of training samples (None for all)\n",
    "        n_val: Number of validation samples\n",
    "        force_redownload: Force redownload even if cache exists\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data_list, val_data_list)\n",
    "    \"\"\"\n",
    "    train_cache = Path(CONFIG['DATA_DIR']) / f\"train_{n_train}.pkl\"\n",
    "    val_cache = Path(CONFIG['DATA_DIR']) / f\"val_{n_val}.pkl\"\n",
    "    \n",
    "    # Check if cache exists\n",
    "    if not force_redownload and train_cache.exists() and val_cache.exists():\n",
    "        print(f\"Loading cached dataset from {CONFIG['DATA_DIR']}\")\n",
    "        with open(train_cache, 'rb') as f:\n",
    "            train_data = pickle.load(f)\n",
    "        with open(val_cache, 'rb') as f:\n",
    "            val_data = pickle.load(f)\n",
    "        print(f\"Loaded {len(train_data)} training samples and {len(val_data)} validation samples from cache\")\n",
    "        return train_data, val_data\n",
    "    \n",
    "    # Download from HuggingFace\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    try:\n",
    "        base_iterable = load_dataset(\"google/scin\", split=\"train\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset. Error: {e}\")\n",
    "        print(\"Please ensure you have an internet connection and have accepted 'google/scin' terms if any.\")\n",
    "        raise\n",
    "    \n",
    "    image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
    "    \n",
    "    # Load validation data\n",
    "    print(f\"Loading {n_val} validation samples...\")\n",
    "    val_data = []\n",
    "    for item in tqdm(base_iterable.take(n_val), total=n_val, desc=\"Loading val samples\"):\n",
    "        text = item.get(\"related_category\")\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    val_data.append({\"image\": image.convert(\"RGB\"), \"text\": text})\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                    break\n",
    "    \n",
    "    # Load training data\n",
    "    train_iterable = base_iterable.skip(n_val)\n",
    "    print(f\"Loading training samples (max: {n_train if n_train else 'all'})...\")\n",
    "    train_data = []\n",
    "    \n",
    "    if n_train:\n",
    "        iterator = tqdm(train_iterable.take(n_train), total=n_train, desc=\"Loading train samples\")\n",
    "    else:\n",
    "        iterator = tqdm(train_iterable, desc=\"Loading train samples\")\n",
    "    \n",
    "    for item in iterator:\n",
    "        text = item.get(\"related_category\")\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    train_data.append({\"image\": image.convert(\"RGB\"), \"text\": text})\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                    break\n",
    "        \n",
    "        if n_train and len(train_data) >= n_train:\n",
    "            break\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving {len(train_data)} training samples to cache...\")\n",
    "    with open(train_cache, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    \n",
    "    print(f\"Saving {len(val_data)} validation samples to cache...\")\n",
    "    with open(val_cache, 'wb') as f:\n",
    "        pickle.dump(val_data, f)\n",
    "    \n",
    "    print(f\"Dataset cached to {CONFIG['DATA_DIR']}\")\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear all cached dataset files.\"\"\"\n",
    "    cache_dir = Path(CONFIG['DATA_DIR'])\n",
    "    if cache_dir.exists():\n",
    "        for file in cache_dir.glob(\"*.pkl\"):\n",
    "            file.unlink()\n",
    "            print(f\"Deleted {file}\")\n",
    "        print(\"Cache cleared\")\n",
    "    else:\n",
    "        print(\"No cache to clear\")\n",
    "\n",
    "\n",
    "# Load or download dataset\n",
    "train_data, val_data = download_and_cache_dataset(\n",
    "    n_train=CONFIG['N_TRAIN_SAMPLES'],\n",
    "    n_val=CONFIG['N_VAL_SAMPLES'],\n",
    "    force_redownload=False  # Set to True to force redownload\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(train_data)} train, {len(val_data)} val samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb328cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SCIN_Dataset with 3032 samples.\n",
      "Initializing SCIN_Dataset with 747 samples.\n",
      "Datasets created: 3032 train, 747 val\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Classes\n",
    "\n",
    "class SCIN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for SCIN data.\n",
    "    Works with pre-loaded data lists (from cache or download).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        print(f\"Initializing SCIN_Dataset with {len(data_list)} samples.\")\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    \"\"\"\n",
    "    Data collator with robust error handling.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    processed_texts_input_ids = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            if item is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            img = item.get(\"image\")\n",
    "            txt = item.get(\"text\")\n",
    "\n",
    "            # Check for invalid content\n",
    "            if img is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            if txt is None or txt.strip() == \"\":\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Process valid items\n",
    "            inputs = processor(\n",
    "                text=[txt],\n",
    "                images=[img],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=64\n",
    "            )\n",
    "\n",
    "            processed_images.append(inputs[\"pixel_values\"])\n",
    "            processed_texts_input_ids.append(inputs[\"input_ids\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING (collate_fn): Skipping item {i} due to error: {e}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    if not processed_images:\n",
    "        if len(batch) > 0:\n",
    "            print(f\"ERROR: Entire batch was skipped! ({skipped_count} items failed)\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        batch_pixel_values = torch.cat(processed_images, dim=0)\n",
    "        batch_input_ids = torch.cat(processed_texts_input_ids, dim=0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": batch_pixel_values,\n",
    "            \"input_ids\": batch_input_ids\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final batch collation: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SCIN_Dataset(train_data)\n",
    "val_dataset = SCIN_Dataset(val_data)\n",
    "\n",
    "print(f\"Datasets created: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29799ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor and models from: google/siglip-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and processor loaded successfully\n",
      "Applying LoRA configuration (rank=16, alpha=16)...\n",
      "LoRA applied. Trainable parameters:\n",
      "trainable params: 1,179,648 || all params: 204,335,618 || trainable%: 0.5773\n",
      "\n",
      "Models ready on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Loading\n",
    "\n",
    "def load_models_and_processor(model_id, device):\n",
    "    \"\"\"\n",
    "    Load processor and two model instances (base and tunable).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (processor, base_model, model_to_tune)\n",
    "    \"\"\"\n",
    "    print(f\"Loading processor and models from: {model_id}\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    \n",
    "    # Base model (for baseline evaluation)\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype\n",
    "    ).to(device)\n",
    "    \n",
    "    # Model to fine-tune\n",
    "    model_to_tune = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    \n",
    "    print(\"Models and processor loaded successfully\")\n",
    "    return processor, base_model, model_to_tune\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=16, alpha=16):\n",
    "    \"\"\"\n",
    "    Apply LoRA configuration to a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "    \n",
    "    Returns:\n",
    "        Model with LoRA applied\n",
    "    \"\"\"\n",
    "    print(f\"Applying LoRA configuration (rank={rank}, alpha={alpha})...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\"LoRA applied. Trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load models\n",
    "processor, base_model, model_to_tune = load_models_and_processor(\n",
    "    CONFIG['MODEL_ID'], \n",
    "    device\n",
    ")\n",
    "\n",
    "# Apply LoRA to the tunable model\n",
    "model_to_tune = apply_lora(\n",
    "    model_to_tune,\n",
    "    rank=CONFIG['LORA_RANK'],\n",
    "    alpha=CONFIG['LORA_ALPHA']\n",
    ")\n",
    "model_to_tune = model_to_tune.to(device)\n",
    "\n",
    "print(f\"\\nModels ready on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deeebe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics and loss functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Metrics and Loss Functions\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Debug: Check what we received\n",
    "    print(f\"\\n[compute_metrics] Called with predictions type: {type(eval_pred.predictions)}\")\n",
    "    \n",
    "    if eval_pred.predictions is None:\n",
    "        print(\"[compute_metrics] ERROR: predictions is None!\")\n",
    "        return {\n",
    "            \"accuracy\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1\": 0.0,\n",
    "        }\n",
    "    \n",
    "    logits = eval_pred.predictions\n",
    "    print(f\"[compute_metrics] Predictions shape: {logits.shape}\")\n",
    "    print(f\"[compute_metrics] Predictions range: [{logits.min():.4f}, {logits.max():.4f}]\")\n",
    "    \n",
    "    # For contrastive learning, each sample should match itself\n",
    "    # So we predict which position in the batch is the correct match\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.arange(len(predictions))\n",
    "    \n",
    "    print(f\"[compute_metrics] Predicted labels (first 10): {predictions[:10]}\")\n",
    "    print(f\"[compute_metrics] True labels (first 10): {true_labels[:10]}\")\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"[compute_metrics] Computed: acc={acc:.4f}, prec={precision:.4f}, rec={recall:.4f}, f1={f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_loss_function(logits_per_image, logits_per_text, loss_type=\"sigmoid\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute loss based on loss type.\n",
    "    \n",
    "    Args:\n",
    "        logits_per_image: Image logits\n",
    "        logits_per_text: Text logits\n",
    "        loss_type: \"contrastive\" or \"sigmoid\"\n",
    "        device: Device for computation\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    \n",
    "    if batch_size <= 1:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    if loss_type == \"contrastive\":\n",
    "        labels = torch.arange(batch_size, device=device)\n",
    "        loss_images = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_images + loss_text) / 2.0\n",
    "    elif loss_type == \"sigmoid\":\n",
    "        labels = torch.eye(batch_size, device=device)\n",
    "        loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "        loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "        loss = (loss_images + loss_text) / 2.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"Metrics and loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084dfb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTrainer class defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Custom Trainer Class\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with:\n",
    "    1. Switchable loss (Contrastive or Sigmoid)\n",
    "    2. Gradient accumulation for heatmap\n",
    "    3. Proper evaluation with loss and metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, loss_type=\"contrastive\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        print(f\"CustomTrainer initialized with loss_type: {self.loss_type}\")\n",
    "        \n",
    "        # Gradient tracking for heatmap\n",
    "        self.gradient_accumulator = defaultdict(float)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Batch stats\n",
    "        self.successful_batches = 0\n",
    "        self.skipped_batches_eval = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Training loss computation.\"\"\"\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            dummy_loss = torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "            return (dummy_loss, {}) if return_outputs else dummy_loss\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        \n",
    "        loss = compute_loss_function(\n",
    "            logits_per_image,\n",
    "            logits_per_text,\n",
    "            loss_type=self.loss_type,\n",
    "            device=model.device\n",
    "        )\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        \"\"\"Training step with gradient tracking.\"\"\"\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "        if loss is not None:\n",
    "            self.step_count += 1\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and param.requires_grad:\n",
    "                        self.gradient_accumulator[name] += param.grad.norm().item()\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"Evaluation step with loss computation.\"\"\"\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            self.skipped_batches_eval += 1\n",
    "            return (None, None, None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            batch_size = logits_per_image.shape[0]\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "                print(\"WARNING: NaN or Inf detected in logits during eval.\")\n",
    "                self.skipped_batches_eval += 1\n",
    "                return (None, None, None)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = None\n",
    "            if batch_size <= 1:\n",
    "                self.skipped_batches_eval += 1\n",
    "            else:\n",
    "                loss = compute_loss_function(\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    loss_type=self.loss_type,\n",
    "                    device=model.device\n",
    "                )\n",
    "                self.successful_batches += 1\n",
    "        \n",
    "        predictions = logits_per_image.cpu()\n",
    "        return (loss, predictions, None)\n",
    "\n",
    "    def _extract_layer_index(self, name_parts):\n",
    "        \"\"\"Extract layer index from parameter name.\"\"\"\n",
    "        for part in name_parts:\n",
    "            if part.isdigit():\n",
    "                return int(part)\n",
    "        return None\n",
    "\n",
    "    def _extract_component_name(self, name_parts):\n",
    "        \"\"\"Extract component name from parameter name.\"\"\"\n",
    "        name_str = \".\".join(name_parts)\n",
    "        if \"lora_A\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA A (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA A (Value)\"\n",
    "        elif \"lora_B\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA B (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA B (Value)\"\n",
    "        if \"q_proj\" in name_str: return \"Query Proj\"\n",
    "        if \"v_proj\" in name_str: return \"Value Proj\"\n",
    "        if \"k_proj\" in name_str: return \"Key Proj\"\n",
    "        if \"fc1\" in name_str: return \"MLP Layer 1\"\n",
    "        if \"fc2\" in name_str: return \"MLP Layer 2\"\n",
    "        return None\n",
    "\n",
    "    def _process_gradients_for_heatmap(self):\n",
    "        \"\"\"Process accumulated gradients for heatmap generation.\"\"\"\n",
    "        if self.step_count == 0:\n",
    "            print(\"No training steps recorded. Skipping heatmap.\")\n",
    "            return None, None, []\n",
    "\n",
    "        vision_data = defaultdict(lambda: defaultdict(float))\n",
    "        text_data = defaultdict(lambda: defaultdict(float))\n",
    "        skipped_params = []\n",
    "\n",
    "        for name, avg_grad_norm in self.gradient_accumulator.items():\n",
    "            avg_norm = avg_grad_norm / self.step_count\n",
    "            parts = name.split('.')\n",
    "            layer_idx = self._extract_layer_index(parts)\n",
    "            component = self._extract_component_name(parts)\n",
    "            \n",
    "            if layer_idx is None or component is None:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "                continue\n",
    "            \n",
    "            if \"vision_model\" in name:\n",
    "                vision_data[layer_idx][component] = avg_norm\n",
    "            elif \"text_model\" in name:\n",
    "                text_data[layer_idx][component] = avg_norm\n",
    "            else:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "\n",
    "        vision_df = pd.DataFrame.from_dict(vision_data, orient='index').sort_index()\n",
    "        text_df = pd.DataFrame.from_dict(text_data, orient='index').sort_index()\n",
    "        return vision_df, text_df, skipped_params\n",
    "\n",
    "    def plot_final_heatmap(self, save_path):\n",
    "        \"\"\"Generate and save gradient impact heatmap.\"\"\"\n",
    "        print(\"\\nGenerating final gradient heatmaps...\")\n",
    "        vision_df, text_df, skipped = self._process_gradients_for_heatmap()\n",
    "        \n",
    "        if vision_df is None or (vision_df.empty and text_df.empty):\n",
    "            print(\"No gradient data collected. Skipping heatmap file.\")\n",
    "            return\n",
    "        \n",
    "        if skipped:\n",
    "            print(f\"[WARN] Skipped {len(skipped)} LoRA params (couldn't parse name)\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "        vmin = 0.0\n",
    "        vmax = max(\n",
    "            vision_df.max().max() if not vision_df.empty else 0,\n",
    "            text_df.max().max() if not text_df.empty else 0\n",
    "        )\n",
    "        if vmax == 0:\n",
    "            vmax = 1.0\n",
    "\n",
    "        if not vision_df.empty:\n",
    "            sns.heatmap(vision_df, ax=ax1, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax1.set_title(\"Vision Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax1.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax1.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"No Vision Gradients Found\", ha='center', va='center')\n",
    "            ax1.set_title(\"Vision Encoder Impact\", fontsize=16)\n",
    "\n",
    "        if not text_df.empty:\n",
    "            sns.heatmap(text_df, ax=ax2, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax2.set_title(\"Text Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax2.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax2.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No Text Gradients Found\", ha='center', va='center')\n",
    "            ax2.set_title(\"Text Encoder Impact\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Heatmap saved to: {save_path}\")\n",
    "\n",
    "\n",
    "print(\"CustomTrainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77453198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DEBUG training arguments (quick run)...\n",
      "CustomTrainer initialized with loss_type: sigmoid\n",
      "Trainer initialized and ready\n",
      "Loss type: sigmoid\n",
      "Training steps: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training Setup\n",
    "def create_training_args(config, device, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Create training arguments.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        device: Device string\n",
    "        debug_mode: If True, use minimal steps for quick testing\n",
    "    \n",
    "    Returns:\n",
    "        TrainingArguments\n",
    "    \"\"\"\n",
    "    use_fp16 = True if device == \"cuda\" else False\n",
    "    \n",
    "    if debug_mode:\n",
    "        print(\"Creating DEBUG training arguments (quick run)...\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=config['OUTPUT_DIR'],\n",
    "            per_device_train_batch_size=config['BATCH_SIZE'],\n",
    "            per_device_eval_batch_size=config['BATCH_SIZE'],\n",
    "            max_steps=2,\n",
    "            eval_strategy=\"steps\",  # THIS IS THE FIX\n",
    "            eval_steps=1,\n",
    "            logging_steps=1,\n",
    "            warmup_steps=1,\n",
    "            weight_decay=0.01,\n",
    "            learning_rate=config['LEARNING_RATE'],\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            fp16=use_fp16,\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "            prediction_loss_only=False,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating FULL training arguments...\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=config['OUTPUT_DIR'],\n",
    "            per_device_train_batch_size=config['BATCH_SIZE'],\n",
    "            per_device_eval_batch_size=config['BATCH_SIZE'],\n",
    "            max_steps=config['MAX_STEPS'],\n",
    "            weight_decay=0.01,\n",
    "            learning_rate=config['LEARNING_RATE'],\n",
    "            warmup_steps=50,\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=250,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=250,\n",
    "            load_best_model_at_end=False,\n",
    "            fp16=use_fp16,\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "            prediction_loss_only=False,\n",
    "        )\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "# Create training arguments\n",
    "# Set debug_mode=True for quick testing, False for full training\n",
    "training_args = create_training_args(CONFIG, device, debug_mode=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model_to_tune,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: collate_fn(data, processor),\n",
    "    loss_type=CONFIG['LOSS_TYPE'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized and ready\")\n",
    "print(f\"Loss type: {CONFIG['LOSS_TYPE']}\")\n",
    "print(f\"Training steps: {training_args.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfe6ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL: Manual Evaluation Function (Use this instead of trainer.evaluate())\n",
    "\n",
    "# def manual_evaluate(model, eval_dataset, processor, batch_size=16, device='cuda', loss_type='sigmoid'):\n",
    "#     \"\"\"\n",
    "#     Manually evaluate the model on the eval dataset.\n",
    "#     This bypasses the Trainer's evaluation loop issues.\n",
    "    \n",
    "#     Args:\n",
    "#         model: Model to evaluate\n",
    "#         eval_dataset: Dataset to evaluate on\n",
    "#         processor: Processor for collation\n",
    "#         batch_size: Batch size\n",
    "#         device: Device string\n",
    "#         loss_type: Loss type ('sigmoid' or 'contrastive')\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary of metrics\n",
    "#     \"\"\"\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     import time\n",
    "    \n",
    "#     print(f\"\\nManually evaluating on {len(eval_dataset)} samples...\")\n",
    "    \n",
    "#     model.eval()\n",
    "    \n",
    "#     # Create dataloader\n",
    "#     dataloader = DataLoader(\n",
    "#         eval_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=lambda batch: collate_fn(batch, processor)\n",
    "#     )\n",
    "    \n",
    "#     all_losses = []\n",
    "#     all_predictions = []  # Store predictions for each sample individually\n",
    "#     skipped_batches = 0\n",
    "#     total_samples = 0\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, inputs in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "#             # Skip empty batches\n",
    "#             if not inputs or \"pixel_values\" not in inputs:\n",
    "#                 skipped_batches += 1\n",
    "#                 continue\n",
    "            \n",
    "#             # Move to device\n",
    "#             inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "#             # Forward pass\n",
    "#             try:\n",
    "#                 outputs = model(**inputs)\n",
    "#                 logits_per_image = outputs.logits_per_image\n",
    "#                 logits_per_text = outputs.logits_per_text\n",
    "#                 batch_size_actual = logits_per_image.shape[0]\n",
    "                \n",
    "#                 # Skip batches with size 1\n",
    "#                 if batch_size_actual <= 1:\n",
    "#                     skipped_batches += 1\n",
    "#                     continue\n",
    "                \n",
    "#                 # Compute loss\n",
    "#                 loss = compute_loss_function(\n",
    "#                     logits_per_image,\n",
    "#                     logits_per_text,\n",
    "#                     loss_type=loss_type,\n",
    "#                     device=device\n",
    "#                 )\n",
    "                \n",
    "#                 all_losses.append(loss.item())\n",
    "                \n",
    "#                 # For each sample in the batch, extract its predictions\n",
    "#                 # We need to handle variable batch sizes correctly\n",
    "#                 logits_np = logits_per_image.cpu().numpy()\n",
    "                \n",
    "#                 # For contrastive learning, each sample should predict its own position\n",
    "#                 # in the current batch. So for a batch of size N:\n",
    "#                 # - Sample 0 should predict position 0\n",
    "#                 # - Sample 1 should predict position 1, etc.\n",
    "#                 for i in range(batch_size_actual):\n",
    "#                     predicted_position = np.argmax(logits_np[i])\n",
    "#                     true_position = i\n",
    "                    \n",
    "#                     # Store as binary: correct (1) or incorrect (0)\n",
    "#                     all_predictions.append({\n",
    "#                         'predicted': predicted_position,\n",
    "#                         'true': true_position,\n",
    "#                         'correct': int(predicted_position == true_position),\n",
    "#                         'batch_size': batch_size_actual\n",
    "#                     })\n",
    "                \n",
    "#                 total_samples += batch_size_actual\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in batch {batch_idx}: {e}\")\n",
    "#                 skipped_batches += 1\n",
    "#                 continue\n",
    "    \n",
    "#     runtime = time.time() - start_time\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     if not all_predictions:\n",
    "#         print(\"ERROR: No valid predictions collected!\")\n",
    "#         return {\n",
    "#             \"eval_loss\": float('nan'),\n",
    "#             \"eval_accuracy\": 0.0,\n",
    "#             \"eval_precision\": 0.0,\n",
    "#             \"eval_recall\": 0.0,\n",
    "#             \"eval_f1\": 0.0,\n",
    "#             \"eval_runtime\": runtime,\n",
    "#             \"eval_samples_per_second\": 0.0,\n",
    "#         }\n",
    "    \n",
    "#     # Calculate accuracy (simple: how many correct predictions)\n",
    "#     num_correct = sum(p['correct'] for p in all_predictions)\n",
    "#     accuracy = num_correct / len(all_predictions)\n",
    "    \n",
    "#     # For precision, recall, F1, we need to be more careful\n",
    "#     # In contrastive learning, we have a multi-class problem where each batch\n",
    "#     # has its own set of classes. Let's compute per-batch metrics and average.\n",
    "    \n",
    "#     # Group predictions by batch\n",
    "#     batch_metrics = []\n",
    "#     current_batch = []\n",
    "#     current_batch_size = None\n",
    "    \n",
    "#     for pred in all_predictions:\n",
    "#         if current_batch_size is None:\n",
    "#             current_batch_size = pred['batch_size']\n",
    "        \n",
    "#         if pred['batch_size'] == current_batch_size and len(current_batch) < current_batch_size:\n",
    "#             current_batch.append(pred)\n",
    "#         else:\n",
    "#             # Process completed batch\n",
    "#             if current_batch:\n",
    "#                 y_true = [p['true'] for p in current_batch]\n",
    "#                 y_pred = [p['predicted'] for p in current_batch]\n",
    "                \n",
    "#                 # Only compute metrics if we have the full batch\n",
    "#                 if len(y_true) == current_batch_size:\n",
    "#                     prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "#                         y_true, y_pred, average='macro', zero_division=0\n",
    "#                     )\n",
    "#                     batch_metrics.append({\n",
    "#                         'precision': prec,\n",
    "#                         'recall': rec,\n",
    "#                         'f1': f1\n",
    "#                     })\n",
    "            \n",
    "#             # Start new batch\n",
    "#             current_batch = [pred]\n",
    "#             current_batch_size = pred['batch_size']\n",
    "    \n",
    "#     # Process last batch\n",
    "#     if current_batch and len(current_batch) == current_batch_size:\n",
    "#         y_true = [p['true'] for p in current_batch]\n",
    "#         y_pred = [p['predicted'] for p in current_batch]\n",
    "#         prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "#             y_true, y_pred, average='macro', zero_division=0\n",
    "#         )\n",
    "#         batch_metrics.append({\n",
    "#             'precision': prec,\n",
    "#             'recall': rec,\n",
    "#             'f1': f1\n",
    "#         })\n",
    "    \n",
    "#     # Average metrics across batches\n",
    "#     if batch_metrics:\n",
    "#         avg_precision = np.mean([m['precision'] for m in batch_metrics])\n",
    "#         avg_recall = np.mean([m['recall'] for m in batch_metrics])\n",
    "#         avg_f1 = np.mean([m['f1'] for m in batch_metrics])\n",
    "#     else:\n",
    "#         avg_precision = 0.0\n",
    "#         avg_recall = 0.0\n",
    "#         avg_f1 = 0.0\n",
    "    \n",
    "#     avg_loss = np.mean(all_losses) if all_losses else float('nan')\n",
    "#     samples_per_second = total_samples / runtime if runtime > 0 else 0.0\n",
    "    \n",
    "#     metrics = {\n",
    "#         \"eval_loss\": avg_loss,\n",
    "#         \"eval_accuracy\": accuracy,\n",
    "#         \"eval_precision\": avg_precision,\n",
    "#         \"eval_recall\": avg_recall,\n",
    "#         \"eval_f1\": avg_f1,\n",
    "#         \"eval_runtime\": runtime,\n",
    "#         \"eval_samples_per_second\": samples_per_second,\n",
    "#         \"eval_samples\": total_samples,\n",
    "#         \"eval_batches_processed\": len(all_losses),\n",
    "#         \"eval_batches_skipped\": skipped_batches,\n",
    "#     }\n",
    "    \n",
    "#     print(f\"\\nEvaluation complete!\")\n",
    "#     print(f\"  Batches processed: {len(all_losses)}\")\n",
    "#     print(f\"  Batches skipped: {skipped_batches}\")\n",
    "#     print(f\"  Total samples evaluated: {total_samples}\")\n",
    "#     print(f\"  Correct predictions: {num_correct}/{len(all_predictions)}\")\n",
    "#     print(f\"  Runtime: {runtime:.2f}s\")\n",
    "#     print(f\"  Samples/sec: {samples_per_second:.2f}\")\n",
    "    \n",
    "#     return metrics\n",
    "\n",
    "\n",
    "# print(\"Manual evaluation function defined (FIXED for variable batch sizes)!\")\n",
    "# print(\"Use: metrics = manual_evaluate(model, val_dataset, processor, device=device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57618ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)\n",
      "============================================================\n",
      "\n",
      "Baseline Evaluation Metrics:\n",
      "{'eval_loss': 0.6139736771583557, 'eval_runtime': 5.3692, 'eval_samples_per_second': 139.126, 'eval_steps_per_second': 8.754, 'epoch': 0.010526315789473684}\n",
      "\n",
      "Baseline evaluation complete. Ready for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Run Baseline Evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the Trainer's standard evaluate() method\n",
    "# We temporarily swap in the base_model to evaluate it\n",
    "trainer.model = base_model\n",
    "baseline_metrics = trainer.evaluate()\n",
    "trainer.model = model_to_tune # Swap back to the model we're training\n",
    "\n",
    "print(\"\\nBaseline Evaluation Metrics:\")\n",
    "print(baseline_metrics) # This will now have all metrics\n",
    "\n",
    "print(\"\\nBaseline evaluation complete. Ready for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958aad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8a: Run Baseline Evaluation ( Uses manual_evaluate function )\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Use manual evaluation for more reliable results\n",
    "# baseline_metrics = manual_evaluate(\n",
    "#     model=base_model,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     processor=processor,\n",
    "#     batch_size=CONFIG['BATCH_SIZE'],\n",
    "#     device=device,\n",
    "#     loss_type=CONFIG['LOSS_TYPE']\n",
    "# )\n",
    "\n",
    "# print(\"\\nBaseline Evaluation Metrics:\")\n",
    "# for key, value in baseline_metrics.items():\n",
    "#     if isinstance(value, float):\n",
    "#         print(f\"  {key}: {value:.4f}\")\n",
    "#     else:\n",
    "#         print(f\"  {key}: {value}\")\n",
    "\n",
    "# print(\"\\nBaseline evaluation complete. Ready for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03b0115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING FINE-TUNING\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.581083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.548397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINE-TUNING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run Fine-Tuning\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c65ceed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING FINAL EVALUATION (AFTER FINE-TUNING)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Metrics:\n",
      "{'eval_loss': 0.54839688539505, 'eval_runtime': 5.5327, 'eval_samples_per_second': 135.015, 'eval_steps_per_second': 8.495, 'epoch': 0.010526315789473684}\n",
      "\n",
      "============================================================\n",
      "COMPARISON: BASELINE vs FINE-TUNED\n",
      "============================================================\n",
      "Metric                    Baseline     Fine-Tuned   Change      \n",
      "-----------------------------------------------------------------\n",
      "eval_loss                 0.6140       0.5484       -0.0656      📉\n",
      "eval_accuracy             N/A          N/A          N/A         \n",
      "eval_precision            N/A          N/A          N/A         \n",
      "eval_recall               N/A          N/A          N/A         \n",
      "eval_f1                   N/A          N/A          N/A         \n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run Final Evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING FINAL EVALUATION (AFTER FINE-TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the Trainer's standard evaluate() method\n",
    "final_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Evaluation Metrics:\")\n",
    "print(final_metrics) # This will also have all metrics\n",
    "\n",
    "# --- Keep the comparison logic ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: BASELINE vs FINE-TUNED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_keys = ['eval_loss', 'eval_accuracy', 'eval_precision', 'eval_recall', 'eval_f1']\n",
    "print(f\"{'Metric':<25} {'Baseline':<12} {'Fine-Tuned':<12} {'Change':<12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for key in comparison_keys:\n",
    "    if key in baseline_metrics and key in final_metrics:\n",
    "        baseline_val = baseline_metrics.get(key)\n",
    "        final_val = final_metrics.get(key)\n",
    "        \n",
    "        if baseline_val is not None and final_val is not None:\n",
    "            change = final_val - baseline_val\n",
    "            change_str = f\"{change:+.4f}\"\n",
    "            \n",
    "            if key != 'eval_loss':  # Higher is better\n",
    "                indicator = \"📈\" if change > 0 else \"📉\" if change < 0 else \"➡️\"\n",
    "            else:  # Lower is better for loss\n",
    "                indicator = \"📉\" if change < 0 else \"📈\" if change > 0 else \"➡️\"\n",
    "            \n",
    "            print(f\"{key:<25} {baseline_val:<12.4f} {final_val:<12.4f} {change_str:<12} {indicator}\")\n",
    "        else:\n",
    "            print(f\"{key:<25} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n",
    "    else:\n",
    "        print(f\"{key:<25} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10a: Run Final Evaluation ( Uses manual_evaluate function )\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"RUNNING FINAL EVALUATION (AFTER FINE-TUNING)\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Use manual evaluation for more reliable results\n",
    "# final_metrics = manual_evaluate(\n",
    "#     model=model_to_tune,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     processor=processor,\n",
    "#     batch_size=CONFIG['BATCH_SIZE'],\n",
    "#     device=device,\n",
    "#     loss_type=CONFIG['LOSS_TYPE']\n",
    "# )\n",
    "\n",
    "# print(\"\\nFinal Evaluation Metrics:\")\n",
    "# for key, value in final_metrics.items():\n",
    "#     if isinstance(value, float):\n",
    "#         print(f\"  {key}: {value:.4f}\")\n",
    "#     else:\n",
    "#         print(f\"  {key}: {value}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"COMPARISON: BASELINE vs FINE-TUNED\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# comparison_keys = ['eval_loss', 'eval_accuracy', 'eval_precision', 'eval_recall', 'eval_f1']\n",
    "# print(f\"{'Metric':<25} {'Baseline':<12} {'Fine-Tuned':<12} {'Change':<12}\")\n",
    "# print(\"-\" * 65)\n",
    "\n",
    "# for key in comparison_keys:\n",
    "#     if key in baseline_metrics and key in final_metrics:\n",
    "#         baseline_val = baseline_metrics.get(key)\n",
    "#         final_val = final_metrics.get(key)\n",
    "        \n",
    "#         if baseline_val is not None and final_val is not None:\n",
    "#             change = final_val - baseline_val\n",
    "#             change_str = f\"{change:+.4f}\"\n",
    "            \n",
    "#             # Add emoji indicators\n",
    "#             if key != 'eval_loss':  # Higher is better\n",
    "#                 indicator = \"📈\" if change > 0 else \"📉\" if change < 0 else \"➡️\"\n",
    "#             else:  # Lower is better for loss\n",
    "#                 indicator = \"📉\" if change < 0 else \"📈\" if change > 0 else \"➡️\"\n",
    "            \n",
    "#             print(f\"{key:<25} {baseline_val:<12.4f} {final_val:<12.4f} {change_str:<12} {indicator}\")\n",
    "#         else:\n",
    "#             print(f\"{key:<25} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n",
    "#     else:\n",
    "#         print(f\"{key:<25} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10f86e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING MODEL AND GENERATING VISUALIZATIONS\n",
      "============================================================\n",
      "\n",
      "LoRA adapter saved to: ./siglip-scin-lora/final-adapter\n",
      "\n",
      "Generating final gradient heatmaps...\n",
      "Heatmap saved to: ./siglip-scin-lora/gradient_impact_heatmap.png\n",
      "\n",
      "Model and visualizations saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save Model and Generate Heatmap\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING MODEL AND GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save LoRA adapter\n",
    "final_adapter_path = os.path.join(CONFIG['OUTPUT_DIR'], \"final-adapter\")\n",
    "model_to_tune.save_pretrained(final_adapter_path)\n",
    "processor.save_pretrained(final_adapter_path)\n",
    "print(f\"\\nLoRA adapter saved to: {final_adapter_path}\")\n",
    "\n",
    "# Generate and save heatmap\n",
    "heatmap_path = os.path.join(CONFIG['OUTPUT_DIR'], \"gradient_impact_heatmap.png\")\n",
    "trainer.plot_final_heatmap(save_path=heatmap_path)\n",
    "\n",
    "print(\"\\nModel and visualizations saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4186dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualitative analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Qualitative Analysis Functions\n",
    "\n",
    "def get_similarity_scores(model, processor, image, text_probes, device):\n",
    "    \"\"\"\n",
    "    Get model similarity scores for an image and text probes.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        processor: Processor for inputs\n",
    "        image: PIL Image\n",
    "        text_probes: List of text strings\n",
    "        device: Device string\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of similarity scores\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=text_probes,\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        scores = torch.sigmoid(outputs.logits_per_image).cpu().numpy().flatten()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def plot_similarity_scores(base_scores, tuned_scores, probes, true_category, save_path):\n",
    "    \"\"\"\n",
    "    Generate bar chart comparing baseline and fine-tuned similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        base_scores: Baseline model scores\n",
    "        tuned_scores: Fine-tuned model scores\n",
    "        probes: List of text probes\n",
    "        true_category: The correct category\n",
    "        save_path: Path to save plot\n",
    "    \"\"\"\n",
    "    df_data = {\n",
    "        \"Text Probe\": probes * 2,\n",
    "        \"Similarity Score\": np.concatenate([base_scores, tuned_scores]),\n",
    "        \"Model\": [\"Baseline\"] * len(probes) + [\"Fine-Tuned\"] * len(probes)\n",
    "    }\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"Text Probe\",\n",
    "        y=\"Similarity Score\",\n",
    "        hue=\"Model\",\n",
    "        palette={\"Baseline\": \"lightblue\", \"Fine-Tuned\": \"darkblue\"}\n",
    "    )\n",
    "\n",
    "    # Highlight correct category\n",
    "    ax = plt.gca()\n",
    "    for i, probe in enumerate(probes):\n",
    "        if probe == true_category:\n",
    "            ax.get_xticklabels()[i].set_color(\"red\")\n",
    "            ax.get_xticklabels()[i].set_fontweight(\"bold\")\n",
    "\n",
    "    plt.title(f\"Qualitative Similarity Test (True Category: {true_category})\", fontsize=16)\n",
    "    plt.ylabel(\"Similarity Score (Sigmoid)\", fontsize=12)\n",
    "    plt.xlabel(\"Text Probes\", fontsize=12)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.legend(title=\"Model\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity plot saved to: {save_path}\")\n",
    "\n",
    "\n",
    "def run_qualitative_test(base_model, tuned_model, processor, device, output_dir, \n",
    "                         custom_image=None, custom_category=None, custom_probes=None):\n",
    "    \"\"\"\n",
    "    Run qualitative similarity test on a sample image.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Baseline model\n",
    "        tuned_model: Fine-tuned model\n",
    "        processor: Processor\n",
    "        device: Device string\n",
    "        output_dir: Directory to save results\n",
    "        custom_image: Optional PIL Image to test (if None, uses dataset sample)\n",
    "        custom_category: Optional category label\n",
    "        custom_probes: Optional list of text probes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING QUALITATIVE SIMILARITY TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Default test probes\n",
    "    if custom_probes is None:\n",
    "        TEXT_PROBES = [\n",
    "            \"benign keratosis\",\n",
    "            \"melanoma\", \n",
    "            \"nevus\",\n",
    "            \"eczema\",\n",
    "            \"basal cell carcinoma\"\n",
    "        ]\n",
    "    else:\n",
    "        TEXT_PROBES = custom_probes\n",
    "    \n",
    "    try:\n",
    "        # Get test image\n",
    "        if custom_image is not None:\n",
    "            image = custom_image.convert(\"RGB\") if custom_image.mode != \"RGB\" else custom_image\n",
    "            SAMPLE_IMG_CATEGORY = custom_category if custom_category else TEXT_PROBES[0]\n",
    "            print(f\"Using custom image\")\n",
    "        else:\n",
    "            # Use a sample from the validation dataset\n",
    "            print(\"Using sample from validation dataset...\")\n",
    "            sample_idx = 0\n",
    "            sample = val_data[sample_idx]\n",
    "            image = sample['image']\n",
    "            SAMPLE_IMG_CATEGORY = sample['text']\n",
    "            print(f\"Using validation sample {sample_idx}\")\n",
    "        \n",
    "        print(f\"True category: {SAMPLE_IMG_CATEGORY}\")\n",
    "        print(f\"Text probes: {TEXT_PROBES}\")\n",
    "        \n",
    "        # Get scores from both models\n",
    "        print(\"\\nComputing similarity scores...\")\n",
    "        base_scores = get_similarity_scores(base_model, processor, image, TEXT_PROBES, device)\n",
    "        tuned_scores = get_similarity_scores(tuned_model, processor, image, TEXT_PROBES, device)\n",
    "        \n",
    "        # Print scores\n",
    "        print(\"\\nSimilarity Scores:\")\n",
    "        print(f\"{'Probe':<25} {'Baseline':<12} {'Fine-Tuned':<12} {'Change':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, probe in enumerate(TEXT_PROBES):\n",
    "            change = tuned_scores[i] - base_scores[i]\n",
    "            marker = \" *\" if probe == SAMPLE_IMG_CATEGORY else \"\"\n",
    "            print(f\"{probe:<25} {base_scores[i]:<12.4f} {tuned_scores[i]:<12.4f} {change:+.4f}{marker}\")\n",
    "        \n",
    "        # Plot and save\n",
    "        sim_report_path = os.path.join(output_dir, \"similarity_report.png\")\n",
    "        plot_similarity_scores(base_scores, tuned_scores, TEXT_PROBES, SAMPLE_IMG_CATEGORY, sim_report_path)\n",
    "        \n",
    "        print(\"\\nQualitative test complete!\")\n",
    "        return sim_report_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during qualitative test: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Qualitative analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc57adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING QUALITATIVE SIMILARITY TEST\n",
      "============================================================\n",
      "Using sample from validation dataset...\n",
      "Using validation sample 0\n",
      "True category: RASH\n",
      "Text probes: ['benign keratosis', 'melanoma', 'nevus', 'eczema', 'basal cell carcinoma']\n",
      "\n",
      "Computing similarity scores...\n",
      "\n",
      "Similarity Scores:\n",
      "Probe                     Baseline     Fine-Tuned   Change      \n",
      "------------------------------------------------------------\n",
      "benign keratosis          0.0021       0.0021       +0.0000\n",
      "melanoma                  0.0000       0.0000       +0.0000\n",
      "nevus                     0.0013       0.0013       +0.0000\n",
      "eczema                    0.0482       0.0482       +0.0000\n",
      "basal cell carcinoma      0.0000       0.0000       +0.0000\n",
      "Similarity plot saved to: ./siglip-scin-lora/similarity_report.png\n",
      "\n",
      "Qualitative test complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Run Qualitative Test\n",
    "\n",
    "similarity_plot_path = run_qualitative_test(\n",
    "    base_model=base_model,\n",
    "    tuned_model=model_to_tune.to(device),\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    output_dir=CONFIG['OUTPUT_DIR'],\n",
    "    custom_image=None,  # Uses val_data[0]\n",
    "    custom_category=None,\n",
    "    custom_probes=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e0c01f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING FINAL REPORT\n",
      "============================================================\n",
      "Report saved to: ./siglip-scin-lora/final_report.md\n",
      "\n",
      "============================================================\n",
      "ALL TASKS COMPLETE!\n",
      "============================================================\n",
      "Results saved to: ./siglip-scin-lora\n",
      "Files generated:\n",
      "  - final-adapter/ (LoRA weights)\n",
      "  - gradient_impact_heatmap.png\n",
      "  - similarity_report.png\n",
      "  - final_report.md\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Generate Final Report\n",
    "\n",
    "def generate_final_report(baseline_metrics, final_metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Generate a markdown report with all results.\n",
    "    \n",
    "    Args:\n",
    "        baseline_metrics: Dictionary of baseline metrics\n",
    "        final_metrics: Dictionary of final metrics\n",
    "        output_dir: Directory to save report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report_path = os.path.join(output_dir, \"final_report.md\")\n",
    "    \n",
    "    report_content = \"# Fine-Tuning Experiment Report\\n\\n\"\n",
    "    report_content += f\"**Model:** {CONFIG['MODEL_ID']}\\n\"\n",
    "    report_content += f\"**Loss Type:** {CONFIG['LOSS_TYPE']}\\n\"\n",
    "    report_content += f\"**LoRA Rank:** {CONFIG['LORA_RANK']}\\n\"\n",
    "    report_content += f\"**LoRA Alpha:** {CONFIG['LORA_ALPHA']}\\n\"\n",
    "    report_content += f\"**Training Steps:** {CONFIG['MAX_STEPS']}\\n\"\n",
    "    report_content += f\"**Learning Rate:** {CONFIG['LEARNING_RATE']}\\n\\n\"\n",
    "    \n",
    "    # Quantitative metrics\n",
    "    report_content += \"## 1. Quantitative Metrics\\n\\n\"\n",
    "    report_content += \"Comparison of model performance on the validation set before and after fine-tuning.\\n\\n\"\n",
    "    report_content += \"| Metric | Baseline (Before) | Fine-Tuned (After) | Change |\\n\"\n",
    "    report_content += \"| :--- | :--- | :--- | :--- |\\n\"\n",
    "    \n",
    "    def get_metric(metrics, key, precision=4):\n",
    "        val = metrics.get(key)\n",
    "        if val is None:\n",
    "            return \"N/A\"\n",
    "        return f\"{val:.{precision}f}\"\n",
    "    \n",
    "    def get_change(baseline, final, key, precision=4):\n",
    "        b = baseline.get(key)\n",
    "        f = final.get(key)\n",
    "        if b is None or f is None:\n",
    "            return \"N/A\"\n",
    "        change = f - b\n",
    "        sign = \"+\" if change >= 0 else \"\"\n",
    "        return f\"{sign}{change:.{precision}f}\"\n",
    "    \n",
    "    metric_keys = [\n",
    "        (\"eval_loss\", \"Eval Loss\"),\n",
    "        (\"eval_accuracy\", \"Accuracy\"),\n",
    "        (\"eval_precision\", \"Precision (Macro)\"),\n",
    "        (\"eval_recall\", \"Recall (Macro)\"),\n",
    "        (\"eval_f1\", \"F1-Score (Macro)\"),\n",
    "        (\"eval_runtime\", \"Eval Runtime (s)\"),\n",
    "    ]\n",
    "    \n",
    "    for key, name in metric_keys:\n",
    "        b_val = get_metric(baseline_metrics, key)\n",
    "        f_val = get_metric(final_metrics, key)\n",
    "        c_val = get_change(baseline_metrics, final_metrics, key)\n",
    "        report_content += f\"| **{name}** | {b_val} | {f_val} | {c_val} |\\n\"\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    report_content += \"\\n## 2. Qualitative Analysis (Similarity Test)\\n\\n\"\n",
    "    report_content += \"This test shows how the model's understanding of specific concepts changed.\\n\\n\"\n",
    "    report_content += \"![Similarity Plot](similarity_report.png)\\n\\n\"\n",
    "    report_content += \"**Interpretation:** The fine-tuned model should show higher similarity scores for the correct category.\\n\"\n",
    "    \n",
    "    # Gradient heatmap\n",
    "    report_content += \"\\n## 3. Gradient Impact Heatmap\\n\\n\"\n",
    "    report_content += \"This heatmap shows which parts of the model were modified most during fine-tuning.\\n\\n\"\n",
    "    report_content += \"![Gradient Impact Heatmap](gradient_impact_heatmap.png)\\n\\n\"\n",
    "    report_content += \"**Interpretation:** Brighter colors indicate layers heavily modified by fine-tuning.\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    try:\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "        print(f\"Report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving report: {e}\")\n",
    "\n",
    "\n",
    "# Generate the report\n",
    "generate_final_report(\n",
    "    baseline_metrics=baseline_metrics,\n",
    "    final_metrics=final_metrics,\n",
    "    output_dir=CONFIG['OUTPUT_DIR']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TASKS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Results saved to: {CONFIG['OUTPUT_DIR']}\")\n",
    "print(\"Files generated:\")\n",
    "print(\"  - final-adapter/ (LoRA weights)\")\n",
    "print(\"  - gradient_impact_heatmap.png\")\n",
    "print(\"  - similarity_report.png\")\n",
    "print(\"  - final_report.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CELL: Utility Functions for Debugging and Analysis\n",
    "\n",
    "def inspect_dataset_sample(dataset, n=3):\n",
    "    \"\"\"\n",
    "    Inspect first n samples from dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset to inspect\n",
    "        n: Number of samples to show\n",
    "    \"\"\"\n",
    "    print(f\"Inspecting first {n} samples from dataset:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(n, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(f\"  Text: {sample['text']}\")\n",
    "        print(f\"  Image: {sample['image'].size}, mode={sample['image'].mode}\")\n",
    "        \n",
    "        # Display image inline (if in Jupyter)\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(sample['image'])\n",
    "        except:\n",
    "            print(\"  (Image display not available)\")\n",
    "\n",
    "\n",
    "def test_single_batch(trainer, dataset, n_samples=4):\n",
    "    \"\"\"\n",
    "    Test processing a single batch through the model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance\n",
    "        dataset: Dataset to sample from\n",
    "        n_samples: Batch size to test\n",
    "    \"\"\"\n",
    "    print(f\"Testing single batch with {n_samples} samples...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get a small batch\n",
    "    batch = [dataset[i] for i in range(min(n_samples, len(dataset)))]\n",
    "    \n",
    "    # Process through collate_fn\n",
    "    inputs = trainer.data_collator(batch)\n",
    "    \n",
    "    if not inputs:\n",
    "        print(\"ERROR: Batch processing failed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Batch processed successfully!\")\n",
    "    print(f\"  pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "    print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    try:\n",
    "        trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = trainer.model(**inputs)\n",
    "            print(f\"  logits_per_image shape: {outputs.logits_per_image.shape}\")\n",
    "            print(f\"  logits_per_text shape: {outputs.logits_per_text.shape}\")\n",
    "        print(\"\\nForward pass successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in forward pass: {e}\")\n",
    "\n",
    "\n",
    "def compare_model_predictions(base_model, tuned_model, processor, image, text, device):\n",
    "    \"\"\"\n",
    "    Compare predictions from base and tuned models on a single example.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Baseline model\n",
    "        tuned_model: Fine-tuned model\n",
    "        processor: Processor\n",
    "        image: PIL Image\n",
    "        text: Text string\n",
    "        device: Device string\n",
    "    \"\"\"\n",
    "    print(\"Comparing model predictions...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "    \n",
    "    # Base model\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model(**inputs)\n",
    "        base_score = torch.sigmoid(base_outputs.logits_per_image).item()\n",
    "    \n",
    "    # Tuned model\n",
    "    tuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tuned_outputs = tuned_model(**inputs)\n",
    "        tuned_score = torch.sigmoid(tuned_outputs.logits_per_image).item()\n",
    "    \n",
    "    print(f\"Baseline similarity: {base_score:.4f}\")\n",
    "    print(f\"Fine-tuned similarity: {tuned_score:.4f}\")\n",
    "    print(f\"Change: {tuned_score - base_score:+.4f}\")\n",
    "\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    Plot training loss over time.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance with logged history\n",
    "    \"\"\"\n",
    "    history = trainer.state.log_history\n",
    "    \n",
    "    # Extract loss values\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry:\n",
    "            train_losses.append(entry['loss'])\n",
    "            steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            eval_losses.append(entry['eval_loss'])\n",
    "    \n",
    "    if not train_losses:\n",
    "        print(\"No training history to plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, train_losses, 'b-', label='Training Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Evaluation loss\n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = [entry['step'] for entry in history if 'eval_loss' in entry]\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', label='Eval Loss')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Evaluation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'training_history.png'))\n",
    "    plt.show()\n",
    "    print(f\"Training history plot saved\")\n",
    "\n",
    "\n",
    "def load_saved_adapter(adapter_path, base_model_id, device):\n",
    "    \"\"\"\n",
    "    Load a saved LoRA adapter.\n",
    "    \n",
    "    Args:\n",
    "        adapter_path: Path to saved adapter\n",
    "        base_model_id: Base model ID\n",
    "        device: Device string\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model with adapter\n",
    "    \"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(f\"Loading adapter from {adapter_path}...\")\n",
    "    \n",
    "    base_model = AutoModel.from_pretrained(base_model_id).to(device)\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    print(\"Adapter loaded successfully\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Utility functions defined!\")\n",
    "print(\"\\nAvailable utilities:\")\n",
    "print(\"  - inspect_dataset_sample(dataset, n=3)\")\n",
    "print(\"  - test_single_batch(trainer, dataset, n_samples=4)\")\n",
    "print(\"  - compare_model_predictions(base_model, tuned_model, processor, image, text, device)\")\n",
    "print(\"  - plot_training_history(trainer)\")\n",
    "print(\"  - load_saved_adapter(adapter_path, base_model_id, device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf8b2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation debug...\n",
      "============================================================\n",
      "DEBUGGING EVALUATION PROCESS\n",
      "============================================================\n",
      "\n",
      "Eval dataset size: 747\n",
      "Eval batch size: 16\n",
      "Eval dataloader batches: 47\n",
      "\n",
      "--- Batch 0 ---\n",
      "  pixel_values shape: torch.Size([16, 3, 224, 224])\n",
      "  input_ids shape: torch.Size([16, 64])\n",
      "  ✓ Loss: 0.6532\n",
      "  ✓ Predictions shape: torch.Size([16, 16])\n",
      "  Labels is None (expected for this task)\n",
      "\n",
      "--- Batch 1 ---\n",
      "  pixel_values shape: torch.Size([16, 3, 224, 224])\n",
      "  input_ids shape: torch.Size([16, 64])\n",
      "  ✓ Loss: 0.6200\n",
      "  ✓ Predictions shape: torch.Size([16, 16])\n",
      "  Labels is None (expected for this task)\n",
      "\n",
      "--- Batch 2 ---\n",
      "  pixel_values shape: torch.Size([16, 3, 224, 224])\n",
      "  input_ids shape: torch.Size([16, 64])\n",
      "  ✓ Loss: 0.6437\n",
      "  ✓ Predictions shape: torch.Size([16, 16])\n",
      "  Labels is None (expected for this task)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Successful batches: 3\n",
      "Skipped batches: 0\n",
      "\n",
      "Total predictions collected: torch.Size([48, 16])\n",
      "Predictions range: [-17.2793, 2.1042]\n",
      "\n",
      "--- Testing compute_metrics ---\n",
      "\n",
      "[compute_metrics] Called with predictions type: <class 'numpy.ndarray'>\n",
      "[compute_metrics] Predictions shape: (48, 16)\n",
      "[compute_metrics] Predictions range: [-17.2793, 2.1042]\n",
      "[compute_metrics] Predicted labels (first 10): [0 0 0 0 0 0 0 0 0 0]\n",
      "[compute_metrics] True labels (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "[compute_metrics] Computed: acc=0.0208, prec=0.0007, rec=0.0208, f1=0.0013\n",
      "✓ compute_metrics succeeded:\n",
      "  accuracy: 0.0208\n",
      "  precision: 0.0007\n",
      "  recall: 0.0208\n",
      "  f1: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1586283/1303307807.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_preds = torch.cat([torch.tensor(p) for p in all_predictions], dim=0)\n"
     ]
    }
   ],
   "source": [
    "# DEBUG CELL: Check what's happening during evaluation\n",
    "\n",
    "def debug_evaluation(trainer, num_batches=2):\n",
    "    \"\"\"\n",
    "    Debug the evaluation process to see what's being returned.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance\n",
    "        num_batches: Number of batches to test\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DEBUGGING EVALUATION PROCESS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get a few samples\n",
    "    eval_dataloader = trainer.get_eval_dataloader()\n",
    "    \n",
    "    print(f\"\\nEval dataset size: {len(trainer.eval_dataset)}\")\n",
    "    print(f\"Eval batch size: {trainer.args.per_device_eval_batch_size}\")\n",
    "    print(f\"Eval dataloader batches: {len(eval_dataloader)}\")\n",
    "    \n",
    "    # Test a few batches\n",
    "    trainer.model.eval()\n",
    "    all_predictions = []\n",
    "    all_losses = []\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(eval_dataloader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n--- Batch {batch_idx} ---\")\n",
    "        \n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            print(\"  ⚠️ Empty batch (skipped by collate_fn)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "        print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run prediction_step\n",
    "        loss, predictions, labels = trainer.prediction_step(\n",
    "            trainer.model, \n",
    "            inputs, \n",
    "            prediction_loss_only=False\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            print(f\"  ✓ Loss: {loss.item():.4f}\")\n",
    "            all_losses.append(loss.item())\n",
    "        else:\n",
    "            print(f\"  ⚠️ Loss is None\")\n",
    "        \n",
    "        if predictions is not None:\n",
    "            print(f\"  ✓ Predictions shape: {predictions.shape}\")\n",
    "            all_predictions.append(predictions)\n",
    "        else:\n",
    "            print(f\"  ⚠️ Predictions is None\")\n",
    "        \n",
    "        if labels is not None:\n",
    "            print(f\"  ✓ Labels shape: {labels.shape}\")\n",
    "        else:\n",
    "            print(f\"  Labels is None (expected for this task)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Successful batches: {len(all_losses)}\")\n",
    "    print(f\"Skipped batches: {trainer.skipped_batches_eval}\")\n",
    "    \n",
    "    if all_predictions:\n",
    "        all_preds = torch.cat([torch.tensor(p) for p in all_predictions], dim=0)\n",
    "        print(f\"\\nTotal predictions collected: {all_preds.shape}\")\n",
    "        print(f\"Predictions range: [{all_preds.min():.4f}, {all_preds.max():.4f}]\")\n",
    "        \n",
    "        # Test compute_metrics manually\n",
    "        print(\"\\n--- Testing compute_metrics ---\")\n",
    "        from collections import namedtuple\n",
    "        EvalPrediction = namedtuple('EvalPrediction', ['predictions', 'label_ids'])\n",
    "        \n",
    "        # Create fake eval prediction\n",
    "        eval_pred = EvalPrediction(\n",
    "            predictions=all_preds.numpy(),\n",
    "            label_ids=None\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            metrics = compute_metrics(eval_pred)\n",
    "            print(\"✓ compute_metrics succeeded:\")\n",
    "            for k, v in metrics.items():\n",
    "                print(f\"  {k}: {v:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ compute_metrics failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"\\n⚠️ No predictions collected!\")\n",
    "        print(\"This means all batches were skipped or failed.\")\n",
    "\n",
    "\n",
    "# Run the debug\n",
    "print(\"Running evaluation debug...\")\n",
    "debug_evaluation(trainer, num_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49dc57e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINER CONFIGURATION DIAGNOSIS\n",
      "============================================================\n",
      "\n",
      "1. compute_metrics function:\n",
      "   ✓ compute_metrics is set: <function compute_metrics at 0x7a9ec85ea660>\n",
      "\n",
      "2. Training Arguments:\n",
      "   prediction_loss_only: False\n",
      "   ✓ prediction_loss_only is False\n",
      "   eval_strategy: IntervalStrategy.NO\n",
      "   eval_steps: 1\n",
      "\n",
      "3. Datasets:\n",
      "   ✓ Train dataset: 3032 samples\n",
      "   ✓ Eval dataset: 747 samples\n",
      "\n",
      "4. Model:\n",
      "   Device: cuda:0\n",
      "   Training mode: False\n",
      "\n",
      "5. Testing single batch evaluation:\n",
      "   ✓ Batch loaded successfully\n",
      "     pixel_values shape: torch.Size([16, 3, 224, 224])\n",
      "   ✓ Loss computed: 0.6532\n",
      "   ✓ Predictions shape: torch.Size([16, 16])\n",
      "   ✓ compute_metrics executed successfully:\n",
      "     accuracy: 0.0625\n",
      "     precision: 0.0039\n",
      "     recall: 0.0625\n",
      "     f1: 0.0074\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSE CELL: Check if trainer is properly configured\n",
    "\n",
    "def diagnose_trainer(trainer):\n",
    "    \"\"\"\n",
    "    Check trainer configuration for common issues.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINER CONFIGURATION DIAGNOSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check compute_metrics\n",
    "    print(\"\\n1. compute_metrics function:\")\n",
    "    if trainer.compute_metrics is None:\n",
    "        print(\"   ❌ compute_metrics is None - metrics won't be computed!\")\n",
    "        print(\"   FIX: Set compute_metrics when creating trainer\")\n",
    "    else:\n",
    "        print(f\"   ✓ compute_metrics is set: {trainer.compute_metrics}\")\n",
    "    \n",
    "    # Check training args\n",
    "    print(\"\\n2. Training Arguments:\")\n",
    "    print(f\"   prediction_loss_only: {trainer.args.prediction_loss_only}\")\n",
    "    if trainer.args.prediction_loss_only:\n",
    "        print(\"   ⚠️ prediction_loss_only is True - compute_metrics will be ignored!\")\n",
    "        print(\"   FIX: Set prediction_loss_only=False in TrainingArguments\")\n",
    "    else:\n",
    "        print(\"   ✓ prediction_loss_only is False\")\n",
    "    \n",
    "    print(f\"   eval_strategy: {trainer.args.eval_strategy}\")\n",
    "    print(f\"   eval_steps: {trainer.args.eval_steps}\")\n",
    "    \n",
    "    # Check datasets\n",
    "    print(\"\\n3. Datasets:\")\n",
    "    if trainer.train_dataset:\n",
    "        print(f\"   ✓ Train dataset: {len(trainer.train_dataset)} samples\")\n",
    "    else:\n",
    "        print(\"   ❌ No train dataset\")\n",
    "    \n",
    "    if trainer.eval_dataset:\n",
    "        print(f\"   ✓ Eval dataset: {len(trainer.eval_dataset)} samples\")\n",
    "    else:\n",
    "        print(\"   ❌ No eval dataset\")\n",
    "    \n",
    "    # Check model\n",
    "    print(\"\\n4. Model:\")\n",
    "    print(f\"   Device: {next(trainer.model.parameters()).device}\")\n",
    "    print(f\"   Training mode: {trainer.model.training}\")\n",
    "    \n",
    "    # Test a single batch\n",
    "    print(\"\\n5. Testing single batch evaluation:\")\n",
    "    try:\n",
    "        eval_dataloader = trainer.get_eval_dataloader()\n",
    "        first_batch = next(iter(eval_dataloader))\n",
    "        \n",
    "        if not first_batch or \"pixel_values\" not in first_batch:\n",
    "            print(\"   ❌ First batch is empty or invalid!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   ✓ Batch loaded successfully\")\n",
    "        print(f\"     pixel_values shape: {first_batch['pixel_values'].shape}\")\n",
    "        \n",
    "        # Move to device\n",
    "        first_batch = {k: v.to(device) for k, v in first_batch.items()}\n",
    "        \n",
    "        # Test prediction_step\n",
    "        trainer.model.eval()\n",
    "        loss, predictions, labels = trainer.prediction_step(\n",
    "            trainer.model,\n",
    "            first_batch,\n",
    "            prediction_loss_only=False\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            print(f\"   ✓ Loss computed: {loss.item():.4f}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Loss is None\")\n",
    "        \n",
    "        if predictions is not None:\n",
    "            print(f\"   ✓ Predictions shape: {predictions.shape}\")\n",
    "            \n",
    "            # Test compute_metrics with this batch\n",
    "            if trainer.compute_metrics:\n",
    "                from collections import namedtuple\n",
    "                EvalPrediction = namedtuple('EvalPrediction', ['predictions', 'label_ids'])\n",
    "                eval_pred = EvalPrediction(\n",
    "                    predictions=predictions.numpy() if hasattr(predictions, 'numpy') else predictions,\n",
    "                    label_ids=None\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    metrics = trainer.compute_metrics(eval_pred)\n",
    "                    print(f\"   ✓ compute_metrics executed successfully:\")\n",
    "                    for k, v in metrics.items():\n",
    "                        print(f\"     {k}: {v:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ compute_metrics failed: {e}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Predictions is None\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error during batch test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_trainer(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
