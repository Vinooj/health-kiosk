{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa946627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoo/projects/health-kiosk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration: {'MODEL_ID': 'google/siglip-base-patch16-224', 'OUTPUT_DIR': './siglip-scin-lora', 'DATA_DIR': './data/scin_cache', 'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'LORA_RANK': 16, 'LORA_ALPHA': 16, 'MAX_STEPS': 500, 'LOSS_TYPE': 'sigmoid', 'N_VAL_SAMPLES': 1000, 'N_TRAIN_SAMPLES': 5000}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'MODEL_ID': \"google/siglip-base-patch16-224\",\n",
    "    'OUTPUT_DIR': \"./siglip-scin-lora\",\n",
    "    'DATA_DIR': \"./data/scin_cache\",  # Local cache directory\n",
    "    'BATCH_SIZE': 16,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'LORA_RANK': 16,\n",
    "    'LORA_ALPHA': 16,\n",
    "    'MAX_STEPS': 500,\n",
    "    'LOSS_TYPE': \"sigmoid\",  # or \"contrastive\"\n",
    "    'N_VAL_SAMPLES': 1000,\n",
    "    'N_TRAIN_SAMPLES': 5000,  # Set to None to use all available\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "os.makedirs(CONFIG['DATA_DIR'], exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0dc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some datasets params were ignored: ['splits', 'download_size', 'dataset_size']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading val samples: 100%|██████████| 1000/1000 [01:06<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training samples (max: 5000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train samples:  81%|████████  | 4033/5000 [04:32<01:05, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 3032 training samples to cache...\n",
      "Saving 747 validation samples to cache...\n",
      "Dataset cached to ./data/scin_cache\n",
      "\n",
      "Dataset loaded: 3032 train, 747 val samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading and Caching Functions\n",
    "\n",
    "def download_and_cache_dataset(n_train=5000, n_val=1000, force_redownload=False):\n",
    "    \"\"\"\n",
    "    Download dataset from HuggingFace and cache locally.\n",
    "    \n",
    "    Args:\n",
    "        n_train: Number of training samples (None for all)\n",
    "        n_val: Number of validation samples\n",
    "        force_redownload: Force redownload even if cache exists\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data_list, val_data_list)\n",
    "    \"\"\"\n",
    "    train_cache = Path(CONFIG['DATA_DIR']) / f\"train_{n_train}.pkl\"\n",
    "    val_cache = Path(CONFIG['DATA_DIR']) / f\"val_{n_val}.pkl\"\n",
    "    \n",
    "    # Check if cache exists\n",
    "    if not force_redownload and train_cache.exists() and val_cache.exists():\n",
    "        print(f\"Loading cached dataset from {CONFIG['DATA_DIR']}\")\n",
    "        with open(train_cache, 'rb') as f:\n",
    "            train_data = pickle.load(f)\n",
    "        with open(val_cache, 'rb') as f:\n",
    "            val_data = pickle.load(f)\n",
    "        print(f\"Loaded {len(train_data)} training samples and {len(val_data)} validation samples from cache\")\n",
    "        return train_data, val_data\n",
    "    \n",
    "    # Download from HuggingFace\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    try:\n",
    "        base_iterable = load_dataset(\"google/scin\", split=\"train\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset. Error: {e}\")\n",
    "        print(\"Please ensure you have an internet connection and have accepted 'google/scin' terms if any.\")\n",
    "        raise\n",
    "    \n",
    "    image_columns = [\"image_1_path\", \"image_2_path\", \"image_3_path\"]\n",
    "    \n",
    "    # Load validation data\n",
    "    print(f\"Loading {n_val} validation samples...\")\n",
    "    val_data = []\n",
    "    for item in tqdm(base_iterable.take(n_val), total=n_val, desc=\"Loading val samples\"):\n",
    "        text = item.get(\"related_category\")\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    val_data.append({\"image\": image.convert(\"RGB\"), \"text\": text})\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                    break\n",
    "    \n",
    "    # Load training data\n",
    "    train_iterable = base_iterable.skip(n_val)\n",
    "    print(f\"Loading training samples (max: {n_train if n_train else 'all'})...\")\n",
    "    train_data = []\n",
    "    \n",
    "    if n_train:\n",
    "        iterator = tqdm(train_iterable.take(n_train), total=n_train, desc=\"Loading train samples\")\n",
    "    else:\n",
    "        iterator = tqdm(train_iterable, desc=\"Loading train samples\")\n",
    "    \n",
    "    for item in iterator:\n",
    "        text = item.get(\"related_category\")\n",
    "        if not text or not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        for img_col in image_columns:\n",
    "            image = item.get(img_col)\n",
    "            if image and isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    train_data.append({\"image\": image.convert(\"RGB\"), \"text\": text})\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting image, skipping: {e}\")\n",
    "                    break\n",
    "        \n",
    "        if n_train and len(train_data) >= n_train:\n",
    "            break\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving {len(train_data)} training samples to cache...\")\n",
    "    with open(train_cache, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    \n",
    "    print(f\"Saving {len(val_data)} validation samples to cache...\")\n",
    "    with open(val_cache, 'wb') as f:\n",
    "        pickle.dump(val_data, f)\n",
    "    \n",
    "    print(f\"Dataset cached to {CONFIG['DATA_DIR']}\")\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear all cached dataset files.\"\"\"\n",
    "    cache_dir = Path(CONFIG['DATA_DIR'])\n",
    "    if cache_dir.exists():\n",
    "        for file in cache_dir.glob(\"*.pkl\"):\n",
    "            file.unlink()\n",
    "            print(f\"Deleted {file}\")\n",
    "        print(\"Cache cleared\")\n",
    "    else:\n",
    "        print(\"No cache to clear\")\n",
    "\n",
    "\n",
    "# Load or download dataset\n",
    "train_data, val_data = download_and_cache_dataset(\n",
    "    n_train=CONFIG['N_TRAIN_SAMPLES'],\n",
    "    n_val=CONFIG['N_VAL_SAMPLES'],\n",
    "    force_redownload=False  # Set to True to force redownload\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(train_data)} train, {len(val_data)} val samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb328cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Classes\n",
    "\n",
    "class SCIN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for SCIN data.\n",
    "    Works with pre-loaded data lists (from cache or download).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        print(f\"Initializing SCIN_Dataset with {len(data_list)} samples.\")\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    \"\"\"\n",
    "    Data collator with robust error handling.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    processed_texts_input_ids = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            if item is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            img = item.get(\"image\")\n",
    "            txt = item.get(\"text\")\n",
    "\n",
    "            # Check for invalid content\n",
    "            if img is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            if txt is None or txt.strip() == \"\":\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Process valid items\n",
    "            inputs = processor(\n",
    "                text=[txt],\n",
    "                images=[img],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=64\n",
    "            )\n",
    "\n",
    "            processed_images.append(inputs[\"pixel_values\"])\n",
    "            processed_texts_input_ids.append(inputs[\"input_ids\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING (collate_fn): Skipping item {i} due to error: {e}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    if not processed_images:\n",
    "        if len(batch) > 0:\n",
    "            print(f\"ERROR: Entire batch was skipped! ({skipped_count} items failed)\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        batch_pixel_values = torch.cat(processed_images, dim=0)\n",
    "        batch_input_ids = torch.cat(processed_texts_input_ids, dim=0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": batch_pixel_values,\n",
    "            \"input_ids\": batch_input_ids\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final batch collation: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SCIN_Dataset(train_data)\n",
    "val_dataset = SCIN_Dataset(val_data)\n",
    "\n",
    "print(f\"Datasets created: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Loading\n",
    "\n",
    "def load_models_and_processor(model_id, device):\n",
    "    \"\"\"\n",
    "    Load processor and two model instances (base and tunable).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (processor, base_model, model_to_tune)\n",
    "    \"\"\"\n",
    "    print(f\"Loading processor and models from: {model_id}\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    \n",
    "    # Base model (for baseline evaluation)\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype\n",
    "    ).to(device)\n",
    "    \n",
    "    # Model to fine-tune\n",
    "    model_to_tune = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    \n",
    "    print(\"Models and processor loaded successfully\")\n",
    "    return processor, base_model, model_to_tune\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=16, alpha=16):\n",
    "    \"\"\"\n",
    "    Apply LoRA configuration to a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "    \n",
    "    Returns:\n",
    "        Model with LoRA applied\n",
    "    \"\"\"\n",
    "    print(f\"Applying LoRA configuration (rank={rank}, alpha={alpha})...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\"LoRA applied. Trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load models\n",
    "processor, base_model, model_to_tune = load_models_and_processor(\n",
    "    CONFIG['MODEL_ID'], \n",
    "    device\n",
    ")\n",
    "\n",
    "# Apply LoRA to the tunable model\n",
    "model_to_tune = apply_lora(\n",
    "    model_to_tune,\n",
    "    rank=CONFIG['LORA_RANK'],\n",
    "    alpha=CONFIG['LORA_ALPHA']\n",
    ")\n",
    "model_to_tune = model_to_tune.to(device)\n",
    "\n",
    "print(f\"\\nModels ready on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeebe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Metrics and Loss Functions\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.arange(len(predictions))\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_loss_function(logits_per_image, logits_per_text, loss_type=\"sigmoid\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute loss based on loss type.\n",
    "    \n",
    "    Args:\n",
    "        logits_per_image: Image logits\n",
    "        logits_per_text: Text logits\n",
    "        loss_type: \"contrastive\" or \"sigmoid\"\n",
    "        device: Device for computation\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    \n",
    "    if batch_size <= 1:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    if loss_type == \"contrastive\":\n",
    "        labels = torch.arange(batch_size, device=device)\n",
    "        loss_images = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_images + loss_text) / 2.0\n",
    "    elif loss_type == \"sigmoid\":\n",
    "        labels = torch.eye(batch_size, device=device)\n",
    "        loss_images = F.binary_cross_entropy_with_logits(logits_per_image, labels)\n",
    "        loss_text = F.binary_cross_entropy_with_logits(logits_per_text, labels)\n",
    "        loss = (loss_images + loss_text) / 2.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"Metrics and loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Custom Trainer Class\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with:\n",
    "    1. Switchable loss (Contrastive or Sigmoid)\n",
    "    2. Gradient accumulation for heatmap\n",
    "    3. Proper evaluation with loss and metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, loss_type=\"contrastive\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "        print(f\"CustomTrainer initialized with loss_type: {self.loss_type}\")\n",
    "        \n",
    "        # Gradient tracking for heatmap\n",
    "        self.gradient_accumulator = defaultdict(float)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Batch stats\n",
    "        self.successful_batches = 0\n",
    "        self.skipped_batches_eval = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Training loss computation.\"\"\"\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            dummy_loss = torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "            return (dummy_loss, {}) if return_outputs else dummy_loss\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        \n",
    "        loss = compute_loss_function(\n",
    "            logits_per_image,\n",
    "            logits_per_text,\n",
    "            loss_type=self.loss_type,\n",
    "            device=model.device\n",
    "        )\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        \"\"\"Training step with gradient tracking.\"\"\"\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "        if loss is not None:\n",
    "            self.step_count += 1\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and param.requires_grad:\n",
    "                        self.gradient_accumulator[name] += param.grad.norm().item()\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"Evaluation step with loss computation.\"\"\"\n",
    "        if not inputs or \"pixel_values\" not in inputs:\n",
    "            self.skipped_batches_eval += 1\n",
    "            return (None, None, None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            batch_size = logits_per_image.shape[0]\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "                print(\"WARNING: NaN or Inf detected in logits during eval.\")\n",
    "                self.skipped_batches_eval += 1\n",
    "                return (None, None, None)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = None\n",
    "            if batch_size <= 1:\n",
    "                self.skipped_batches_eval += 1\n",
    "            else:\n",
    "                loss = compute_loss_function(\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    loss_type=self.loss_type,\n",
    "                    device=model.device\n",
    "                )\n",
    "                self.successful_batches += 1\n",
    "        \n",
    "        predictions = logits_per_image.cpu()\n",
    "        return (loss, predictions, None)\n",
    "\n",
    "    def _extract_layer_index(self, name_parts):\n",
    "        \"\"\"Extract layer index from parameter name.\"\"\"\n",
    "        for part in name_parts:\n",
    "            if part.isdigit():\n",
    "                return int(part)\n",
    "        return None\n",
    "\n",
    "    def _extract_component_name(self, name_parts):\n",
    "        \"\"\"Extract component name from parameter name.\"\"\"\n",
    "        name_str = \".\".join(name_parts)\n",
    "        if \"lora_A\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA A (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA A (Value)\"\n",
    "        elif \"lora_B\" in name_str:\n",
    "            if \"q_proj\" in name_str: return \"LoRA B (Query)\"\n",
    "            if \"v_proj\" in name_str: return \"LoRA B (Value)\"\n",
    "        if \"q_proj\" in name_str: return \"Query Proj\"\n",
    "        if \"v_proj\" in name_str: return \"Value Proj\"\n",
    "        if \"k_proj\" in name_str: return \"Key Proj\"\n",
    "        if \"fc1\" in name_str: return \"MLP Layer 1\"\n",
    "        if \"fc2\" in name_str: return \"MLP Layer 2\"\n",
    "        return None\n",
    "\n",
    "    def _process_gradients_for_heatmap(self):\n",
    "        \"\"\"Process accumulated gradients for heatmap generation.\"\"\"\n",
    "        if self.step_count == 0:\n",
    "            print(\"No training steps recorded. Skipping heatmap.\")\n",
    "            return None, None, []\n",
    "\n",
    "        vision_data = defaultdict(lambda: defaultdict(float))\n",
    "        text_data = defaultdict(lambda: defaultdict(float))\n",
    "        skipped_params = []\n",
    "\n",
    "        for name, avg_grad_norm in self.gradient_accumulator.items():\n",
    "            avg_norm = avg_grad_norm / self.step_count\n",
    "            parts = name.split('.')\n",
    "            layer_idx = self._extract_layer_index(parts)\n",
    "            component = self._extract_component_name(parts)\n",
    "            \n",
    "            if layer_idx is None or component is None:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "                continue\n",
    "            \n",
    "            if \"vision_model\" in name:\n",
    "                vision_data[layer_idx][component] = avg_norm\n",
    "            elif \"text_model\" in name:\n",
    "                text_data[layer_idx][component] = avg_norm\n",
    "            else:\n",
    "                if \"lora_\" in name:\n",
    "                    skipped_params.append(name)\n",
    "\n",
    "        vision_df = pd.DataFrame.from_dict(vision_data, orient='index').sort_index()\n",
    "        text_df = pd.DataFrame.from_dict(text_data, orient='index').sort_index()\n",
    "        return vision_df, text_df, skipped_params\n",
    "\n",
    "    def plot_final_heatmap(self, save_path):\n",
    "        \"\"\"Generate and save gradient impact heatmap.\"\"\"\n",
    "        print(\"\\nGenerating final gradient heatmaps...\")\n",
    "        vision_df, text_df, skipped = self._process_gradients_for_heatmap()\n",
    "        \n",
    "        if vision_df is None or (vision_df.empty and text_df.empty):\n",
    "            print(\"No gradient data collected. Skipping heatmap file.\")\n",
    "            return\n",
    "        \n",
    "        if skipped:\n",
    "            print(f\"[WARN] Skipped {len(skipped)} LoRA params (couldn't parse name)\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "        vmin = 0.0\n",
    "        vmax = max(\n",
    "            vision_df.max().max() if not vision_df.empty else 0,\n",
    "            text_df.max().max() if not text_df.empty else 0\n",
    "        )\n",
    "        if vmax == 0:\n",
    "            vmax = 1.0\n",
    "\n",
    "        if not vision_df.empty:\n",
    "            sns.heatmap(vision_df, ax=ax1, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax1.set_title(\"Vision Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax1.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax1.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"No Vision Gradients Found\", ha='center', va='center')\n",
    "            ax1.set_title(\"Vision Encoder Impact\", fontsize=16)\n",
    "\n",
    "        if not text_df.empty:\n",
    "            sns.heatmap(text_df, ax=ax2, cmap=\"magma\", annot=True, fmt=\".2e\",\n",
    "                        linewidths=.5, vmin=vmin, vmax=vmax)\n",
    "            ax2.set_title(\"Text Encoder Impact (Avg. Gradient Norm)\", fontsize=16)\n",
    "            ax2.set_ylabel(\"Layer Depth\", fontsize=12)\n",
    "            ax2.set_xlabel(\"Transformer Component (LoRA)\", fontsize=12)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No Text Gradients Found\", ha='center', va='center')\n",
    "            ax2.set_title(\"Text Encoder Impact\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Heatmap saved to: {save_path}\")\n",
    "\n",
    "\n",
    "print(\"CustomTrainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77453198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Setup\n",
    "\n",
    "def create_training_args(config, device, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Create training arguments.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        device: Device string\n",
    "        debug_mode: If True, use minimal steps for quick testing\n",
    "    \n",
    "    Returns:\n",
    "        TrainingArguments\n",
    "    \"\"\"\n",
    "    use_fp16 = True if device == \"cuda\" else False\n",
    "    \n",
    "    if debug_mode:\n",
    "        print(\"Creating DEBUG training arguments (quick run)...\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=config['OUTPUT_DIR'],\n",
    "            per_device_train_batch_size=config['BATCH_SIZE'],\n",
    "            per_device_eval_batch_size=config['BATCH_SIZE'],\n",
    "            max_steps=2,\n",
    "            eval_steps=1,\n",
    "            logging_steps=1,\n",
    "            warmup_steps=1,\n",
    "            weight_decay=0.01,\n",
    "            learning_rate=config['LEARNING_RATE'],\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            fp16=use_fp16,\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "            prediction_loss_only=False,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating FULL training arguments...\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=config['OUTPUT_DIR'],\n",
    "            per_device_train_batch_size=config['BATCH_SIZE'],\n",
    "            per_device_eval_batch_size=config['BATCH_SIZE'],\n",
    "            max_steps=config['MAX_STEPS'],\n",
    "            weight_decay=0.01,\n",
    "            learning_rate=config['LEARNING_RATE'],\n",
    "            warmup_steps=50,\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=250,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=250,\n",
    "            load_best_model_at_end=False,\n",
    "            fp16=use_fp16,\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "            prediction_loss_only=False,\n",
    "        )\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "# Create training arguments\n",
    "# Set debug_mode=True for quick testing, False for full training\n",
    "training_args = create_training_args(CONFIG, device, debug_mode=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model_to_tune,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: collate_fn(data, processor),\n",
    "    loss_type=CONFIG['LOSS_TYPE'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized and ready\")\n",
    "print(f\"Loss type: {CONFIG['LOSS_TYPE']}\")\n",
    "print(f\"Training steps: {training_args.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57618ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run Baseline Evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING BASELINE EVALUATION (BEFORE FINE-TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temporarily swap to base model for baseline\n",
    "trainer.model = base_model\n",
    "baseline_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Evaluation Metrics:\")\n",
    "for key, value in baseline_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "# Swap back to the model we want to tune\n",
    "trainer.model = model_to_tune.to(device)\n",
    "\n",
    "print(\"\\nBaseline evaluation complete. Ready for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Run Fine-Tuning\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ceed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run Final Evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING FINAL EVALUATION (AFTER FINE-TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Evaluation Metrics:\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: BASELINE vs FINE-TUNED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_keys = ['eval_loss', 'eval_accuracy', 'eval_precision', 'eval_recall', 'eval_f1']\n",
    "for key in comparison_keys:\n",
    "    if key in baseline_metrics and key in final_metrics:\n",
    "        baseline_val = baseline_metrics[key]\n",
    "        final_val = final_metrics[key]\n",
    "        change = final_val - baseline_val\n",
    "        print(f\"{key:20s}: {baseline_val:.4f} -> {final_val:.4f} (Δ {change:+.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f86e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Model and Generate Heatmap\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING MODEL AND GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save LoRA adapter\n",
    "final_adapter_path = os.path.join(CONFIG['OUTPUT_DIR'], \"final-adapter\")\n",
    "model_to_tune.save_pretrained(final_adapter_path)\n",
    "processor.save_pretrained(final_adapter_path)\n",
    "print(f\"\\nLoRA adapter saved to: {final_adapter_path}\")\n",
    "\n",
    "# Generate and save heatmap\n",
    "heatmap_path = os.path.join(CONFIG['OUTPUT_DIR'], \"gradient_impact_heatmap.png\")\n",
    "trainer.plot_final_heatmap(save_path=heatmap_path)\n",
    "\n",
    "print(\"\\nModel and visualizations saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4186dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Qualitative Analysis Functions\n",
    "\n",
    "def get_similarity_scores(model, processor, image, text_probes, device):\n",
    "    \"\"\"\n",
    "    Get model similarity scores for an image and text probes.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        processor: Processor for inputs\n",
    "        image: PIL Image\n",
    "        text_probes: List of text strings\n",
    "        device: Device string\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of similarity scores\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=text_probes,\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        scores = torch.sigmoid(outputs.logits_per_image).cpu().numpy().flatten()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def plot_similarity_scores(base_scores, tuned_scores, probes, true_category, save_path):\n",
    "    \"\"\"\n",
    "    Generate bar chart comparing baseline and fine-tuned similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        base_scores: Baseline model scores\n",
    "        tuned_scores: Fine-tuned model scores\n",
    "        probes: List of text probes\n",
    "        true_category: The correct category\n",
    "        save_path: Path to save plot\n",
    "    \"\"\"\n",
    "    df_data = {\n",
    "        \"Text Probe\": probes * 2,\n",
    "        \"Similarity Score\": np.concatenate([base_scores, tuned_scores]),\n",
    "        \"Model\": [\"Baseline\"] * len(probes) + [\"Fine-Tuned\"] * len(probes)\n",
    "    }\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"Text Probe\",\n",
    "        y=\"Similarity Score\",\n",
    "        hue=\"Model\",\n",
    "        palette={\"Baseline\": \"lightblue\", \"Fine-Tuned\": \"darkblue\"}\n",
    "    )\n",
    "\n",
    "    # Highlight correct category\n",
    "    ax = plt.gca()\n",
    "    for i, probe in enumerate(probes):\n",
    "        if probe == true_category:\n",
    "            ax.get_xticklabels()[i].set_color(\"red\")\n",
    "            ax.get_xticklabels()[i].set_fontweight(\"bold\")\n",
    "\n",
    "    plt.title(f\"Qualitative Similarity Test (True Category: {true_category})\", fontsize=16)\n",
    "    plt.ylabel(\"Similarity Score (Sigmoid)\", fontsize=12)\n",
    "    plt.xlabel(\"Text Probes\", fontsize=12)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.legend(title=\"Model\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity plot saved to: {save_path}\")\n",
    "\n",
    "\n",
    "def run_qualitative_test(base_model, tuned_model, processor, device, output_dir):\n",
    "    \"\"\"\n",
    "    Run qualitative similarity test on a sample image.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Baseline model\n",
    "        tuned_model: Fine-tuned model\n",
    "        processor: Processor\n",
    "        device: Device string\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING QUALITATIVE SIMILARITY TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test image and probes\n",
    "    SAMPLE_IMG_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Seborrhoeic_keratosis_-_close-up.jpg/800px-Seborrhoeic_keratosis_-_close-up.jpg\"\n",
    "    SAMPLE_IMG_CATEGORY = \"benign keratosis\"\n",
    "    TEXT_PROBES = [\n",
    "        SAMPLE_IMG_CATEGORY,\n",
    "        \"melanoma\",\n",
    "        \"nevus\",\n",
    "        \"eczema\",\n",
    "        \"basal cell carcinoma\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Test image: {SAMPLE_IMG_URL}\")\n",
    "    print(f\"True category: {SAMPLE_IMG_CATEGORY}\")\n",
    "    print(f\"Text probes: {TEXT_PROBES}\")\n",
    "    \n",
    "    try:\n",
    "        # Download and prepare image\n",
    "        response = requests.get(SAMPLE_IMG_URL)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        \n",
    "        # Get scores from both models\n",
    "        print(\"\\nComputing similarity scores...\")\n",
    "        base_scores = get_similarity_scores(base_model, processor, image, TEXT_PROBES, device)\n",
    "        tuned_scores = get_similarity_scores(tuned_model, processor, image, TEXT_PROBES, device)\n",
    "        \n",
    "        # Print scores\n",
    "        print(\"\\nSimilarity Scores:\")\n",
    "        print(f\"{'Probe':<25} {'Baseline':<12} {'Fine-Tuned':<12} {'Change':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, probe in enumerate(TEXT_PROBES):\n",
    "            change = tuned_scores[i] - base_scores[i]\n",
    "            marker = \" *\" if probe == SAMPLE_IMG_CATEGORY else \"\"\n",
    "            print(f\"{probe:<25} {base_scores[i]:<12.4f} {tuned_scores[i]:<12.4f} {change:+.4f}{marker}\")\n",
    "        \n",
    "        # Plot and save\n",
    "        sim_report_path = os.path.join(output_dir, \"similarity_report.png\")\n",
    "        plot_similarity_scores(base_scores, tuned_scores, TEXT_PROBES, SAMPLE_IMG_CATEGORY, sim_report_path)\n",
    "        \n",
    "        print(\"\\nQualitative test complete!\")\n",
    "        return sim_report_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during qualitative test: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Qualitative analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Run Qualitative Test\n",
    "\n",
    "similarity_plot_path = run_qualitative_test(\n",
    "    base_model=base_model,\n",
    "    tuned_model=model_to_tune.to(device),\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    output_dir=CONFIG['OUTPUT_DIR']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Generate Final Report\n",
    "\n",
    "def generate_final_report(baseline_metrics, final_metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Generate a markdown report with all results.\n",
    "    \n",
    "    Args:\n",
    "        baseline_metrics: Dictionary of baseline metrics\n",
    "        final_metrics: Dictionary of final metrics\n",
    "        output_dir: Directory to save report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report_path = os.path.join(output_dir, \"final_report.md\")\n",
    "    \n",
    "    report_content = \"# Fine-Tuning Experiment Report\\n\\n\"\n",
    "    report_content += f\"**Model:** {CONFIG['MODEL_ID']}\\n\"\n",
    "    report_content += f\"**Loss Type:** {CONFIG['LOSS_TYPE']}\\n\"\n",
    "    report_content += f\"**LoRA Rank:** {CONFIG['LORA_RANK']}\\n\"\n",
    "    report_content += f\"**LoRA Alpha:** {CONFIG['LORA_ALPHA']}\\n\"\n",
    "    report_content += f\"**Training Steps:** {CONFIG['MAX_STEPS']}\\n\"\n",
    "    report_content += f\"**Learning Rate:** {CONFIG['LEARNING_RATE']}\\n\\n\"\n",
    "    \n",
    "    # Quantitative metrics\n",
    "    report_content += \"## 1. Quantitative Metrics\\n\\n\"\n",
    "    report_content += \"Comparison of model performance on the validation set before and after fine-tuning.\\n\\n\"\n",
    "    report_content += \"| Metric | Baseline (Before) | Fine-Tuned (After) | Change |\\n\"\n",
    "    report_content += \"| :--- | :--- | :--- | :--- |\\n\"\n",
    "    \n",
    "    def get_metric(metrics, key, precision=4):\n",
    "        val = metrics.get(key)\n",
    "        if val is None:\n",
    "            return \"N/A\"\n",
    "        return f\"{val:.{precision}f}\"\n",
    "    \n",
    "    def get_change(baseline, final, key, precision=4):\n",
    "        b = baseline.get(key)\n",
    "        f = final.get(key)\n",
    "        if b is None or f is None:\n",
    "            return \"N/A\"\n",
    "        change = f - b\n",
    "        sign = \"+\" if change >= 0 else \"\"\n",
    "        return f\"{sign}{change:.{precision}f}\"\n",
    "    \n",
    "    metric_keys = [\n",
    "        (\"eval_loss\", \"Eval Loss\"),\n",
    "        (\"eval_accuracy\", \"Accuracy\"),\n",
    "        (\"eval_precision\", \"Precision (Macro)\"),\n",
    "        (\"eval_recall\", \"Recall (Macro)\"),\n",
    "        (\"eval_f1\", \"F1-Score (Macro)\"),\n",
    "        (\"eval_runtime\", \"Eval Runtime (s)\"),\n",
    "    ]\n",
    "    \n",
    "    for key, name in metric_keys:\n",
    "        b_val = get_metric(baseline_metrics, key)\n",
    "        f_val = get_metric(final_metrics, key)\n",
    "        c_val = get_change(baseline_metrics, final_metrics, key)\n",
    "        report_content += f\"| **{name}** | {b_val} | {f_val} | {c_val} |\\n\"\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    report_content += \"\\n## 2. Qualitative Analysis (Similarity Test)\\n\\n\"\n",
    "    report_content += \"This test shows how the model's understanding of specific concepts changed.\\n\\n\"\n",
    "    report_content += \"![Similarity Plot](similarity_report.png)\\n\\n\"\n",
    "    report_content += \"**Interpretation:** The fine-tuned model should show higher similarity scores for the correct category.\\n\"\n",
    "    \n",
    "    # Gradient heatmap\n",
    "    report_content += \"\\n## 3. Gradient Impact Heatmap\\n\\n\"\n",
    "    report_content += \"This heatmap shows which parts of the model were modified most during fine-tuning.\\n\\n\"\n",
    "    report_content += \"![Gradient Impact Heatmap](gradient_impact_heatmap.png)\\n\\n\"\n",
    "    report_content += \"**Interpretation:** Brighter colors indicate layers heavily modified by fine-tuning.\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    try:\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "        print(f\"Report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving report: {e}\")\n",
    "\n",
    "\n",
    "# Generate the report\n",
    "generate_final_report(\n",
    "    baseline_metrics=baseline_metrics,\n",
    "    final_metrics=final_metrics,\n",
    "    output_dir=CONFIG['OUTPUT_DIR']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TASKS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Results saved to: {CONFIG['OUTPUT_DIR']}\")\n",
    "print(\"Files generated:\")\n",
    "print(\"  - final-adapter/ (LoRA weights)\")\n",
    "print(\"  - gradient_impact_heatmap.png\")\n",
    "print(\"  - similarity_report.png\")\n",
    "print(\"  - final_report.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CELL: Utility Functions for Debugging and Analysis\n",
    "\n",
    "def inspect_dataset_sample(dataset, n=3):\n",
    "    \"\"\"\n",
    "    Inspect first n samples from dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset to inspect\n",
    "        n: Number of samples to show\n",
    "    \"\"\"\n",
    "    print(f\"Inspecting first {n} samples from dataset:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(n, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(f\"  Text: {sample['text']}\")\n",
    "        print(f\"  Image: {sample['image'].size}, mode={sample['image'].mode}\")\n",
    "        \n",
    "        # Display image inline (if in Jupyter)\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(sample['image'])\n",
    "        except:\n",
    "            print(\"  (Image display not available)\")\n",
    "\n",
    "\n",
    "def test_single_batch(trainer, dataset, n_samples=4):\n",
    "    \"\"\"\n",
    "    Test processing a single batch through the model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance\n",
    "        dataset: Dataset to sample from\n",
    "        n_samples: Batch size to test\n",
    "    \"\"\"\n",
    "    print(f\"Testing single batch with {n_samples} samples...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get a small batch\n",
    "    batch = [dataset[i] for i in range(min(n_samples, len(dataset)))]\n",
    "    \n",
    "    # Process through collate_fn\n",
    "    inputs = trainer.data_collator(batch)\n",
    "    \n",
    "    if not inputs:\n",
    "        print(\"ERROR: Batch processing failed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Batch processed successfully!\")\n",
    "    print(f\"  pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "    print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    try:\n",
    "        trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = trainer.model(**inputs)\n",
    "            print(f\"  logits_per_image shape: {outputs.logits_per_image.shape}\")\n",
    "            print(f\"  logits_per_text shape: {outputs.logits_per_text.shape}\")\n",
    "        print(\"\\nForward pass successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in forward pass: {e}\")\n",
    "\n",
    "\n",
    "def compare_model_predictions(base_model, tuned_model, processor, image, text, device):\n",
    "    \"\"\"\n",
    "    Compare predictions from base and tuned models on a single example.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Baseline model\n",
    "        tuned_model: Fine-tuned model\n",
    "        processor: Processor\n",
    "        image: PIL Image\n",
    "        text: Text string\n",
    "        device: Device string\n",
    "    \"\"\"\n",
    "    print(\"Comparing model predictions...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "    \n",
    "    # Base model\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model(**inputs)\n",
    "        base_score = torch.sigmoid(base_outputs.logits_per_image).item()\n",
    "    \n",
    "    # Tuned model\n",
    "    tuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tuned_outputs = tuned_model(**inputs)\n",
    "        tuned_score = torch.sigmoid(tuned_outputs.logits_per_image).item()\n",
    "    \n",
    "    print(f\"Baseline similarity: {base_score:.4f}\")\n",
    "    print(f\"Fine-tuned similarity: {tuned_score:.4f}\")\n",
    "    print(f\"Change: {tuned_score - base_score:+.4f}\")\n",
    "\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    Plot training loss over time.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance with logged history\n",
    "    \"\"\"\n",
    "    history = trainer.state.log_history\n",
    "    \n",
    "    # Extract loss values\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry:\n",
    "            train_losses.append(entry['loss'])\n",
    "            steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            eval_losses.append(entry['eval_loss'])\n",
    "    \n",
    "    if not train_losses:\n",
    "        print(\"No training history to plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, train_losses, 'b-', label='Training Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Evaluation loss\n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = [entry['step'] for entry in history if 'eval_loss' in entry]\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', label='Eval Loss')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Evaluation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'training_history.png'))\n",
    "    plt.show()\n",
    "    print(f\"Training history plot saved\")\n",
    "\n",
    "\n",
    "def load_saved_adapter(adapter_path, base_model_id, device):\n",
    "    \"\"\"\n",
    "    Load a saved LoRA adapter.\n",
    "    \n",
    "    Args:\n",
    "        adapter_path: Path to saved adapter\n",
    "        base_model_id: Base model ID\n",
    "        device: Device string\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model with adapter\n",
    "    \"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(f\"Loading adapter from {adapter_path}...\")\n",
    "    \n",
    "    base_model = AutoModel.from_pretrained(base_model_id).to(device)\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    print(\"Adapter loaded successfully\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Utility functions defined!\")\n",
    "print(\"\\nAvailable utilities:\")\n",
    "print(\"  - inspect_dataset_sample(dataset, n=3)\")\n",
    "print(\"  - test_single_batch(trainer, dataset, n_samples=4)\")\n",
    "print(\"  - compare_model_predictions(base_model, tuned_model, processor, image, text, device)\")\n",
    "print(\"  - plot_training_history(trainer)\")\n",
    "print(\"  - load_saved_adapter(adapter_path, base_model_id, device)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-kiosk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
